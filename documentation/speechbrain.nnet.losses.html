

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>speechbrain.nnet.losses module &mdash; SpeechBrain 0.0.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="speechbrain.nnet.normalization module" href="speechbrain.nnet.normalization.html" />
    <link rel="prev" title="speechbrain.nnet.linear module" href="speechbrain.nnet.linear.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> SpeechBrain
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting started:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Quick installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="installation.html#install-via-pypi">Install via PyPI</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#install-locally">Install locally</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#test-installation">Test Installation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="experiment.html">Running an experiment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="experiment.html#yaml-basics">YAML basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="experiment.html#running-arguments">Running arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="experiment.html#tensor-format">Tensor format</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="multigpu.html">Basics of multi-GPU</a><ul>
<li class="toctree-l2"><a class="reference internal" href="multigpu.html#multi-gpu-training-using-data-parallel">Multi-GPU training using Data Parallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="multigpu.html#multi-gpu-training-using-distributed-data-parallel-ddp">Multi-GPU training using Distributed Data Parallel (DDP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="multigpu.html#with-multiple-machines-suppose-you-have-2-servers-with-2-gpus">With multiple machines (suppose you have 2 servers with 2 GPUs):</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#zen-of-speechbrain">Zen of Speechbrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#how-to-get-your-code-in-speechbrain">How to get your code in SpeechBrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#python">Python</a><ul>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#version">Version</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#formatting">Formatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#adding-dependencies">Adding dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#testing">Testing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#documentation">Documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#development-tools">Development tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#flake8">flake8</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#pre-commit">pre-commit</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#the-git-pre-commit-hooks">the git pre-commit hooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#the-git-pre-push-hooks">the git pre-push hooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#pytest-doctests">pytest doctests</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#continuous-integration">Continuous integration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#what-is-ci">What is CI?</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#ci-cd-pipelines">CI / CD Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#our-test-suite">Our test suite</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#pull-request-review-guide">Pull Request review guide</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Documentation:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="speechbrain.html">Core library (speechbrain)</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="speechbrain.core.html">speechbrain.core module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.core.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.core.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.yaml.html">speechbrain.yaml module</a></li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.alignment.html">speechbrain.alignment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.alignment.aligner.html">speechbrain.alignment.aligner module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.alignment.aligner.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.alignment.aligner.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.dataio.html">speechbrain.dataio</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.batch.html">speechbrain.dataio.batch module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.batch.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.batch.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.dataio.html">speechbrain.dataio.dataio module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.dataio.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.dataio.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.dataloader.html">speechbrain.dataio.dataloader module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.dataloader.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.dataloader.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.dataset.html">speechbrain.dataio.dataset module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.dataset.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.dataset.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.encoder.html">speechbrain.dataio.encoder module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.encoder.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.encoder.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.legacy.html">speechbrain.dataio.legacy module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.legacy.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.legacy.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.sampler.html">speechbrain.dataio.sampler module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.sampler.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.sampler.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.wer.html">speechbrain.dataio.wer module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.wer.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.wer.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.decoders.html">speechbrain.decoders</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.decoders.ctc.html">speechbrain.decoders.ctc module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.decoders.ctc.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.decoders.ctc.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.decoders.seq2seq.html">speechbrain.decoders.seq2seq module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.decoders.seq2seq.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.decoders.seq2seq.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.decoders.transducer.html">speechbrain.decoders.transducer module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.decoders.transducer.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.decoders.transducer.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.lm.html">speechbrain.lm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.lm.arpa.html">speechbrain.lm.arpa module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lm.arpa.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lm.arpa.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.lm.counting.html">speechbrain.lm.counting module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lm.counting.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lm.counting.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.lm.ngram.html">speechbrain.lm.ngram module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lm.ngram.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lm.ngram.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.lobes.html">speechbrain.lobes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.lobes.augment.html">speechbrain.lobes.augment module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.augment.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.augment.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.lobes.features.html">speechbrain.lobes.features module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.features.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.features.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.lobes.models.html">speechbrain.lobes.models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.CRDNN.html">speechbrain.lobes.models.CRDNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.ContextNet.html">speechbrain.lobes.models.ContextNet module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.ECAPA_TDNN.html">speechbrain.lobes.models.ECAPA_TDNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.ESPnetVGG.html">speechbrain.lobes.models.ESPnetVGG module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.MetricGAN_D.html">speechbrain.lobes.models.MetricGAN_D module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.RNNLM.html">speechbrain.lobes.models.RNNLM module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.VanillaNN.html">speechbrain.lobes.models.VanillaNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.Xvector.html">speechbrain.lobes.models.Xvector module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.conv_tasnet.html">speechbrain.lobes.models.conv_tasnet module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.convolution.html">speechbrain.lobes.models.convolution module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.dual_path.html">speechbrain.lobes.models.dual_path module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.transformer.html">speechbrain.lobes.models.transformer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="speechbrain.nnet.html">speechbrain.nnet</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.CNN.html">speechbrain.nnet.CNN module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.CNN.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.CNN.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.RNN.html">speechbrain.nnet.RNN module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.RNN.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.RNN.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.activations.html">speechbrain.nnet.activations module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.activations.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.activations.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.attention.html">speechbrain.nnet.attention module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.attention.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.attention.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.containers.html">speechbrain.nnet.containers module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.containers.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.containers.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.dropout.html">speechbrain.nnet.dropout module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.dropout.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.dropout.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.embedding.html">speechbrain.nnet.embedding module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.embedding.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.embedding.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.linear.html">speechbrain.nnet.linear module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.linear.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.linear.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">speechbrain.nnet.losses module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.normalization.html">speechbrain.nnet.normalization module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.normalization.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.normalization.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.pooling.html">speechbrain.nnet.pooling module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.pooling.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.pooling.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.schedulers.html">speechbrain.nnet.schedulers module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.schedulers.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.schedulers.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.complex_networks.html">speechbrain.nnet.complex_networks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.complex_networks.c_CNN.html">speechbrain.nnet.complex_networks.c_CNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.complex_networks.c_RNN.html">speechbrain.nnet.complex_networks.c_RNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.complex_networks.c_linear.html">speechbrain.nnet.complex_networks.c_linear module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.complex_networks.c_normalization.html">speechbrain.nnet.complex_networks.c_normalization module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.complex_networks.c_ops.html">speechbrain.nnet.complex_networks.c_ops module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.loss.html">speechbrain.nnet.loss</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.loss.stoi_loss.html">speechbrain.nnet.loss.stoi_loss module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.loss.transducer_loss.html">speechbrain.nnet.loss.transducer_loss module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.quaternion_networks.html">speechbrain.nnet.quaternion_networks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.quaternion_networks.q_CNN.html">speechbrain.nnet.quaternion_networks.q_CNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.quaternion_networks.q_RNN.html">speechbrain.nnet.quaternion_networks.q_RNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.quaternion_networks.q_linear.html">speechbrain.nnet.quaternion_networks.q_linear module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.quaternion_networks.q_normalization.html">speechbrain.nnet.quaternion_networks.q_normalization module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.quaternion_networks.q_ops.html">speechbrain.nnet.quaternion_networks.q_ops module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.transducer.html">speechbrain.nnet.transducer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.transducer.transducer_joint.html">speechbrain.nnet.transducer.transducer_joint module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.processing.html">speechbrain.processing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.processing.NMF.html">speechbrain.processing.NMF module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.NMF.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.NMF.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.processing.PLDA_LDA.html">speechbrain.processing.PLDA_LDA module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.PLDA_LDA.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.PLDA_LDA.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.processing.decomposition.html">speechbrain.processing.decomposition module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.decomposition.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.decomposition.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.processing.diarization.html">speechbrain.processing.diarization module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.diarization.html#reference">Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.diarization.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.diarization.html#id1">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.processing.features.html">speechbrain.processing.features module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.features.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.features.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.processing.multi_mic.html">speechbrain.processing.multi_mic module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.multi_mic.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.multi_mic.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.processing.signal_processing.html">speechbrain.processing.signal_processing module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.signal_processing.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.signal_processing.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.processing.speech_augmentation.html">speechbrain.processing.speech_augmentation module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.speech_augmentation.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.speech_augmentation.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.tokenizers.html">speechbrain.tokenizers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.tokenizers.SentencePiece.html">speechbrain.tokenizers.SentencePiece module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.tokenizers.SentencePiece.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.tokenizers.SentencePiece.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.utils.html">speechbrain.utils</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.Accuracy.html">speechbrain.utils.Accuracy module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.Accuracy.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.Accuracy.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.DER.html">speechbrain.utils.DER module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.DER.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.DER.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.checkpoints.html">speechbrain.utils.checkpoints module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.checkpoints.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.checkpoints.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.convert_model.html">speechbrain.utils.convert_model module</a></li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.data_pipeline.html">speechbrain.utils.data_pipeline module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.data_pipeline.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.data_pipeline.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.data_utils.html">speechbrain.utils.data_utils module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.data_utils.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.data_utils.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.depgraph.html">speechbrain.utils.depgraph module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.depgraph.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.depgraph.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.distributed.html">speechbrain.utils.distributed module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.distributed.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.distributed.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.edit_distance.html">speechbrain.utils.edit_distance module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.edit_distance.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.edit_distance.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.epoch_loop.html">speechbrain.utils.epoch_loop module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.epoch_loop.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.epoch_loop.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.logger.html">speechbrain.utils.logger module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.logger.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.logger.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.metric_stats.html">speechbrain.utils.metric_stats module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.metric_stats.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.metric_stats.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.parameter_transfer.html">speechbrain.utils.parameter_transfer module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.parameter_transfer.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.parameter_transfer.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.superpowers.html">speechbrain.utils.superpowers module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.superpowers.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.superpowers.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.train_logger.html">speechbrain.utils.train_logger module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.train_logger.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.train_logger.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Runnable Tools (tools)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tools.compute_wer.html">tools.compute_wer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="tools.compute_wer.html#usage">Usage</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">SpeechBrain</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="speechbrain.html">speechbrain</a> &raquo;</li>
        
          <li><a href="speechbrain.nnet.html">speechbrain.nnet</a> &raquo;</li>
        
      <li>speechbrain.nnet.losses module</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/speechbrain.nnet.losses.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-speechbrain.nnet.losses">
<span id="speechbrain-nnet-losses-module"></span><h1>speechbrain.nnet.losses module<a class="headerlink" href="#module-speechbrain.nnet.losses" title="Permalink to this headline">¶</a></h1>
<p>Losses for training neural networks.</p>
<dl class="docutils">
<dt>Authors</dt><dd><ul class="simple">
<li>Mirco Ravanelli 2020</li>
<li>Samuele Cornell 2020</li>
<li>Hwidong Na 2020</li>
<li>Yan Gao 2020</li>
<li>Titouan Parcollet 2020</li>
</ul>
</dd>
</dl>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>Classes:</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#speechbrain.nnet.losses.AdditiveAngularMargin" title="speechbrain.nnet.losses.AdditiveAngularMargin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">AdditiveAngularMargin</span></code></a></td>
<td>An implementation of Additive Angular Margin (AAM) proposed in the following paper: ‘’‘Margin Matters: Towards More Discriminative Deep Neural Network Embeddings for Speaker Recognition’‘’ (<a class="reference external" href="https://arxiv.org/abs/1906.07317">https://arxiv.org/abs/1906.07317</a>)</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#speechbrain.nnet.losses.AngularMargin" title="speechbrain.nnet.losses.AngularMargin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">AngularMargin</span></code></a></td>
<td>An implementation of Angular Margin (AM) proposed in the following paper: ‘’‘Margin Matters: Towards More Discriminative Deep Neural Network Embeddings for Speaker Recognition’‘’ (<a class="reference external" href="https://arxiv.org/abs/1906.07317">https://arxiv.org/abs/1906.07317</a>)</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#speechbrain.nnet.losses.LogSoftmaxWrapper" title="speechbrain.nnet.losses.LogSoftmaxWrapper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LogSoftmaxWrapper</span></code></a></td>
<td><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">returns:</th><td class="field-body"><ul class="first last simple">
<li><strong>loss</strong> (<em>torch.Tensor</em>) – Learning loss</li>
</ul>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#speechbrain.nnet.losses.PitWrapper" title="speechbrain.nnet.losses.PitWrapper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PitWrapper</span></code></a></td>
<td>Permutation Invariant Wrapper to allow Permutation Invariant Training (PIT) with existing losses.</td>
</tr>
</tbody>
</table>
<p>Functions:</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#speechbrain.nnet.losses.BCE_loss" title="speechbrain.nnet.losses.BCE_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BCE_loss</span></code></a></td>
<td>Computes binary cross-entropy (BCE) loss.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#speechbrain.nnet.losses.cal_si_snr" title="speechbrain.nnet.losses.cal_si_snr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cal_si_snr</span></code></a></td>
<td>Calculate SI-SNR.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#speechbrain.nnet.losses.ce_kd" title="speechbrain.nnet.losses.ce_kd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ce_kd</span></code></a></td>
<td>Simple version of distillation fro cross-entropy loss.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#speechbrain.nnet.losses.classification_error" title="speechbrain.nnet.losses.classification_error"><code class="xref py py-obj docutils literal notranslate"><span class="pre">classification_error</span></code></a></td>
<td>Computes the classification error at frame or batch level.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#speechbrain.nnet.losses.compute_masked_loss" title="speechbrain.nnet.losses.compute_masked_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">compute_masked_loss</span></code></a></td>
<td>Compute the true average loss of a set of waveforms of unequal length.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#speechbrain.nnet.losses.ctc_loss" title="speechbrain.nnet.losses.ctc_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ctc_loss</span></code></a></td>
<td>CTC loss.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#speechbrain.nnet.losses.ctc_loss_kd" title="speechbrain.nnet.losses.ctc_loss_kd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ctc_loss_kd</span></code></a></td>
<td>Knowledge distillation for CTC loss.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#speechbrain.nnet.losses.get_mask" title="speechbrain.nnet.losses.get_mask"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_mask</span></code></a></td>
<td><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">param source:</th><td class="field-body"></td>
</tr>
</tbody>
</table>
</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#speechbrain.nnet.losses.get_si_snr_with_pitwrapper" title="speechbrain.nnet.losses.get_si_snr_with_pitwrapper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_si_snr_with_pitwrapper</span></code></a></td>
<td>This function wraps si_snr calculation with the speechbrain pit-wrapper.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#speechbrain.nnet.losses.kldiv_loss" title="speechbrain.nnet.losses.kldiv_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kldiv_loss</span></code></a></td>
<td>Computes the KL-divergence error at the batch level.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#speechbrain.nnet.losses.l1_loss" title="speechbrain.nnet.losses.l1_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">l1_loss</span></code></a></td>
<td>Compute the true l1 loss, accounting for length differences.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#speechbrain.nnet.losses.mse_loss" title="speechbrain.nnet.losses.mse_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mse_loss</span></code></a></td>
<td>Compute the true mean squared error, accounting for length differences.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#speechbrain.nnet.losses.nll_loss" title="speechbrain.nnet.losses.nll_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nll_loss</span></code></a></td>
<td>Computes negative log likelihood loss.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#speechbrain.nnet.losses.nll_loss_kd" title="speechbrain.nnet.losses.nll_loss_kd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nll_loss_kd</span></code></a></td>
<td>Knowledge distillation for negative log-likelihood loss.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#speechbrain.nnet.losses.transducer_loss" title="speechbrain.nnet.losses.transducer_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transducer_loss</span></code></a></td>
<td>Transducer loss, see <cite>speechbrain/nnet/transducer/transducer_loss.py</cite>.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#speechbrain.nnet.losses.truncate" title="speechbrain.nnet.losses.truncate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">truncate</span></code></a></td>
<td>Ensure that predictions and targets are the same length.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="speechbrain.nnet.losses.transducer_loss">
<code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">transducer_loss</code><span class="sig-paren">(</span><em><span class="n">log_probs</span></em>, <em><span class="n">targets</span></em>, <em><span class="n">input_lens</span></em>, <em><span class="n">target_lens</span></em>, <em><span class="n">blank_index</span></em>, <em><span class="n">reduction</span><span class="o">=</span><span class="default_value">'mean'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#transducer_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.transducer_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Transducer loss, see <cite>speechbrain/nnet/transducer/transducer_loss.py</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>predictions</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Predicted tensor, of shape [batch, time, chars].</li>
<li><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Target tensor, without any blanks, of shape [batch, target_len].</li>
<li><strong>input_lens</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Length of each utterance.</li>
<li><strong>target_lens</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Length of each target sequence.</li>
<li><strong>blank_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – The location of the blank symbol among the character indexes.</li>
<li><strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)"><em>str</em></a>) – Specifies the reduction to apply to the output: ‘mean’ | ‘batchmean’ | ‘sum’.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py class">
<dt id="speechbrain.nnet.losses.PitWrapper">
<em class="property">class </em><code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">PitWrapper</code><span class="sig-paren">(</span><em><span class="n">base_loss</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#PitWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.PitWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Permutation Invariant Wrapper to allow Permutation Invariant Training
(PIT) with existing losses.</p>
<p>Permutation invariance is calculated over the sources/classes axis which is
assumed to be the rightmost dimension: predictions and targets tensors are
assumed to have shape [batch, …, channels, sources].</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>base_loss</strong> (<em>function</em>) – Base loss function, e.g. torch.nn.MSELoss. It is assumed that it takes
two arguments:
predictions and targets and no reduction is performed.
(if a pytorch loss is used, the user must specify reduction=”none”).</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>pit_loss</strong> – Torch module supporting forward method for PIT.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))">torch.nn.Module</a></td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pit_mse</span> <span class="o">=</span> <span class="n">PitWrapper</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predictions</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">p</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">,</span> <span class="n">opt_p</span> <span class="o">=</span> <span class="n">pit_mse</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span>
<span class="go">tensor([0., 0.])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="speechbrain.nnet.losses.PitWrapper.reorder_tensor">
<code class="descname">reorder_tensor</code><span class="sig-paren">(</span><em><span class="n">tensor</span></em>, <em><span class="n">p</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#PitWrapper.reorder_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.PitWrapper.reorder_tensor" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Tensor to reorder given the optimal permutation, of shape
[batch, …, sources].</li>
<li><strong>p</strong> (<em>list of tuples</em>) – List of optimal permutations, e.g. for batch=2 and n_sources=3
[(0, 1, 2), (0, 2, 1].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>reordered</strong> – Reordered tensor given permutation p.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))">torch.Tensor</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py method">
<dt id="speechbrain.nnet.losses.PitWrapper.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em><span class="n">preds</span></em>, <em><span class="n">targets</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#PitWrapper.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.PitWrapper.forward" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>preds</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Network predictions tensor, of shape
[batch, channels, …, sources].</li>
<li><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Target tensor, of shape [batch, channels, …, sources].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><strong>loss</strong> (<em>torch.Tensor</em>) – Permutation invariant loss for current examples, tensor of
shape [batch]</li>
<li><strong>perms</strong> (<em>list</em>) – List of indexes for optimal permutation of the inputs over
sources.
e.g., [(0, 1, 2), (2, 1, 0)] for three sources and 2 examples
per batch.</li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py attribute">
<dt id="speechbrain.nnet.losses.PitWrapper.training">
<code class="descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a></em><a class="headerlink" href="#speechbrain.nnet.losses.PitWrapper.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="speechbrain.nnet.losses.ctc_loss">
<code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">ctc_loss</code><span class="sig-paren">(</span><em><span class="n">log_probs</span></em>, <em><span class="n">targets</span></em>, <em><span class="n">input_lens</span></em>, <em><span class="n">target_lens</span></em>, <em><span class="n">blank_index</span></em>, <em><span class="n">reduction</span><span class="o">=</span><span class="default_value">'mean'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#ctc_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.ctc_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>CTC loss.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>predictions</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Predicted tensor, of shape [batch, time, chars].</li>
<li><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Target tensor, without any blanks, of shape [batch, target_len]</li>
<li><strong>input_lens</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Length of each utterance.</li>
<li><strong>target_lens</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Length of each target sequence.</li>
<li><strong>blank_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – The location of the blank symbol among the character indexes.</li>
<li><strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)"><em>str</em></a>) – What reduction to apply to the output. ‘mean’, ‘sum’, ‘batch’,
‘batchmean’, ‘none’.
See pytorch for ‘mean’, ‘sum’, ‘none’. The ‘batch’ option returns
one loss per item in the batch, ‘batchmean’ returns sum / batch size.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py function">
<dt id="speechbrain.nnet.losses.l1_loss">
<code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">l1_loss</code><span class="sig-paren">(</span><em><span class="n">predictions</span></em>, <em><span class="n">targets</span></em>, <em><span class="n">length</span><span class="o">=</span><span class="default_value">None</span></em>, <em><span class="n">allowed_len_diff</span><span class="o">=</span><span class="default_value">3</span></em>, <em><span class="n">reduction</span><span class="o">=</span><span class="default_value">'mean'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#l1_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.l1_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the true l1 loss, accounting for length differences.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>predictions</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Predicted tensor, of shape <code class="docutils literal notranslate"><span class="pre">[batch,</span> <span class="pre">time,</span> <span class="pre">*]</span></code>.</li>
<li><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Target tensor with the same size as predicted tensor.</li>
<li><strong>length</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Length of each utterance for computing true error with a mask.</li>
<li><strong>allowed_len_diff</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – Length difference that will be tolerated before raising an exception.</li>
<li><strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)"><em>str</em></a>) – Options are ‘mean’, ‘batch’, ‘batchmean’, ‘sum’.
See pytorch for ‘mean’, ‘sum’. The ‘batch’ option returns
one loss per item in the batch, ‘batchmean’ returns sum / batch size.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l1_loss</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]]))</span>
<span class="go">tensor(0.1000)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="speechbrain.nnet.losses.mse_loss">
<code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">mse_loss</code><span class="sig-paren">(</span><em><span class="n">predictions</span></em>, <em><span class="n">targets</span></em>, <em><span class="n">length</span><span class="o">=</span><span class="default_value">None</span></em>, <em><span class="n">allowed_len_diff</span><span class="o">=</span><span class="default_value">3</span></em>, <em><span class="n">reduction</span><span class="o">=</span><span class="default_value">'mean'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#mse_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.mse_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the true mean squared error, accounting for length differences.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>predictions</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Predicted tensor, of shape <code class="docutils literal notranslate"><span class="pre">[batch,</span> <span class="pre">time,</span> <span class="pre">*]</span></code>.</li>
<li><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Target tensor with the same size as predicted tensor.</li>
<li><strong>length</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Length of each utterance for computing true error with a mask.</li>
<li><strong>allowed_len_diff</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – Length difference that will be tolerated before raising an exception.</li>
<li><strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)"><em>str</em></a>) – Options are ‘mean’, ‘batch’, ‘batchmean’, ‘sum’.
See pytorch for ‘mean’, ‘sum’. The ‘batch’ option returns
one loss per item in the batch, ‘batchmean’ returns sum / batch size.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mse_loss</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]]))</span>
<span class="go">tensor(0.0100)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="speechbrain.nnet.losses.classification_error">
<code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">classification_error</code><span class="sig-paren">(</span><em><span class="n">probabilities</span></em>, <em><span class="n">targets</span></em>, <em><span class="n">length</span><span class="o">=</span><span class="default_value">None</span></em>, <em><span class="n">allowed_len_diff</span><span class="o">=</span><span class="default_value">3</span></em>, <em><span class="n">reduction</span><span class="o">=</span><span class="default_value">'mean'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#classification_error"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.classification_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the classification error at frame or batch level.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>probabilities</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The posterior probabilities of shape
[batch, prob] or [batch, frames, prob]</li>
<li><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The targets, of shape [batch] or [batch, frames]</li>
<li><strong>length</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Length of each utterance, if frame-level loss is desired.</li>
<li><strong>allowed_len_diff</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – Length difference that will be tolerated before raising an exception.</li>
<li><strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)"><em>str</em></a>) – Options are ‘mean’, ‘batch’, ‘batchmean’, ‘sum’.
See pytorch for ‘mean’, ‘sum’. The ‘batch’ option returns
one loss per item in the batch, ‘batchmean’ returns sum / batch size.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classification_error</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">tensor(0.5000)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="speechbrain.nnet.losses.nll_loss">
<code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">nll_loss</code><span class="sig-paren">(</span><em><span class="n">log_probabilities</span></em>, <em><span class="n">targets</span></em>, <em><span class="n">length</span><span class="o">=</span><span class="default_value">None</span></em>, <em><span class="n">label_smoothing</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em><span class="n">allowed_len_diff</span><span class="o">=</span><span class="default_value">3</span></em>, <em><span class="n">reduction</span><span class="o">=</span><span class="default_value">'mean'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#nll_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.nll_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes negative log likelihood loss.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>log_probabilities</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The probabilities after log has been applied.
Format is [batch, log_p] or [batch, frames, log_p].</li>
<li><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The targets, of shape [batch] or [batch, frames].</li>
<li><strong>length</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Length of each utterance, if frame-level loss is desired.</li>
<li><strong>allowed_len_diff</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – Length difference that will be tolerated before raising an exception.</li>
<li><strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)"><em>str</em></a>) – Options are ‘mean’, ‘batch’, ‘batchmean’, ‘sum’.
See pytorch for ‘mean’, ‘sum’. The ‘batch’ option returns
one loss per item in the batch, ‘batchmean’ returns sum / batch size.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nll_loss</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">tensor(1.2040)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="speechbrain.nnet.losses.BCE_loss">
<code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">BCE_loss</code><span class="sig-paren">(</span><em><span class="n">inputs</span></em>, <em><span class="n">targets</span></em>, <em><span class="n">length</span><span class="o">=</span><span class="default_value">None</span></em>, <em><span class="n">weight</span><span class="o">=</span><span class="default_value">None</span></em>, <em><span class="n">pos_weight</span><span class="o">=</span><span class="default_value">None</span></em>, <em><span class="n">reduction</span><span class="o">=</span><span class="default_value">'mean'</span></em>, <em><span class="n">allowed_len_diff</span><span class="o">=</span><span class="default_value">3</span></em>, <em><span class="n">label_smoothing</span><span class="o">=</span><span class="default_value">0.0</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#BCE_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.BCE_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes binary cross-entropy (BCE) loss. It also applies the sigmoid
function directly (this improves the numerical stability).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>inputs</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The output before applying the final softmax
Format is [batch, 1] or [batch, frames, 1].</li>
<li><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The targets, of shape [batch] or [batch, frames].</li>
<li><strong>length</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Length of each utterance, if frame-level loss is desired.</li>
<li><strong>weight</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – A manual rescaling weight if provided it’s repeated to match input
tensor shape.</li>
<li><strong>pos_weight</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – A weight of positive examples. Must be a vector with length equal to
the number of classes.</li>
<li><strong>allowed_len_diff</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – Length difference that will be tolerated before raising an exception.</li>
<li><strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)"><em>str</em></a>) – Options are ‘mean’, ‘batch’, ‘batchmean’, ‘sum’.
See pytorch for ‘mean’, ‘sum’. The ‘batch’ option returns
one loss per item in the batch, ‘batchmean’ returns sum / batch size.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">10.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">BCE_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="go">tensor(0.0013)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="speechbrain.nnet.losses.kldiv_loss">
<code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">kldiv_loss</code><span class="sig-paren">(</span><em><span class="n">log_probabilities</span></em>, <em><span class="n">targets</span></em>, <em><span class="n">length</span><span class="o">=</span><span class="default_value">None</span></em>, <em><span class="n">label_smoothing</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em><span class="n">allowed_len_diff</span><span class="o">=</span><span class="default_value">3</span></em>, <em><span class="n">pad_idx</span><span class="o">=</span><span class="default_value">0</span></em>, <em><span class="n">reduction</span><span class="o">=</span><span class="default_value">'mean'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#kldiv_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.kldiv_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the KL-divergence error at the batch level.
This loss applies label smoothing directly to the targets</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>probabilities</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The posterior probabilities of shape
[batch, prob] or [batch, frames, prob].</li>
<li><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The targets, of shape [batch] or [batch, frames].</li>
<li><strong>length</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Length of each utterance, if frame-level loss is desired.</li>
<li><strong>allowed_len_diff</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – Length difference that will be tolerated before raising an exception.</li>
<li><strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)"><em>str</em></a>) – Options are ‘mean’, ‘batch’, ‘batchmean’, ‘sum’.
See pytorch for ‘mean’, ‘sum’. The ‘batch’ option returns
one loss per item in the batch, ‘batchmean’ returns sum / batch size.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kldiv_loss</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">tensor(1.2040)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="speechbrain.nnet.losses.truncate">
<code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">truncate</code><span class="sig-paren">(</span><em><span class="n">predictions</span></em>, <em><span class="n">targets</span></em>, <em><span class="n">allowed_len_diff</span><span class="o">=</span><span class="default_value">3</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#truncate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.truncate" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensure that predictions and targets are the same length.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>predictions</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – First tensor for checking length.</li>
<li><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Second tensor for checking length.</li>
<li><strong>allowed_len_diff</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – Length difference that will be tolerated before raising an exception.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py function">
<dt id="speechbrain.nnet.losses.compute_masked_loss">
<code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">compute_masked_loss</code><span class="sig-paren">(</span><em><span class="n">loss_fn</span></em>, <em><span class="n">predictions</span></em>, <em><span class="n">targets</span></em>, <em><span class="n">length</span><span class="o">=</span><span class="default_value">None</span></em>, <em><span class="n">label_smoothing</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em><span class="n">reduction</span><span class="o">=</span><span class="default_value">'mean'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#compute_masked_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.compute_masked_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the true average loss of a set of waveforms of unequal length.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>loss_fn</strong> (<em>function</em>) – A function for computing the loss taking just predictions and targets.
Should return all the losses, not a reduction (e.g. reduction=”none”).</li>
<li><strong>predictions</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – First argument to loss function.</li>
<li><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Second argument to loss function.</li>
<li><strong>length</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Length of each utterance to compute mask. If None, global average is
computed and returned.</li>
<li><strong>label_smoothing</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – The proportion of label smoothing. Should only be used for NLL loss.
Ref: Regularizing Neural Networks by Penalizing Confident Output
Distributions. <a class="reference external" href="https://arxiv.org/abs/1701.06548">https://arxiv.org/abs/1701.06548</a></li>
<li><strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)"><em>str</em></a>) – One of ‘mean’, ‘batch’, ‘batchmean’, ‘none’ where ‘mean’ returns a
single value and ‘batch’ returns one per item in the batch and
‘batchmean’ is sum / batch_size and ‘none’ returns all.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py function">
<dt id="speechbrain.nnet.losses.get_si_snr_with_pitwrapper">
<code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">get_si_snr_with_pitwrapper</code><span class="sig-paren">(</span><em><span class="n">source</span></em>, <em><span class="n">estimate_source</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#get_si_snr_with_pitwrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.get_si_snr_with_pitwrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>This function wraps si_snr calculation with the speechbrain pit-wrapper.</p>
<dl class="docutils">
<dt>source: [B, T, C],</dt><dd>Where B is the batch size, T is the length of the sources, C is
the number of sources the ordering is made so that this loss is
compatible with the class PitWrapper.</dd>
<dt>estimate_source: [B, T, C]</dt><dd>The estimated source.</dd>
</dl>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">600</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xhat</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">si_snr</span> <span class="o">=</span> <span class="o">-</span><span class="n">get_si_snr_with_pitwrapper</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">xhat</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">si_snr</span><span class="p">)</span>
<span class="go">tensor([135.2284, 135.2284, 135.2284])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="speechbrain.nnet.losses.cal_si_snr">
<code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">cal_si_snr</code><span class="sig-paren">(</span><em><span class="n">source</span></em>, <em><span class="n">estimate_source</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#cal_si_snr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.cal_si_snr" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate SI-SNR.</p>
<dl class="docutils">
<dt>source: [T, B, C],</dt><dd>Where B is batch size, T is the length of the sources, C is the number of sources
the ordering is made so that this loss is compatible with the class PitWrapper.</dd>
<dt>estimate_source: [T, B, C]</dt><dd>The estimated source.</dd>
</dl>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">123</span><span class="p">,</span> <span class="mi">45</span><span class="p">],</span> <span class="p">[</span><span class="mi">34</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">2312</span><span class="p">,</span> <span class="mi">421</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xhat</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xhat</span> <span class="o">=</span> <span class="n">xhat</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">si_snr</span> <span class="o">=</span> <span class="o">-</span><span class="n">cal_si_snr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">xhat</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">si_snr</span><span class="p">)</span>
<span class="go">tensor([[[ 25.2142, 144.1789],</span>
<span class="go">         [130.9283,  25.2142]]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="speechbrain.nnet.losses.get_mask">
<code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">get_mask</code><span class="sig-paren">(</span><em><span class="n">source</span></em>, <em><span class="n">source_lengths</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#get_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.get_mask" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>source</strong> (<em>[</em><em>T</em><em>, </em><em>B</em><em>, </em><em>C</em><em>]</em>) – </li>
<li><strong>source_lengths</strong> (<em>[</em><em>B</em><em>]</em>) – </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul>
<li><p class="first"><strong>mask</strong> (<em>[T, B, 1]</em>)</p>
</li>
<li><p class="first"><em>Example</em></p>
</li>
<li><p class="first"><em>———</em></p>
</li>
<li><p class="first"><em>&gt;&gt;&gt; source = torch.randn(4, 3, 2)</em></p>
</li>
<li><p class="first"><em>&gt;&gt;&gt; source_lengths = torch.Tensor([2, 1, 4]).int()</em></p>
</li>
<li><p class="first"><em>&gt;&gt;&gt; mask = get_mask(source, source_lengths)</em></p>
</li>
<li><p class="first"><em>&gt;&gt;&gt; print(mask)</em></p>
</li>
<li><p class="first"><em>tensor([[[1.],</em> – [1.],
[1.]],</p>
</li>
<li><p class="first"><em>&lt;BLANKLINE&gt;</em> –</p>
<dl class="docutils">
<dt>[[1.],</dt><dd><p>[0.],
[1.]],</p>
</dd>
</dl>
</li>
<li><p class="first"><em>&lt;BLANKLINE&gt;</em> –</p>
<dl class="docutils">
<dt>[[0.],</dt><dd><p>[0.],
[1.]],</p>
</dd>
</dl>
</li>
<li><p class="first"><em>&lt;BLANKLINE&gt;</em> –</p>
<dl class="docutils">
<dt>[[0.],</dt><dd><p>[0.],
[1.]]])</p>
</dd>
</dl>
</li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py class">
<dt id="speechbrain.nnet.losses.AngularMargin">
<em class="property">class </em><code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">AngularMargin</code><span class="sig-paren">(</span><em><span class="n">margin</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em><span class="n">scale</span><span class="o">=</span><span class="default_value">1.0</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#AngularMargin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.AngularMargin" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>An implementation of Angular Margin (AM) proposed in the following
paper: ‘’‘Margin Matters: Towards More Discriminative Deep Neural Network
Embeddings for Speaker Recognition’‘’ (<a class="reference external" href="https://arxiv.org/abs/1906.07317">https://arxiv.org/abs/1906.07317</a>)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – The margin for cosine similiarity</li>
<li><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – The scale for cosine similiarity</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>predictions</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))">torch.Tensor</a></p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pred</span> <span class="o">=</span> <span class="n">AngularMargin</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predictions</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predictions</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">predictions</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="go">tensor([ True, False,  True, False])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="speechbrain.nnet.losses.AngularMargin.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em><span class="n">outputs</span></em>, <em><span class="n">targets</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#AngularMargin.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.AngularMargin.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute AM between two tensors</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>outputs</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The outputs of shape [N, C], cosine simiarity is required.</li>
<li><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The targets of shape [N, C], where the margin is applied for.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>predictions</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))">torch.Tensor</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py attribute">
<dt id="speechbrain.nnet.losses.AngularMargin.training">
<code class="descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a></em><a class="headerlink" href="#speechbrain.nnet.losses.AngularMargin.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="speechbrain.nnet.losses.AdditiveAngularMargin">
<em class="property">class </em><code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">AdditiveAngularMargin</code><span class="sig-paren">(</span><em><span class="n">margin</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em><span class="n">scale</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em><span class="n">easy_margin</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#AdditiveAngularMargin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.AdditiveAngularMargin" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#speechbrain.nnet.losses.AngularMargin" title="speechbrain.nnet.losses.AngularMargin"><code class="xref py py-class docutils literal notranslate"><span class="pre">speechbrain.nnet.losses.AngularMargin</span></code></a></p>
<p>An implementation of Additive Angular Margin (AAM) proposed
in the following paper: ‘’‘Margin Matters: Towards More Discriminative Deep
Neural Network Embeddings for Speaker Recognition’‘’
(<a class="reference external" href="https://arxiv.org/abs/1906.07317">https://arxiv.org/abs/1906.07317</a>)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – The margin for cosine similiarity.</li>
<li><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – The scale for cosine similiarity.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>predictions</strong> – Tensor.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))">torch.Tensor</a></p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred</span> <span class="o">=</span> <span class="n">AdditiveAngularMargin</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predictions</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predictions</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">predictions</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="go">tensor([ True, False,  True, False])</span>
</pre></div>
</div>
<dl class="py method">
<dt id="speechbrain.nnet.losses.AdditiveAngularMargin.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em><span class="n">outputs</span></em>, <em><span class="n">targets</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#AdditiveAngularMargin.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.AdditiveAngularMargin.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute AAM between two tensors</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>outputs</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The outputs of shape [N, C], cosine simiarity is required.</li>
<li><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The targets of shape [N, C], where the margin is applied for.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>predictions</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))">torch.Tensor</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py attribute">
<dt id="speechbrain.nnet.losses.AdditiveAngularMargin.training">
<code class="descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a></em><a class="headerlink" href="#speechbrain.nnet.losses.AdditiveAngularMargin.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="speechbrain.nnet.losses.LogSoftmaxWrapper">
<em class="property">class </em><code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">LogSoftmaxWrapper</code><span class="sig-paren">(</span><em><span class="n">loss_fn</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#LogSoftmaxWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.LogSoftmaxWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><ul class="simple">
<li><strong>loss</strong> (<em>torch.Tensor</em>) – Learning loss</li>
<li><strong>predictions</strong> (<em>torch.Tensor</em>) – Log probabilities</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_prob</span> <span class="o">=</span> <span class="n">LogSoftmaxWrapper</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">log_prob</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="mi">1</span>
<span class="go">tensor(True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_prob</span> <span class="o">=</span> <span class="n">LogSoftmaxWrapper</span><span class="p">(</span><span class="n">AngularMargin</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">log_prob</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="mi">1</span>
<span class="go">tensor(True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_prob</span> <span class="o">=</span> <span class="n">LogSoftmaxWrapper</span><span class="p">(</span><span class="n">AdditiveAngularMargin</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">log_prob</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="mi">1</span>
<span class="go">tensor(True)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="speechbrain.nnet.losses.LogSoftmaxWrapper.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em><span class="n">outputs</span></em>, <em><span class="n">targets</span></em>, <em><span class="n">length</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#LogSoftmaxWrapper.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.LogSoftmaxWrapper.forward" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>outputs</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Network output tensor, of shape
[batch, 1, outdim].</li>
<li><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Target tensor, of shape [batch, 1].</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>loss</strong> – Loss for current examples.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))">torch.Tensor</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py attribute">
<dt id="speechbrain.nnet.losses.LogSoftmaxWrapper.training">
<code class="descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a></em><a class="headerlink" href="#speechbrain.nnet.losses.LogSoftmaxWrapper.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="speechbrain.nnet.losses.ctc_loss_kd">
<code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">ctc_loss_kd</code><span class="sig-paren">(</span><em><span class="n">log_probs</span></em>, <em><span class="n">targets</span></em>, <em><span class="n">input_lens</span></em>, <em><span class="n">blank_index</span></em>, <em><span class="n">device</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#ctc_loss_kd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.ctc_loss_kd" title="Permalink to this definition">¶</a></dt>
<dd><p>Knowledge distillation for CTC loss.</p>
<p>Distilling Knowledge from Ensembles of Acoustic Models for Joint CTC-Attention End-to-End Speech Recognition.
<a class="reference external" href="https://arxiv.org/abs/2005.09310">https://arxiv.org/abs/2005.09310</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>log_probs</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Predicted tensor from student model, of shape [batch, time, chars].</li>
<li><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Predicted tensor from single teacher model, of shape [batch, time, chars].</li>
<li><strong>input_lens</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Length of each utterance.</li>
<li><strong>blank_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – The location of the blank symbol among the character indexes.</li>
<li><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)"><em>str</em></a>) – Device for computing.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py function">
<dt id="speechbrain.nnet.losses.ce_kd">
<code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">ce_kd</code><span class="sig-paren">(</span><em><span class="n">inp</span></em>, <em><span class="n">target</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#ce_kd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.ce_kd" title="Permalink to this definition">¶</a></dt>
<dd><p>Simple version of distillation fro cross-entropy loss.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>inp</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The probabilities from student model, of shape [batch_size * length, feature]</li>
<li><strong>target</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The probabilities from teacher model, of shape [batch_size * length, feature]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py function">
<dt id="speechbrain.nnet.losses.nll_loss_kd">
<code class="descclassname">speechbrain.nnet.losses.</code><code class="descname">nll_loss_kd</code><span class="sig-paren">(</span><em><span class="n">probabilities</span></em>, <em><span class="n">targets</span></em>, <em><span class="n">rel_lab_lengths</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/nnet/losses.html#nll_loss_kd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.nnet.losses.nll_loss_kd" title="Permalink to this definition">¶</a></dt>
<dd><p>Knowledge distillation for negative log-likelihood loss.</p>
<p>Distilling Knowledge from Ensembles of Acoustic Models for Joint CTC-Attention End-to-End Speech Recognition.
<a class="reference external" href="https://arxiv.org/abs/2005.09310">https://arxiv.org/abs/2005.09310</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>probabilities</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The predicted probabilities from the student model.
Format is [batch, frames, p]</li>
<li><strong>targets</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The target probabilities from the teacher model.
Format is [batch, frames, p]</li>
<li><strong>rel_lab_lengths</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – Length of each utterance, if the frame-level loss is desired.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rel_lab_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nll_loss_kd</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">rel_lab_lengths</span><span class="p">)</span>
<span class="go">tensor(-0.7400)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="speechbrain.nnet.normalization.html" class="btn btn-neutral float-right" title="speechbrain.nnet.normalization module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="speechbrain.nnet.linear.html" class="btn btn-neutral float-left" title="speechbrain.nnet.linear module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, SpeechBrain

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>