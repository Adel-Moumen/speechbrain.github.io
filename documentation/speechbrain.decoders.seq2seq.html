

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>speechbrain.decoders.seq2seq module &mdash; SpeechBrain 0.0.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="speechbrain.decoders.transducer module" href="speechbrain.decoders.transducer.html" />
    <link rel="prev" title="speechbrain.decoders.ctc module" href="speechbrain.decoders.ctc.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> SpeechBrain
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting started:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Quick installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="installation.html#install-via-pypi">Install via PyPI</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#install-locally">Install locally</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#test-installation">Test Installation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="experiment.html">Running an experiment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="experiment.html#yaml-basics">YAML basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="experiment.html#running-arguments">Running arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="experiment.html#tensor-format">Tensor format</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="multigpu.html">Basics of multi-GPU</a><ul>
<li class="toctree-l2"><a class="reference internal" href="multigpu.html#multi-gpu-training-using-data-parallel">Multi-GPU training using Data Parallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="multigpu.html#multi-gpu-training-using-distributed-data-parallel-ddp">Multi-GPU training using Distributed Data Parallel (DDP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="multigpu.html#with-multiple-machines-suppose-you-have-2-servers-with-2-gpus">With multiple machines (suppose you have 2 servers with 2 GPUs):</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#zen-of-speechbrain">Zen of Speechbrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#how-to-get-your-code-in-speechbrain">How to get your code in SpeechBrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#python">Python</a><ul>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#version">Version</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#formatting">Formatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#adding-dependencies">Adding dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#testing">Testing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#documentation">Documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#development-tools">Development tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#flake8">flake8</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#pre-commit">pre-commit</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#the-git-pre-commit-hooks">the git pre-commit hooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#the-git-pre-push-hooks">the git pre-push hooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#pytest-doctests">pytest doctests</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#continuous-integration">Continuous integration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#what-is-ci">What is CI?</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#ci-cd-pipelines">CI / CD Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#our-test-suite">Our test suite</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#pull-request-review-guide">Pull Request review guide</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Documentation:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="speechbrain.html">Core library (speechbrain)</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="speechbrain.core.html">speechbrain.core module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.core.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.core.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.yaml.html">speechbrain.yaml module</a></li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.alignment.html">speechbrain.alignment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.alignment.aligner.html">speechbrain.alignment.aligner module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.alignment.aligner.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.alignment.aligner.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.dataio.html">speechbrain.dataio</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.batch.html">speechbrain.dataio.batch module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.batch.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.batch.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.dataio.html">speechbrain.dataio.dataio module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.dataio.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.dataio.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.dataloader.html">speechbrain.dataio.dataloader module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.dataloader.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.dataloader.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.dataset.html">speechbrain.dataio.dataset module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.dataset.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.dataset.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.encoder.html">speechbrain.dataio.encoder module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.encoder.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.encoder.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.legacy.html">speechbrain.dataio.legacy module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.legacy.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.legacy.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.sampler.html">speechbrain.dataio.sampler module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.sampler.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.sampler.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.wer.html">speechbrain.dataio.wer module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.wer.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.dataio.wer.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.dataio.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="speechbrain.decoders.html">speechbrain.decoders</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="speechbrain.decoders.ctc.html">speechbrain.decoders.ctc module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.decoders.ctc.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.decoders.ctc.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">speechbrain.decoders.seq2seq module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.decoders.transducer.html">speechbrain.decoders.transducer module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.decoders.transducer.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.decoders.transducer.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.lm.html">speechbrain.lm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.lm.arpa.html">speechbrain.lm.arpa module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lm.arpa.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lm.arpa.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.lm.counting.html">speechbrain.lm.counting module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lm.counting.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lm.counting.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.lm.ngram.html">speechbrain.lm.ngram module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lm.ngram.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lm.ngram.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.lobes.html">speechbrain.lobes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.lobes.augment.html">speechbrain.lobes.augment module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.augment.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.augment.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.lobes.features.html">speechbrain.lobes.features module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.features.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.features.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.lobes.models.html">speechbrain.lobes.models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.CRDNN.html">speechbrain.lobes.models.CRDNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.ContextNet.html">speechbrain.lobes.models.ContextNet module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.ECAPA_TDNN.html">speechbrain.lobes.models.ECAPA_TDNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.ESPnetVGG.html">speechbrain.lobes.models.ESPnetVGG module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.MetricGAN_D.html">speechbrain.lobes.models.MetricGAN_D module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.RNNLM.html">speechbrain.lobes.models.RNNLM module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.VanillaNN.html">speechbrain.lobes.models.VanillaNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.Xvector.html">speechbrain.lobes.models.Xvector module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.conv_tasnet.html">speechbrain.lobes.models.conv_tasnet module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.convolution.html">speechbrain.lobes.models.convolution module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.dual_path.html">speechbrain.lobes.models.dual_path module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.lobes.models.transformer.html">speechbrain.lobes.models.transformer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.nnet.html">speechbrain.nnet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.CNN.html">speechbrain.nnet.CNN module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.CNN.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.CNN.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.RNN.html">speechbrain.nnet.RNN module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.RNN.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.RNN.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.activations.html">speechbrain.nnet.activations module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.activations.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.activations.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.attention.html">speechbrain.nnet.attention module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.attention.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.attention.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.containers.html">speechbrain.nnet.containers module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.containers.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.containers.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.dropout.html">speechbrain.nnet.dropout module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.dropout.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.dropout.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.embedding.html">speechbrain.nnet.embedding module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.embedding.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.embedding.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.linear.html">speechbrain.nnet.linear module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.linear.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.linear.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.losses.html">speechbrain.nnet.losses module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.losses.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.losses.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.normalization.html">speechbrain.nnet.normalization module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.normalization.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.normalization.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.pooling.html">speechbrain.nnet.pooling module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.pooling.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.pooling.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.schedulers.html">speechbrain.nnet.schedulers module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.schedulers.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.schedulers.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.complex_networks.html">speechbrain.nnet.complex_networks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.complex_networks.c_CNN.html">speechbrain.nnet.complex_networks.c_CNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.complex_networks.c_RNN.html">speechbrain.nnet.complex_networks.c_RNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.complex_networks.c_linear.html">speechbrain.nnet.complex_networks.c_linear module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.complex_networks.c_normalization.html">speechbrain.nnet.complex_networks.c_normalization module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.complex_networks.c_ops.html">speechbrain.nnet.complex_networks.c_ops module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.loss.html">speechbrain.nnet.loss</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.loss.stoi_loss.html">speechbrain.nnet.loss.stoi_loss module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.loss.transducer_loss.html">speechbrain.nnet.loss.transducer_loss module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.quaternion_networks.html">speechbrain.nnet.quaternion_networks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.quaternion_networks.q_CNN.html">speechbrain.nnet.quaternion_networks.q_CNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.quaternion_networks.q_RNN.html">speechbrain.nnet.quaternion_networks.q_RNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.quaternion_networks.q_linear.html">speechbrain.nnet.quaternion_networks.q_linear module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.quaternion_networks.q_normalization.html">speechbrain.nnet.quaternion_networks.q_normalization module</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.quaternion_networks.q_ops.html">speechbrain.nnet.quaternion_networks.q_ops module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.transducer.html">speechbrain.nnet.transducer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.nnet.transducer.transducer_joint.html">speechbrain.nnet.transducer.transducer_joint module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.nnet.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.processing.html">speechbrain.processing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.processing.NMF.html">speechbrain.processing.NMF module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.NMF.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.NMF.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.processing.PLDA_LDA.html">speechbrain.processing.PLDA_LDA module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.PLDA_LDA.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.PLDA_LDA.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.processing.decomposition.html">speechbrain.processing.decomposition module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.decomposition.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.decomposition.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.processing.diarization.html">speechbrain.processing.diarization module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.diarization.html#reference">Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.diarization.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.diarization.html#id1">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.processing.features.html">speechbrain.processing.features module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.features.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.features.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.processing.multi_mic.html">speechbrain.processing.multi_mic module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.multi_mic.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.multi_mic.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.processing.signal_processing.html">speechbrain.processing.signal_processing module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.signal_processing.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.signal_processing.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.processing.speech_augmentation.html">speechbrain.processing.speech_augmentation module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.speech_augmentation.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.processing.speech_augmentation.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.tokenizers.html">speechbrain.tokenizers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.tokenizers.SentencePiece.html">speechbrain.tokenizers.SentencePiece module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.tokenizers.SentencePiece.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.tokenizers.SentencePiece.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.utils.html">speechbrain.utils</a><ul>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.Accuracy.html">speechbrain.utils.Accuracy module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.Accuracy.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.Accuracy.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.DER.html">speechbrain.utils.DER module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.DER.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.DER.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.checkpoints.html">speechbrain.utils.checkpoints module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.checkpoints.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.checkpoints.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.convert_model.html">speechbrain.utils.convert_model module</a></li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.data_pipeline.html">speechbrain.utils.data_pipeline module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.data_pipeline.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.data_pipeline.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.data_utils.html">speechbrain.utils.data_utils module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.data_utils.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.data_utils.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.depgraph.html">speechbrain.utils.depgraph module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.depgraph.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.depgraph.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.distributed.html">speechbrain.utils.distributed module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.distributed.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.distributed.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.edit_distance.html">speechbrain.utils.edit_distance module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.edit_distance.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.edit_distance.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.epoch_loop.html">speechbrain.utils.epoch_loop module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.epoch_loop.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.epoch_loop.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.logger.html">speechbrain.utils.logger module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.logger.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.logger.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.metric_stats.html">speechbrain.utils.metric_stats module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.metric_stats.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.metric_stats.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.parameter_transfer.html">speechbrain.utils.parameter_transfer module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.parameter_transfer.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.parameter_transfer.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.superpowers.html">speechbrain.utils.superpowers module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.superpowers.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.superpowers.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.train_logger.html">speechbrain.utils.train_logger module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.train_logger.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="speechbrain.utils.train_logger.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="speechbrain.utils.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="speechbrain.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Runnable Tools (tools)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tools.compute_wer.html">tools.compute_wer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="tools.compute_wer.html#usage">Usage</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">SpeechBrain</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="speechbrain.html">speechbrain</a> &raquo;</li>
        
          <li><a href="speechbrain.decoders.html">speechbrain.decoders</a> &raquo;</li>
        
      <li>speechbrain.decoders.seq2seq module</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/speechbrain.decoders.seq2seq.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-speechbrain.decoders.seq2seq">
<span id="speechbrain-decoders-seq2seq-module"></span><h1>speechbrain.decoders.seq2seq module<a class="headerlink" href="#module-speechbrain.decoders.seq2seq" title="Permalink to this headline">¶</a></h1>
<p>Decoding methods for seq2seq autoregressive model.</p>
<dl class="docutils">
<dt>Authors</dt><dd><ul class="simple">
<li>Ju-Chieh Chou 2020</li>
<li>Peter Plantinga 2020</li>
<li>Mirco Ravanelli 2020</li>
<li>Sung-Lin Yeh 2020</li>
</ul>
</dd>
</dl>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>Classes:</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#speechbrain.decoders.seq2seq.S2SBaseSearcher" title="speechbrain.decoders.seq2seq.S2SBaseSearcher"><code class="xref py py-obj docutils literal notranslate"><span class="pre">S2SBaseSearcher</span></code></a></td>
<td>S2SBaseSearcher class to be inherited by other decoding approches for seq2seq model.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#speechbrain.decoders.seq2seq.S2SBeamSearcher" title="speechbrain.decoders.seq2seq.S2SBeamSearcher"><code class="xref py py-obj docutils literal notranslate"><span class="pre">S2SBeamSearcher</span></code></a></td>
<td>This class implements the beam-search algorithm for the seq2seq model.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#speechbrain.decoders.seq2seq.S2SGreedySearcher" title="speechbrain.decoders.seq2seq.S2SGreedySearcher"><code class="xref py py-obj docutils literal notranslate"><span class="pre">S2SGreedySearcher</span></code></a></td>
<td>This class implements the general forward-pass of greedy decoding approach.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearchLM" title="speechbrain.decoders.seq2seq.S2SRNNBeamSearchLM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">S2SRNNBeamSearchLM</span></code></a></td>
<td>This class implements the beam search decoding for AttentionalRNNDecoder (speechbrain/nnet/RNN.py) with LM.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearchTransformerLM" title="speechbrain.decoders.seq2seq.S2SRNNBeamSearchTransformerLM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">S2SRNNBeamSearchTransformerLM</span></code></a></td>
<td>This class implements the beam search decoding for AttentionalRNNDecoder (speechbrain/nnet/RNN.py) with LM.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearcher" title="speechbrain.decoders.seq2seq.S2SRNNBeamSearcher"><code class="xref py py-obj docutils literal notranslate"><span class="pre">S2SRNNBeamSearcher</span></code></a></td>
<td>This class implements the beam search decoding for AttentionalRNNDecoder (speechbrain/nnet/RNN.py).</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#speechbrain.decoders.seq2seq.S2SRNNGreedySearcher" title="speechbrain.decoders.seq2seq.S2SRNNGreedySearcher"><code class="xref py py-obj docutils literal notranslate"><span class="pre">S2SRNNGreedySearcher</span></code></a></td>
<td>This class implements the greedy decoding for AttentionalRNNDecoder (speechbrain/nnet/RNN.py).</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#speechbrain.decoders.seq2seq.S2STransformerBeamSearch" title="speechbrain.decoders.seq2seq.S2STransformerBeamSearch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">S2STransformerBeamSearch</span></code></a></td>
<td>This class implements the beam search decoding for Transformer.</td>
</tr>
</tbody>
</table>
<p>Functions:</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#speechbrain.decoders.seq2seq.batch_filter_seq2seq_output" title="speechbrain.decoders.seq2seq.batch_filter_seq2seq_output"><code class="xref py py-obj docutils literal notranslate"><span class="pre">batch_filter_seq2seq_output</span></code></a></td>
<td>Calling batch_size times of filter_seq2seq_output.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#speechbrain.decoders.seq2seq.filter_seq2seq_output" title="speechbrain.decoders.seq2seq.filter_seq2seq_output"><code class="xref py py-obj docutils literal notranslate"><span class="pre">filter_seq2seq_output</span></code></a></td>
<td>Filter the output until the first eos occurs (exclusive).</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#speechbrain.decoders.seq2seq.inflate_tensor" title="speechbrain.decoders.seq2seq.inflate_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inflate_tensor</span></code></a></td>
<td>This function inflates the tensor for times along dim.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#speechbrain.decoders.seq2seq.mask_by_condition" title="speechbrain.decoders.seq2seq.mask_by_condition"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mask_by_condition</span></code></a></td>
<td>This function will mask some element in the tensor with fill_value, if condition=False.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="speechbrain.decoders.seq2seq.S2SBaseSearcher">
<em class="property">class </em><code class="descclassname">speechbrain.decoders.seq2seq.</code><code class="descname">S2SBaseSearcher</code><span class="sig-paren">(</span><em><span class="n">bos_index</span></em>, <em><span class="n">eos_index</span></em>, <em><span class="n">min_decode_ratio</span></em>, <em><span class="n">max_decode_ratio</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SBaseSearcher"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SBaseSearcher" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>S2SBaseSearcher class to be inherited by other
decoding approches for seq2seq model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>bos_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – The index of the beginning-of-sequence (bos) token.</li>
<li><strong>eos_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – The index of end-of-sequence token.</li>
<li><strong>min_decode_radio</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – The ratio of minimum decoding steps to the length of encoder states.</li>
<li><strong>max_decode_radio</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – The ratio of maximum decoding steps to the length of encoder states.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><em>predictions</em> – Outputs as Python list of lists, with “ragged” dimensions; padding
has been removed.</li>
<li><em>scores</em> – The sum of log probabilities (and possibly
additional heuristic scores) for each prediction.</li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SBaseSearcher.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em><span class="n">enc_states</span></em>, <em><span class="n">wav_len</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SBaseSearcher.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SBaseSearcher.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>This method should implement the forward algorithm of decoding method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>enc_states</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The precomputed encoder states to be used when decoding.
(ex. the encoded speech representation to be attended).</li>
<li><strong>wav_len</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The speechbrain-style relative length.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SBaseSearcher.forward_step">
<code class="descname">forward_step</code><span class="sig-paren">(</span><em><span class="n">inp_tokens</span></em>, <em><span class="n">memory</span></em>, <em><span class="n">enc_states</span></em>, <em><span class="n">enc_lens</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SBaseSearcher.forward_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SBaseSearcher.forward_step" title="Permalink to this definition">¶</a></dt>
<dd><p>This method should implement one step of
forwarding operation in the autoregressive model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>inp_tokens</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The input tensor of the current timestep.</li>
<li><strong>memory</strong> (<em>No limit</em>) – The memory variables input for this timestep.
(ex. RNN hidden states).</li>
<li><strong>enc_states</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The encoder states to be attended.</li>
<li><strong>enc_lens</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The actual length of each enc_states sequence.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><strong>log_probs</strong> (<em>torch.Tensor</em>) – Log-probabilities of the current timestep output.</li>
<li><strong>memory</strong> (<em>No limit</em>) – The memory variables generated in this timestep.
(ex. RNN hidden states).</li>
<li><strong>attn</strong> (<em>torch.Tensor</em>) – The attention weight for doing penalty.</li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SBaseSearcher.reset_mem">
<code class="descname">reset_mem</code><span class="sig-paren">(</span><em><span class="n">batch_size</span></em>, <em><span class="n">device</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SBaseSearcher.reset_mem"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SBaseSearcher.reset_mem" title="Permalink to this definition">¶</a></dt>
<dd><p>This method should implement the resetting of
memory variables for the seq2seq model.
E.g., initializing zero vector as initial hidden states.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – The size of the batch.</li>
<li><strong>device</strong> (<em>torch.device</em>) – The device to put the initial variables.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>memory</strong> – The initial memory variable.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">No limit</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SBaseSearcher.lm_forward_step">
<code class="descname">lm_forward_step</code><span class="sig-paren">(</span><em><span class="n">inp_tokens</span></em>, <em><span class="n">memory</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SBaseSearcher.lm_forward_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SBaseSearcher.lm_forward_step" title="Permalink to this definition">¶</a></dt>
<dd><p>This method should implement one step of
forwarding operation for language model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>inp_tokens</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The input tensor of the current timestep.</li>
<li><strong>memory</strong> (<em>No limit</em>) – The momory variables input for this timestep.
(e.g., RNN hidden states).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><strong>log_probs</strong> (<em>torch.Tensor</em>) – Log-probabilities of the current timestep output.</li>
<li><strong>memory</strong> (<em>No limit</em>) – The memory variables generated in this timestep.
(e.g., RNN hidden states).</li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SBaseSearcher.reset_lm_mem">
<code class="descname">reset_lm_mem</code><span class="sig-paren">(</span><em><span class="n">batch_size</span></em>, <em><span class="n">device</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SBaseSearcher.reset_lm_mem"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SBaseSearcher.reset_lm_mem" title="Permalink to this definition">¶</a></dt>
<dd><p>This method should implement the resetting of
memory variables in the language model.
E.g., initializing zero vector as initial hidden states.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – The size of the batch.</li>
<li><strong>device</strong> (<em>torch.device</em>) – The device to put the initial variables.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>memory</strong> – The initial memory variable.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">No limit</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py attribute">
<dt id="speechbrain.decoders.seq2seq.S2SBaseSearcher.training">
<code class="descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a></em><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SBaseSearcher.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="speechbrain.decoders.seq2seq.S2SGreedySearcher">
<em class="property">class </em><code class="descclassname">speechbrain.decoders.seq2seq.</code><code class="descname">S2SGreedySearcher</code><span class="sig-paren">(</span><em><span class="n">bos_index</span></em>, <em><span class="n">eos_index</span></em>, <em><span class="n">min_decode_ratio</span></em>, <em><span class="n">max_decode_ratio</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SGreedySearcher"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SGreedySearcher" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#speechbrain.decoders.seq2seq.S2SBaseSearcher" title="speechbrain.decoders.seq2seq.S2SBaseSearcher"><code class="xref py py-class docutils literal notranslate"><span class="pre">speechbrain.decoders.seq2seq.S2SBaseSearcher</span></code></a></p>
<p>This class implements the general forward-pass of
greedy decoding approach. See also S2SBaseSearcher().</p>
<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SGreedySearcher.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em><span class="n">enc_states</span></em>, <em><span class="n">wav_len</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SGreedySearcher.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SGreedySearcher.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="speechbrain.decoders.seq2seq.S2SGreedySearcher.training">
<code class="descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a></em><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SGreedySearcher.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="speechbrain.decoders.seq2seq.S2SRNNGreedySearcher">
<em class="property">class </em><code class="descclassname">speechbrain.decoders.seq2seq.</code><code class="descname">S2SRNNGreedySearcher</code><span class="sig-paren">(</span><em><span class="n">embedding</span></em>, <em><span class="n">decoder</span></em>, <em><span class="n">linear</span></em>, <em><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SRNNGreedySearcher"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNGreedySearcher" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#speechbrain.decoders.seq2seq.S2SGreedySearcher" title="speechbrain.decoders.seq2seq.S2SGreedySearcher"><code class="xref py py-class docutils literal notranslate"><span class="pre">speechbrain.decoders.seq2seq.S2SGreedySearcher</span></code></a></p>
<p>This class implements the greedy decoding
for AttentionalRNNDecoder (speechbrain/nnet/RNN.py).
See also S2SBaseSearcher() and S2SGreedySearcher().</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>embedding</strong> (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.nn.Module</em></a>) – An embedding layer.</li>
<li><strong>decoder</strong> (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.nn.Module</em></a>) – Attentional RNN decoder.</li>
<li><strong>linear</strong> (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.nn.Module</em></a>) – A linear output layer.</li>
<li><strong>**kwargs</strong> – see S2SBaseSearcher, arguments are directly passed.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span> <span class="o">=</span> <span class="n">sb</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">RNN</span><span class="o">.</span><span class="n">AttentionalRNNDecoder</span><span class="p">(</span>
<span class="gp">... </span>    <span class="s2">&quot;gru&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">enc_dim</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">3</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lin</span> <span class="o">=</span> <span class="n">sb</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_neurons</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">searcher</span> <span class="o">=</span> <span class="n">S2SRNNGreedySearcher</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">embedding</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">decoder</span><span class="o">=</span><span class="n">dec</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">linear</span><span class="o">=</span><span class="n">lin</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">bos_index</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">eos_index</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_decode_ratio</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_decode_ratio</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wav_len</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hyps</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">searcher</span><span class="p">(</span><span class="n">enc</span><span class="p">,</span> <span class="n">wav_len</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SRNNGreedySearcher.reset_mem">
<code class="descname">reset_mem</code><span class="sig-paren">(</span><em><span class="n">batch_size</span></em>, <em><span class="n">device</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SRNNGreedySearcher.reset_mem"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNGreedySearcher.reset_mem" title="Permalink to this definition">¶</a></dt>
<dd><p>When doing greedy search, keep hidden state (hs) adn context vector (c)
as memory.</p>
</dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SRNNGreedySearcher.forward_step">
<code class="descname">forward_step</code><span class="sig-paren">(</span><em><span class="n">inp_tokens</span></em>, <em><span class="n">memory</span></em>, <em><span class="n">enc_states</span></em>, <em><span class="n">enc_lens</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SRNNGreedySearcher.forward_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNGreedySearcher.forward_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="speechbrain.decoders.seq2seq.S2SRNNGreedySearcher.training">
<code class="descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a></em><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNGreedySearcher.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="speechbrain.decoders.seq2seq.S2SBeamSearcher">
<em class="property">class </em><code class="descclassname">speechbrain.decoders.seq2seq.</code><code class="descname">S2SBeamSearcher</code><span class="sig-paren">(</span><em><span class="n">bos_index</span></em>, <em><span class="n">eos_index</span></em>, <em><span class="n">min_decode_ratio</span></em>, <em><span class="n">max_decode_ratio</span></em>, <em><span class="n">beam_size</span></em>, <em><span class="n">topk</span><span class="o">=</span><span class="default_value">1</span></em>, <em><span class="n">return_log_probs</span><span class="o">=</span><span class="default_value">False</span></em>, <em><span class="n">using_eos_threshold</span><span class="o">=</span><span class="default_value">True</span></em>, <em><span class="n">eos_threshold</span><span class="o">=</span><span class="default_value">1.5</span></em>, <em><span class="n">length_normalization</span><span class="o">=</span><span class="default_value">True</span></em>, <em><span class="n">length_rewarding</span><span class="o">=</span><span class="default_value">0</span></em>, <em><span class="n">coverage_penalty</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em><span class="n">lm_weight</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em><span class="n">lm_modules</span><span class="o">=</span><span class="default_value">None</span></em>, <em><span class="n">ctc_weight</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em><span class="n">blank_index</span><span class="o">=</span><span class="default_value">0</span></em>, <em><span class="n">ctc_score_mode</span><span class="o">=</span><span class="default_value">'full'</span></em>, <em><span class="n">ctc_window_size</span><span class="o">=</span><span class="default_value">0</span></em>, <em><span class="n">using_max_attn_shift</span><span class="o">=</span><span class="default_value">False</span></em>, <em><span class="n">max_attn_shift</span><span class="o">=</span><span class="default_value">60</span></em>, <em><span class="n">minus_inf</span><span class="o">=</span><span class="default_value">- 1e+20</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SBeamSearcher"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SBeamSearcher" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#speechbrain.decoders.seq2seq.S2SBaseSearcher" title="speechbrain.decoders.seq2seq.S2SBaseSearcher"><code class="xref py py-class docutils literal notranslate"><span class="pre">speechbrain.decoders.seq2seq.S2SBaseSearcher</span></code></a></p>
<p>This class implements the beam-search algorithm for the seq2seq model.
See also S2SBaseSearcher().</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>bos_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – The index of beginning-of-sequence token.</li>
<li><strong>eos_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – The index of end-of-sequence token.</li>
<li><strong>min_decode_radio</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – The ratio of minimum decoding steps to length of encoder states.</li>
<li><strong>max_decode_radio</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – The ratio of maximum decoding steps to length of encoder states.</li>
<li><strong>beam_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – The width of beam.</li>
<li><strong>topk</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – The number of hypothesis to return. (default: 1)</li>
<li><strong>return_log_probs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a>) – Whether to return log-probabilities. (default: False)</li>
<li><strong>using_eos_threshold</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a>) – Whether to use eos threshold. (default: true)</li>
<li><strong>eos_threshold</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – The threshold coefficient for eos token (default: 1.5). See 3.1.2 in
reference: <a class="reference external" href="https://arxiv.org/abs/1904.02619">https://arxiv.org/abs/1904.02619</a></li>
<li><strong>length_normlization</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a>) – Whether to divide the scores by the length. (default: True)</li>
<li><strong>length_rewarding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – The coefficient of length rewarding (γ).
log P(y|x) + λ log P_LM(y) + γ*len(y). (default: 0.0)</li>
<li><strong>coverage_penalty</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – The coefficient of coverage penalty (η).
log P(y|x) + λ log P_LM(y) + γ*len(y) + η*coverage(x,y). (default: 0.0)
Reference: <a class="reference external" href="https://arxiv.org/pdf/1612.02695.pdf">https://arxiv.org/pdf/1612.02695.pdf</a>, <a class="reference external" href="https://arxiv.org/pdf/1808.10792.pdf">https://arxiv.org/pdf/1808.10792.pdf</a></li>
<li><strong>lm_weight</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – The weight of LM when performing beam search (λ).
log P(y|x) + λ log P_LM(y). (default: 0.0)</li>
<li><strong>ctc_weight</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – The weight of CTC probabilities when performing beam search (λ).
(1-λ) log P(y|x) + λ log P_CTC(y|x). (default: 0.0)</li>
<li><strong>blank_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – The index of the blank token.</li>
<li><strong>ctc_score_mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)"><em>str</em></a>) – Default: “full”
CTC prefix scoring on “partial” token or “full: token.</li>
<li><strong>ctc_window_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – Default: 0
Compute the ctc scores over the time frames using windowing based on attention peaks.
If 0, no windowing applied.</li>
<li><strong>using_max_attn_shift</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><em>bool</em></a>) – Whether using the max_attn_shift constaint. (default: False)</li>
<li><strong>max_attn_shift</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – Beam search will block the beams that attention shift more
than max_attn_shift.
Reference: <a class="reference external" href="https://arxiv.org/abs/1904.02619">https://arxiv.org/abs/1904.02619</a></li>
<li><strong>minus_inf</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – DefaultL -1e20
The value of minus infinity to block some path
of the search.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SBeamSearcher.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em><span class="n">enc_states</span></em>, <em><span class="n">wav_len</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SBeamSearcher.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SBeamSearcher.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SBeamSearcher.ctc_forward_step">
<code class="descname">ctc_forward_step</code><span class="sig-paren">(</span><em><span class="n">x</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SBeamSearcher.ctc_forward_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SBeamSearcher.ctc_forward_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SBeamSearcher.permute_mem">
<code class="descname">permute_mem</code><span class="sig-paren">(</span><em><span class="n">memory</span></em>, <em><span class="n">index</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SBeamSearcher.permute_mem"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SBeamSearcher.permute_mem" title="Permalink to this definition">¶</a></dt>
<dd><p>This method permutes the seq2seq model memory
to synchronize the memory index with the current output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>memory</strong> (<em>No limit</em>) – The memory variable to be permuted.</li>
<li><strong>index</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The index of the previous path.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">The variable of the memory being permuted.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SBeamSearcher.permute_lm_mem">
<code class="descname">permute_lm_mem</code><span class="sig-paren">(</span><em><span class="n">memory</span></em>, <em><span class="n">index</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SBeamSearcher.permute_lm_mem"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SBeamSearcher.permute_lm_mem" title="Permalink to this definition">¶</a></dt>
<dd><p>This method permutes the language model memory
to synchronize the memory index with the current output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>memory</strong> (<em>No limit</em>) – The memory variable to be permuted.</li>
<li><strong>index</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The index of the previous path.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">The variable of the memory being permuted.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="py attribute">
<dt id="speechbrain.decoders.seq2seq.S2SBeamSearcher.training">
<code class="descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a></em><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SBeamSearcher.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="speechbrain.decoders.seq2seq.S2SRNNBeamSearcher">
<em class="property">class </em><code class="descclassname">speechbrain.decoders.seq2seq.</code><code class="descname">S2SRNNBeamSearcher</code><span class="sig-paren">(</span><em><span class="n">embedding</span></em>, <em><span class="n">decoder</span></em>, <em><span class="n">linear</span></em>, <em><span class="n">ctc_linear</span><span class="o">=</span><span class="default_value">None</span></em>, <em><span class="n">temperature</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SRNNBeamSearcher"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearcher" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#speechbrain.decoders.seq2seq.S2SBeamSearcher" title="speechbrain.decoders.seq2seq.S2SBeamSearcher"><code class="xref py py-class docutils literal notranslate"><span class="pre">speechbrain.decoders.seq2seq.S2SBeamSearcher</span></code></a></p>
<p>This class implements the beam search decoding
for AttentionalRNNDecoder (speechbrain/nnet/RNN.py).
See also S2SBaseSearcher(), S2SBeamSearcher().</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>embedding</strong> (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.nn.Module</em></a>) – An embedding layer.</li>
<li><strong>decoder</strong> (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.nn.Module</em></a>) – Attentional RNN decoder.</li>
<li><strong>linear</strong> (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.nn.Module</em></a>) – A linear output layer.</li>
<li><strong>temperature</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – Temperature factor applied to softmax. It changes the probability
distribution, being softer when T&gt;1 and sharper with T&lt;1.</li>
<li><strong>**kwargs</strong> – see S2SBeamSearcher, arguments are directly passed.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span> <span class="o">=</span> <span class="n">sb</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">RNN</span><span class="o">.</span><span class="n">AttentionalRNNDecoder</span><span class="p">(</span>
<span class="gp">... </span>    <span class="s2">&quot;gru&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">enc_dim</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">3</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lin</span> <span class="o">=</span> <span class="n">sb</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_neurons</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ctc_lin</span> <span class="o">=</span> <span class="n">sb</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_neurons</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">searcher</span> <span class="o">=</span> <span class="n">S2SRNNBeamSearcher</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">embedding</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">decoder</span><span class="o">=</span><span class="n">dec</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">linear</span><span class="o">=</span><span class="n">lin</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">ctc_linear</span><span class="o">=</span><span class="n">ctc_lin</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">bos_index</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">eos_index</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">blank_index</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_decode_ratio</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_decode_ratio</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">beam_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wav_len</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hyps</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">searcher</span><span class="p">(</span><span class="n">enc</span><span class="p">,</span> <span class="n">wav_len</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SRNNBeamSearcher.reset_mem">
<code class="descname">reset_mem</code><span class="sig-paren">(</span><em><span class="n">batch_size</span></em>, <em><span class="n">device</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SRNNBeamSearcher.reset_mem"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearcher.reset_mem" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SRNNBeamSearcher.forward_step">
<code class="descname">forward_step</code><span class="sig-paren">(</span><em><span class="n">inp_tokens</span></em>, <em><span class="n">memory</span></em>, <em><span class="n">enc_states</span></em>, <em><span class="n">enc_lens</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SRNNBeamSearcher.forward_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearcher.forward_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SRNNBeamSearcher.permute_mem">
<code class="descname">permute_mem</code><span class="sig-paren">(</span><em><span class="n">memory</span></em>, <em><span class="n">index</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SRNNBeamSearcher.permute_mem"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearcher.permute_mem" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="speechbrain.decoders.seq2seq.S2SRNNBeamSearcher.training">
<code class="descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a></em><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearcher.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="speechbrain.decoders.seq2seq.S2SRNNBeamSearchLM">
<em class="property">class </em><code class="descclassname">speechbrain.decoders.seq2seq.</code><code class="descname">S2SRNNBeamSearchLM</code><span class="sig-paren">(</span><em><span class="n">embedding</span></em>, <em><span class="n">decoder</span></em>, <em><span class="n">linear</span></em>, <em><span class="n">language_model</span></em>, <em><span class="n">temperature_lm</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SRNNBeamSearchLM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearchLM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearcher" title="speechbrain.decoders.seq2seq.S2SRNNBeamSearcher"><code class="xref py py-class docutils literal notranslate"><span class="pre">speechbrain.decoders.seq2seq.S2SRNNBeamSearcher</span></code></a></p>
<p>This class implements the beam search decoding
for AttentionalRNNDecoder (speechbrain/nnet/RNN.py) with LM.
See also S2SBaseSearcher(), S2SBeamSearcher(), S2SRNNBeamSearcher().</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>embedding</strong> (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.nn.Module</em></a>) – An embedding layer.</li>
<li><strong>decoder</strong> (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.nn.Module</em></a>) – Attentional RNN decoder.</li>
<li><strong>linear</strong> (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.nn.Module</em></a>) – A linear output layer.</li>
<li><strong>language_model</strong> (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.nn.Module</em></a>) – A language model.</li>
<li><strong>temperature_lm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – Temperature factor applied to softmax. It changes the probability
distribution, being softer when T&gt;1 and sharper with T&lt;1.</li>
<li><strong>**kwargs</strong> – Arguments to pass to S2SBeamSearcher.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">speechbrain.lobes.models.RNNLM</span> <span class="k">import</span> <span class="n">RNNLM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span> <span class="o">=</span> <span class="n">sb</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">RNN</span><span class="o">.</span><span class="n">AttentionalRNNDecoder</span><span class="p">(</span>
<span class="gp">... </span>    <span class="s2">&quot;gru&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">enc_dim</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">3</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lin</span> <span class="o">=</span> <span class="n">sb</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_neurons</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span> <span class="o">=</span> <span class="n">RNNLM</span><span class="p">(</span><span class="n">output_neurons</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">return_hidden</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">searcher</span> <span class="o">=</span> <span class="n">S2SRNNBeamSearchLM</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">embedding</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">decoder</span><span class="o">=</span><span class="n">dec</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">linear</span><span class="o">=</span><span class="n">lin</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">language_model</span><span class="o">=</span><span class="n">lm</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">bos_index</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">eos_index</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">blank_index</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_decode_ratio</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_decode_ratio</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">beam_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">lm_weight</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wav_len</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hyps</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">searcher</span><span class="p">(</span><span class="n">enc</span><span class="p">,</span> <span class="n">wav_len</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SRNNBeamSearchLM.lm_forward_step">
<code class="descname">lm_forward_step</code><span class="sig-paren">(</span><em><span class="n">inp_tokens</span></em>, <em><span class="n">memory</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SRNNBeamSearchLM.lm_forward_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearchLM.lm_forward_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SRNNBeamSearchLM.permute_lm_mem">
<code class="descname">permute_lm_mem</code><span class="sig-paren">(</span><em><span class="n">memory</span></em>, <em><span class="n">index</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SRNNBeamSearchLM.permute_lm_mem"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearchLM.permute_lm_mem" title="Permalink to this definition">¶</a></dt>
<dd><p>This is to permute lm memory to synchronize with current index
during beam search. The order of beams will be shuffled by scores
every timestep to allow batched beam search.
Further details please refer to speechbrain/decoder/seq2seq.py.</p>
</dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SRNNBeamSearchLM.reset_lm_mem">
<code class="descname">reset_lm_mem</code><span class="sig-paren">(</span><em><span class="n">batch_size</span></em>, <em><span class="n">device</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SRNNBeamSearchLM.reset_lm_mem"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearchLM.reset_lm_mem" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="speechbrain.decoders.seq2seq.S2SRNNBeamSearchLM.training">
<code class="descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a></em><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearchLM.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="speechbrain.decoders.seq2seq.S2SRNNBeamSearchTransformerLM">
<em class="property">class </em><code class="descclassname">speechbrain.decoders.seq2seq.</code><code class="descname">S2SRNNBeamSearchTransformerLM</code><span class="sig-paren">(</span><em><span class="n">embedding</span></em>, <em><span class="n">decoder</span></em>, <em><span class="n">linear</span></em>, <em><span class="n">language_model</span></em>, <em><span class="n">temperature_lm</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SRNNBeamSearchTransformerLM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearchTransformerLM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearcher" title="speechbrain.decoders.seq2seq.S2SRNNBeamSearcher"><code class="xref py py-class docutils literal notranslate"><span class="pre">speechbrain.decoders.seq2seq.S2SRNNBeamSearcher</span></code></a></p>
<p>This class implements the beam search decoding
for AttentionalRNNDecoder (speechbrain/nnet/RNN.py) with LM.
See also S2SBaseSearcher(), S2SBeamSearcher(), S2SRNNBeamSearcher().</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>embedding</strong> (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.nn.Module</em></a>) – An embedding layer.</li>
<li><strong>decoder</strong> (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.nn.Module</em></a>) – Attentional RNN decoder.</li>
<li><strong>linear</strong> (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.nn.Module</em></a>) – A linear output layer.</li>
<li><strong>language_model</strong> (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.nn.Module</em></a>) – A language model.</li>
<li><strong>temperature_lm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – Temperature factor applied to softmax. It changes the probability
distribution, being softer when T&gt;1 and sharper with T&lt;1.</li>
<li><strong>**kwargs</strong> – Arguments to pass to S2SBeamSearcher.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">speechbrain.lobes.models.transformer.TransformerLM</span> <span class="k">import</span> <span class="n">TransformerLM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span> <span class="o">=</span> <span class="n">sb</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">RNN</span><span class="o">.</span><span class="n">AttentionalRNNDecoder</span><span class="p">(</span>
<span class="gp">... </span>    <span class="s2">&quot;gru&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">enc_dim</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">3</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lin</span> <span class="o">=</span> <span class="n">sb</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_neurons</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span> <span class="o">=</span> <span class="n">TransformerLM</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">searcher</span> <span class="o">=</span> <span class="n">S2SRNNBeamSearchTransformerLM</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">embedding</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">decoder</span><span class="o">=</span><span class="n">dec</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">linear</span><span class="o">=</span><span class="n">lin</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">language_model</span><span class="o">=</span><span class="n">lm</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">bos_index</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">eos_index</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">blank_index</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_decode_ratio</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_decode_ratio</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">beam_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">lm_weight</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wav_len</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hyps</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">searcher</span><span class="p">(</span><span class="n">enc</span><span class="p">,</span> <span class="n">wav_len</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SRNNBeamSearchTransformerLM.lm_forward_step">
<code class="descname">lm_forward_step</code><span class="sig-paren">(</span><em><span class="n">inp_tokens</span></em>, <em><span class="n">memory</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SRNNBeamSearchTransformerLM.lm_forward_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearchTransformerLM.lm_forward_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SRNNBeamSearchTransformerLM.permute_lm_mem">
<code class="descname">permute_lm_mem</code><span class="sig-paren">(</span><em><span class="n">memory</span></em>, <em><span class="n">index</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SRNNBeamSearchTransformerLM.permute_lm_mem"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearchTransformerLM.permute_lm_mem" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2SRNNBeamSearchTransformerLM.reset_lm_mem">
<code class="descname">reset_lm_mem</code><span class="sig-paren">(</span><em><span class="n">batch_size</span></em>, <em><span class="n">device</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2SRNNBeamSearchTransformerLM.reset_lm_mem"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearchTransformerLM.reset_lm_mem" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="speechbrain.decoders.seq2seq.S2SRNNBeamSearchTransformerLM.training">
<code class="descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a></em><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2SRNNBeamSearchTransformerLM.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="speechbrain.decoders.seq2seq.inflate_tensor">
<code class="descclassname">speechbrain.decoders.seq2seq.</code><code class="descname">inflate_tensor</code><span class="sig-paren">(</span><em><span class="n">tensor</span></em>, <em><span class="n">times</span></em>, <em><span class="n">dim</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#inflate_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.inflate_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>This function inflates the tensor for times along dim.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The tensor to be inflated.</li>
<li><strong>times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – The tensor will inflate for this number of times.</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a>) – The dim to be inflated.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">The inflated tensor.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))">torch.Tensor</a></p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_tensor</span> <span class="o">=</span> <span class="n">inflate_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_tensor</span>
<span class="go">tensor([[1., 2., 3.],</span>
<span class="go">        [1., 2., 3.],</span>
<span class="go">        [4., 5., 6.],</span>
<span class="go">        [4., 5., 6.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="speechbrain.decoders.seq2seq.mask_by_condition">
<code class="descclassname">speechbrain.decoders.seq2seq.</code><code class="descname">mask_by_condition</code><span class="sig-paren">(</span><em><span class="n">tensor</span></em>, <em><span class="n">cond</span></em>, <em><span class="n">fill_value</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#mask_by_condition"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.mask_by_condition" title="Permalink to this definition">¶</a></dt>
<dd><p>This function will mask some element in the tensor with fill_value, if condition=False.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor</strong> (<a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.Tensor</em></a>) – The tensor to be masked.</li>
<li><strong>cond</strong> (<em>torch.BoolTensor</em>) – This tensor has to be the same size as tensor.
Each element represents whether to keep the value in tensor.</li>
<li><strong>fill_value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)"><em>float</em></a>) – The value to fill in the masked element.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">The masked tensor.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))">torch.Tensor</a></p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cond</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">([[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask_by_condition</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([[1., 2., 0.],</span>
<span class="go">        [4., 0., 0.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt id="speechbrain.decoders.seq2seq.S2STransformerBeamSearch">
<em class="property">class </em><code class="descclassname">speechbrain.decoders.seq2seq.</code><code class="descname">S2STransformerBeamSearch</code><span class="sig-paren">(</span><em><span class="n">modules</span></em>, <em><span class="n">temperature</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em><span class="n">temperature_lm</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2STransformerBeamSearch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2STransformerBeamSearch" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#speechbrain.decoders.seq2seq.S2SBeamSearcher" title="speechbrain.decoders.seq2seq.S2SBeamSearcher"><code class="xref py py-class docutils literal notranslate"><span class="pre">speechbrain.decoders.seq2seq.S2SBeamSearcher</span></code></a></p>
<p>This class implements the beam search decoding
for Transformer.
See also S2SBaseSearcher(), S2SBeamSearcher().</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>model</strong> (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.nn.Module</em></a>) – The model to use for decoding.</li>
<li><strong>linear</strong> (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch vmaster (1.9.0a0+a9f9377 ))"><em>torch.nn.Module</em></a>) – A linear output layer.</li>
<li><strong>**kwargs</strong> – Arguments to pass to S2SBeamSearcher</li>
<li><strong>Example</strong> – </li>
<li><strong>--------</strong> – </li>
<li><strong># see recipes/LibriSpeech/ASR_transformer/experiment.py</strong> (<em>&gt;&gt;&gt;</em>) – </li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2STransformerBeamSearch.reset_mem">
<code class="descname">reset_mem</code><span class="sig-paren">(</span><em><span class="n">batch_size</span></em>, <em><span class="n">device</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2STransformerBeamSearch.reset_mem"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2STransformerBeamSearch.reset_mem" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2STransformerBeamSearch.reset_lm_mem">
<code class="descname">reset_lm_mem</code><span class="sig-paren">(</span><em><span class="n">batch_size</span></em>, <em><span class="n">device</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2STransformerBeamSearch.reset_lm_mem"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2STransformerBeamSearch.reset_lm_mem" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2STransformerBeamSearch.permute_mem">
<code class="descname">permute_mem</code><span class="sig-paren">(</span><em><span class="n">memory</span></em>, <em><span class="n">index</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2STransformerBeamSearch.permute_mem"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2STransformerBeamSearch.permute_mem" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2STransformerBeamSearch.permute_lm_mem">
<code class="descname">permute_lm_mem</code><span class="sig-paren">(</span><em><span class="n">memory</span></em>, <em><span class="n">index</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2STransformerBeamSearch.permute_lm_mem"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2STransformerBeamSearch.permute_lm_mem" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2STransformerBeamSearch.forward_step">
<code class="descname">forward_step</code><span class="sig-paren">(</span><em><span class="n">inp_tokens</span></em>, <em><span class="n">memory</span></em>, <em><span class="n">enc_states</span></em>, <em><span class="n">enc_lens</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2STransformerBeamSearch.forward_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2STransformerBeamSearch.forward_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="speechbrain.decoders.seq2seq.S2STransformerBeamSearch.lm_forward_step">
<code class="descname">lm_forward_step</code><span class="sig-paren">(</span><em><span class="n">inp_tokens</span></em>, <em><span class="n">memory</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#S2STransformerBeamSearch.lm_forward_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2STransformerBeamSearch.lm_forward_step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="speechbrain.decoders.seq2seq.S2STransformerBeamSearch.training">
<code class="descname">training</code><em class="property">: <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a></em><a class="headerlink" href="#speechbrain.decoders.seq2seq.S2STransformerBeamSearch.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="speechbrain.decoders.seq2seq.batch_filter_seq2seq_output">
<code class="descclassname">speechbrain.decoders.seq2seq.</code><code class="descname">batch_filter_seq2seq_output</code><span class="sig-paren">(</span><em><span class="n">prediction</span></em>, <em><span class="n">eos_id</span><span class="o">=</span><span class="default_value">- 1</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#batch_filter_seq2seq_output"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.batch_filter_seq2seq_output" title="Permalink to this definition">¶</a></dt>
<dd><p>Calling batch_size times of filter_seq2seq_output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>prediction</strong> (<em>list of torch.Tensor</em>) – A list containing the output ints predicted by the seq2seq system.</li>
<li><strong>eos_id</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em>, </em><em>string</em>) – The id of the eos.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">The output predicted by seq2seq model.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)">list</a></p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predictions</span> <span class="o">=</span> <span class="n">batch_filter_seq2seq_output</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">eos_id</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predictions</span>
<span class="go">[[1, 2, 3], [2, 3]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="speechbrain.decoders.seq2seq.filter_seq2seq_output">
<code class="descclassname">speechbrain.decoders.seq2seq.</code><code class="descname">filter_seq2seq_output</code><span class="sig-paren">(</span><em><span class="n">string_pred</span></em>, <em><span class="n">eos_id</span><span class="o">=</span><span class="default_value">- 1</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/speechbrain/decoders/seq2seq.html#filter_seq2seq_output"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#speechbrain.decoders.seq2seq.filter_seq2seq_output" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter the output until the first eos occurs (exclusive).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>string_pred</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)"><em>list</em></a>) – A list containing the output strings/ints predicted by the seq2seq system.</li>
<li><strong>eos_id</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)"><em>int</em></a><em>, </em><em>string</em>) – The id of the eos.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">The output predicted by seq2seq model.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.9)">list</a></p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">string_pred</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="s1">&#39;c&#39;</span><span class="p">,</span><span class="s1">&#39;d&#39;</span><span class="p">,</span><span class="s1">&#39;eos&#39;</span><span class="p">,</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">string_out</span> <span class="o">=</span> <span class="n">filter_seq2seq_output</span><span class="p">(</span><span class="n">string_pred</span><span class="p">,</span> <span class="n">eos_id</span><span class="o">=</span><span class="s1">&#39;eos&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">string_out</span>
<span class="go">[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="speechbrain.decoders.transducer.html" class="btn btn-neutral float-right" title="speechbrain.decoders.transducer module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="speechbrain.decoders.ctc.html" class="btn btn-neutral float-left" title="speechbrain.decoders.ctc module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, SpeechBrain

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>