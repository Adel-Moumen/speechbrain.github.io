

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>speechbrain.core &mdash; SpeechBrain 0.0.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> SpeechBrain
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting started:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Quick installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#install-via-pypi">Install via PyPI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#install-locally">Install locally</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#test-installation">Test Installation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../experiment.html">Running an experiment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../experiment.html#yaml-basics">YAML basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../experiment.html#running-arguments">Running arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../experiment.html#tensor-format">Tensor format</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../multigpu.html">Basics of multi-GPU</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../multigpu.html#multi-gpu-training-using-data-parallel">Multi-GPU training using Data Parallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../multigpu.html#multi-gpu-training-using-distributed-data-parallel-ddp">Multi-GPU training using Distributed Data Parallel (DDP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../multigpu.html#with-multiple-machines-suppose-you-have-2-servers-with-2-gpus">With multiple machines (suppose you have 2 servers with 2 GPUs):</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../contributing.html#zen-of-speechbrain">Zen of Speechbrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../contributing.html#how-to-get-your-code-in-speechbrain">How to get your code in SpeechBrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../contributing.html#python">Python</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../contributing.html#version">Version</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contributing.html#formatting">Formatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contributing.html#adding-dependencies">Adding dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contributing.html#testing">Testing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../contributing.html#documentation">Documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../contributing.html#development-tools">Development tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../contributing.html#flake8">flake8</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contributing.html#pre-commit">pre-commit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contributing.html#the-git-pre-commit-hooks">the git pre-commit hooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contributing.html#the-git-pre-push-hooks">the git pre-push hooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contributing.html#pytest-doctests">pytest doctests</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../contributing.html#continuous-integration">Continuous integration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../contributing.html#what-is-ci">What is CI?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contributing.html#ci-cd-pipelines">CI / CD Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contributing.html#our-test-suite">Our test suite</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../contributing.html#pull-request-review-guide">Pull Request review guide</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../speechbrain.html">Core library (speechbrain)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../speechbrain.core.html">speechbrain.core module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.core.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.core.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../speechbrain.yaml.html">speechbrain.yaml module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../speechbrain.alignment.html">speechbrain.alignment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.alignment.aligner.html">speechbrain.alignment.aligner module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.alignment.aligner.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.alignment.aligner.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../speechbrain.dataio.html">speechbrain.dataio</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.dataio.batch.html">speechbrain.dataio.batch module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.dataio.batch.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.dataio.batch.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.dataio.dataio.html">speechbrain.dataio.dataio module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.dataio.dataio.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.dataio.dataio.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.dataio.dataloader.html">speechbrain.dataio.dataloader module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.dataio.dataloader.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.dataio.dataloader.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.dataio.dataset.html">speechbrain.dataio.dataset module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.dataio.dataset.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.dataio.dataset.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.dataio.encoder.html">speechbrain.dataio.encoder module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.dataio.encoder.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.dataio.encoder.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.dataio.legacy.html">speechbrain.dataio.legacy module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.dataio.legacy.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.dataio.legacy.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.dataio.sampler.html">speechbrain.dataio.sampler module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.dataio.sampler.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.dataio.sampler.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.dataio.wer.html">speechbrain.dataio.wer module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.dataio.wer.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.dataio.wer.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.dataio.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.dataio.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../speechbrain.decoders.html">speechbrain.decoders</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.decoders.ctc.html">speechbrain.decoders.ctc module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.decoders.ctc.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.decoders.ctc.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.decoders.seq2seq.html">speechbrain.decoders.seq2seq module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.decoders.seq2seq.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.decoders.seq2seq.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.decoders.transducer.html">speechbrain.decoders.transducer module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.decoders.transducer.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.decoders.transducer.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../speechbrain.lm.html">speechbrain.lm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.lm.arpa.html">speechbrain.lm.arpa module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lm.arpa.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lm.arpa.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.lm.counting.html">speechbrain.lm.counting module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lm.counting.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lm.counting.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.lm.ngram.html">speechbrain.lm.ngram module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lm.ngram.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lm.ngram.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../speechbrain.lobes.html">speechbrain.lobes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.lobes.augment.html">speechbrain.lobes.augment module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lobes.augment.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lobes.augment.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.lobes.features.html">speechbrain.lobes.features module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lobes.features.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lobes.features.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.lobes.models.html">speechbrain.lobes.models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lobes.models.CRDNN.html">speechbrain.lobes.models.CRDNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lobes.models.ContextNet.html">speechbrain.lobes.models.ContextNet module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lobes.models.ECAPA_TDNN.html">speechbrain.lobes.models.ECAPA_TDNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lobes.models.ESPnetVGG.html">speechbrain.lobes.models.ESPnetVGG module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lobes.models.MetricGAN_D.html">speechbrain.lobes.models.MetricGAN_D module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lobes.models.RNNLM.html">speechbrain.lobes.models.RNNLM module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lobes.models.VanillaNN.html">speechbrain.lobes.models.VanillaNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lobes.models.Xvector.html">speechbrain.lobes.models.Xvector module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lobes.models.conv_tasnet.html">speechbrain.lobes.models.conv_tasnet module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lobes.models.convolution.html">speechbrain.lobes.models.convolution module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lobes.models.dual_path.html">speechbrain.lobes.models.dual_path module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.lobes.models.transformer.html">speechbrain.lobes.models.transformer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../speechbrain.nnet.html">speechbrain.nnet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.CNN.html">speechbrain.nnet.CNN module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.CNN.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.CNN.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.RNN.html">speechbrain.nnet.RNN module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.RNN.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.RNN.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.activations.html">speechbrain.nnet.activations module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.activations.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.activations.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.attention.html">speechbrain.nnet.attention module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.attention.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.attention.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.containers.html">speechbrain.nnet.containers module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.containers.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.containers.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.dropout.html">speechbrain.nnet.dropout module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.dropout.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.dropout.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.embedding.html">speechbrain.nnet.embedding module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.embedding.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.embedding.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.linear.html">speechbrain.nnet.linear module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.linear.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.linear.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.losses.html">speechbrain.nnet.losses module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.losses.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.losses.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.normalization.html">speechbrain.nnet.normalization module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.normalization.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.normalization.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.pooling.html">speechbrain.nnet.pooling module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.pooling.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.pooling.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.schedulers.html">speechbrain.nnet.schedulers module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.schedulers.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.schedulers.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.complex_networks.html">speechbrain.nnet.complex_networks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.complex_networks.c_CNN.html">speechbrain.nnet.complex_networks.c_CNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.complex_networks.c_RNN.html">speechbrain.nnet.complex_networks.c_RNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.complex_networks.c_linear.html">speechbrain.nnet.complex_networks.c_linear module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.complex_networks.c_normalization.html">speechbrain.nnet.complex_networks.c_normalization module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.complex_networks.c_ops.html">speechbrain.nnet.complex_networks.c_ops module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.loss.html">speechbrain.nnet.loss</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.loss.stoi_loss.html">speechbrain.nnet.loss.stoi_loss module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.loss.transducer_loss.html">speechbrain.nnet.loss.transducer_loss module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.quaternion_networks.html">speechbrain.nnet.quaternion_networks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.quaternion_networks.q_CNN.html">speechbrain.nnet.quaternion_networks.q_CNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.quaternion_networks.q_RNN.html">speechbrain.nnet.quaternion_networks.q_RNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.quaternion_networks.q_linear.html">speechbrain.nnet.quaternion_networks.q_linear module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.quaternion_networks.q_normalization.html">speechbrain.nnet.quaternion_networks.q_normalization module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.quaternion_networks.q_ops.html">speechbrain.nnet.quaternion_networks.q_ops module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.transducer.html">speechbrain.nnet.transducer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.nnet.transducer.transducer_joint.html">speechbrain.nnet.transducer.transducer_joint module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.nnet.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../speechbrain.processing.html">speechbrain.processing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.processing.NMF.html">speechbrain.processing.NMF module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.processing.NMF.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.processing.NMF.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.processing.PLDA_LDA.html">speechbrain.processing.PLDA_LDA module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.processing.PLDA_LDA.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.processing.PLDA_LDA.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.processing.decomposition.html">speechbrain.processing.decomposition module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.processing.decomposition.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.processing.decomposition.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.processing.diarization.html">speechbrain.processing.diarization module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.processing.diarization.html#reference">Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.processing.diarization.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.processing.diarization.html#id1">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.processing.features.html">speechbrain.processing.features module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.processing.features.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.processing.features.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.processing.multi_mic.html">speechbrain.processing.multi_mic module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.processing.multi_mic.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.processing.multi_mic.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.processing.signal_processing.html">speechbrain.processing.signal_processing module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.processing.signal_processing.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.processing.signal_processing.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.processing.speech_augmentation.html">speechbrain.processing.speech_augmentation module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.processing.speech_augmentation.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.processing.speech_augmentation.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../speechbrain.tokenizers.html">speechbrain.tokenizers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.tokenizers.SentencePiece.html">speechbrain.tokenizers.SentencePiece module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.tokenizers.SentencePiece.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.tokenizers.SentencePiece.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../speechbrain.utils.html">speechbrain.utils</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.utils.Accuracy.html">speechbrain.utils.Accuracy module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.Accuracy.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.Accuracy.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.utils.DER.html">speechbrain.utils.DER module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.DER.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.DER.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.utils.checkpoints.html">speechbrain.utils.checkpoints module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.checkpoints.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.checkpoints.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.utils.convert_model.html">speechbrain.utils.convert_model module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.utils.data_pipeline.html">speechbrain.utils.data_pipeline module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.data_pipeline.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.data_pipeline.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.utils.data_utils.html">speechbrain.utils.data_utils module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.data_utils.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.data_utils.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.utils.depgraph.html">speechbrain.utils.depgraph module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.depgraph.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.depgraph.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.utils.distributed.html">speechbrain.utils.distributed module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.distributed.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.distributed.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.utils.edit_distance.html">speechbrain.utils.edit_distance module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.edit_distance.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.edit_distance.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.utils.epoch_loop.html">speechbrain.utils.epoch_loop module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.epoch_loop.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.epoch_loop.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.utils.logger.html">speechbrain.utils.logger module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.logger.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.logger.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.utils.metric_stats.html">speechbrain.utils.metric_stats module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.metric_stats.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.metric_stats.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.utils.parameter_transfer.html">speechbrain.utils.parameter_transfer module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.parameter_transfer.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.parameter_transfer.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.utils.superpowers.html">speechbrain.utils.superpowers module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.superpowers.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.superpowers.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.utils.train_logger.html">speechbrain.utils.train_logger module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.train_logger.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../speechbrain.utils.train_logger.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.utils.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../speechbrain.utils.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../speechbrain.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../tools.html">Runnable Tools (tools)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tools.compute_wer.html">tools.compute_wer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../tools.compute_wer.html#usage">Usage</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">SpeechBrain</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>speechbrain.core</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for speechbrain.core</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Core SpeechBrain code for running experiments.</span>

<span class="sd">Authors</span>
<span class="sd"> * Peter Plantinga 2020</span>
<span class="sd"> * Abdel Heba 2020</span>
<span class="sd"> * Mirco Ravanelli 2020</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">yaml</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">shutil</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">pathlib</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">speechbrain</span> <span class="k">as</span> <span class="nn">sb</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="k">import</span> <span class="n">date</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="k">import</span> <span class="n">Enum</span><span class="p">,</span> <span class="n">auto</span>
<span class="kn">from</span> <span class="nn">tqdm.contrib</span> <span class="k">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">types</span> <span class="k">import</span> <span class="n">SimpleNamespace</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="k">import</span> <span class="n">SyncBatchNorm</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="k">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="k">import</span> <span class="n">DataParallel</span> <span class="k">as</span> <span class="n">DP</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="k">import</span> <span class="n">IterableDataset</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="k">import</span> <span class="n">DistributedSampler</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="k">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="kn">from</span> <span class="nn">hyperpyyaml</span> <span class="k">import</span> <span class="n">resolve_references</span>
<span class="kn">from</span> <span class="nn">speechbrain.utils.distributed</span> <span class="k">import</span> <span class="n">run_on_main</span>
<span class="kn">from</span> <span class="nn">speechbrain.dataio.dataloader</span> <span class="k">import</span> <span class="n">SaveableDataLoader</span>
<span class="kn">from</span> <span class="nn">speechbrain.dataio.sampler</span> <span class="k">import</span> <span class="n">DistributedSamplerWrapper</span>
<span class="kn">from</span> <span class="nn">speechbrain.dataio.sampler</span> <span class="k">import</span> <span class="n">ReproducibleRandomSampler</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="n">__name__</span><span class="p">)</span>
<span class="n">DEFAULT_LOG_CONFIG</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">__file__</span><span class="p">))</span>
<span class="n">DEFAULT_LOG_CONFIG</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DEFAULT_LOG_CONFIG</span><span class="p">,</span> <span class="s2">&quot;log-config.yaml&quot;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_set_profiling_executor</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_set_profiling_mode</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">INTRA_EPOCH_CKPT_FLAG</span> <span class="o">=</span> <span class="s2">&quot;brain_intra_epoch_ckpt&quot;</span>


<div class="viewcode-block" id="create_experiment_directory"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.create_experiment_directory">[docs]</a><span class="k">def</span> <span class="nf">create_experiment_directory</span><span class="p">(</span>
    <span class="n">experiment_directory</span><span class="p">,</span>
    <span class="n">hyperparams_to_save</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">overrides</span><span class="o">=</span><span class="p">{},</span>
    <span class="n">log_config</span><span class="o">=</span><span class="n">DEFAULT_LOG_CONFIG</span><span class="p">,</span>
    <span class="n">save_env_desc</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create the output folder and relevant experimental files.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    experiment_directory : str</span>
<span class="sd">        The place where the experiment directory should be created.</span>
<span class="sd">    hyperparams_to_save : str</span>
<span class="sd">        A filename of a yaml file representing the parameters for this</span>
<span class="sd">        experiment. If passed, references are resolved, and the result is</span>
<span class="sd">        written to a file in the experiment directory called &quot;hyperparams.yaml&quot;.</span>
<span class="sd">    overrides : dict</span>
<span class="sd">        A mapping of replacements made in the yaml file, to save in yaml.</span>
<span class="sd">    log_config : str</span>
<span class="sd">        A yaml filename containing configuration options for the logger.</span>
<span class="sd">    save_env_desc : bool</span>
<span class="sd">        If True, an environment state description is saved to the experiment</span>
<span class="sd">        directory, in a file called env.log in the experiment directory.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># all writing command must be done with the main_process</span>
        <span class="k">if</span> <span class="n">sb</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">if_main_process</span><span class="p">():</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">experiment_directory</span><span class="p">):</span>
                <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">experiment_directory</span><span class="p">)</span>

            <span class="c1"># Write the parameters file</span>
            <span class="k">if</span> <span class="n">hyperparams_to_save</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">hyperparams_filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="n">experiment_directory</span><span class="p">,</span> <span class="s2">&quot;hyperparams.yaml&quot;</span>
                <span class="p">)</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">hyperparams_to_save</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                    <span class="n">resolved_yaml</span> <span class="o">=</span> <span class="n">resolve_references</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">overrides</span><span class="p">)</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">hyperparams_filename</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">w</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;# Generated </span><span class="si">%s</span><span class="s2"> from:&quot;</span> <span class="o">%</span> <span class="n">date</span><span class="o">.</span><span class="n">today</span><span class="p">(),</span> <span class="n">file</span><span class="o">=</span><span class="n">w</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;# </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">hyperparams_to_save</span><span class="p">),</span> <span class="n">file</span><span class="o">=</span><span class="n">w</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;# yamllint disable&quot;</span><span class="p">,</span> <span class="n">file</span><span class="o">=</span><span class="n">w</span><span class="p">)</span>
                    <span class="n">shutil</span><span class="o">.</span><span class="n">copyfileobj</span><span class="p">(</span><span class="n">resolved_yaml</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

            <span class="c1"># Copy executing file to output directory</span>
            <span class="n">module</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getmodule</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">currentframe</span><span class="p">()</span><span class="o">.</span><span class="n">f_back</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">callingfile</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">__file__</span><span class="p">)</span>
                <span class="n">shutil</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">callingfile</span><span class="p">,</span> <span class="n">experiment_directory</span><span class="p">)</span>

            <span class="c1"># Log exceptions to output automatically</span>
            <span class="n">log_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">experiment_directory</span><span class="p">,</span> <span class="s2">&quot;log.txt&quot;</span><span class="p">)</span>
            <span class="n">logger_overrides</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;handlers&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;file_handler&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;filename&quot;</span><span class="p">:</span> <span class="n">log_file</span><span class="p">}}</span>
            <span class="p">}</span>
            <span class="n">sb</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">setup_logging</span><span class="p">(</span><span class="n">log_config</span><span class="p">,</span> <span class="n">logger_overrides</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">excepthook</span> <span class="o">=</span> <span class="n">_logging_excepthook</span>

            <span class="c1"># Log beginning of experiment!</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Beginning experiment!&quot;</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;Experiment folder: </span><span class="si">{experiment_directory}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="c1"># Save system description:</span>
            <span class="k">if</span> <span class="n">save_env_desc</span><span class="p">:</span>
                <span class="n">description_str</span> <span class="o">=</span> <span class="n">sb</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">get_environment_description</span><span class="p">()</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">experiment_directory</span><span class="p">,</span> <span class="s2">&quot;env.log&quot;</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span>
                <span class="p">)</span> <span class="k">as</span> <span class="n">fo</span><span class="p">:</span>
                    <span class="n">fo</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">description_str</span><span class="p">)</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="c1"># wait for main_process if ddp is used</span>
        <span class="n">sb</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ddp_barrier</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">_logging_excepthook</span><span class="p">(</span><span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">,</span> <span class="n">exc_traceback</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Interrupt exception raising to log the error.&quot;&quot;&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Exception:&quot;</span><span class="p">,</span> <span class="n">exc_info</span><span class="o">=</span><span class="p">(</span><span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">,</span> <span class="n">exc_traceback</span><span class="p">))</span>


<div class="viewcode-block" id="parse_arguments"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.parse_arguments">[docs]</a><span class="k">def</span> <span class="nf">parse_arguments</span><span class="p">(</span><span class="n">arg_list</span><span class="p">):</span>
    <span class="sd">r&quot;&quot;&quot;Parse command-line arguments to the experiment.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    arg_list : list</span>
<span class="sd">        A list of arguments to parse, most often from `sys.argv[1:]`.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    param_file : str</span>
<span class="sd">        The location of the parameters file.</span>
<span class="sd">    run_opts : dict</span>
<span class="sd">        Run options, such as distributed, device, etc.</span>
<span class="sd">    overrides : dict</span>
<span class="sd">        The overrides to pass to ``load_hyperpyyaml``.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; argv = [&#39;hyperparams.yaml&#39;, &#39;--device&#39;, &#39;cuda:1&#39;, &#39;--seed&#39;, &#39;10&#39;]</span>
<span class="sd">    &gt;&gt;&gt; filename, run_opts, overrides = parse_arguments(argv)</span>
<span class="sd">    &gt;&gt;&gt; filename</span>
<span class="sd">    &#39;hyperparams.yaml&#39;</span>
<span class="sd">    &gt;&gt;&gt; run_opts[&quot;device&quot;]</span>
<span class="sd">    &#39;cuda:1&#39;</span>
<span class="sd">    &gt;&gt;&gt; overrides</span>
<span class="sd">    &#39;seed: 10&#39;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Run a SpeechBrain experiment&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;param_file&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;A yaml-formatted file using the extended YAML syntax. &quot;</span>
        <span class="s2">&quot;defined by SpeechBrain.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--debug&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Run the experiment with only a few batches for all &quot;</span>
        <span class="s2">&quot;datasets, to ensure code runs without crashing.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--debug_batches&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Number of batches to run in debug mode.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--debug_epochs&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Number of epochs to run in debug mode. &quot;</span>
        <span class="s2">&quot;If a non-positive number is passed, all epochs are run.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--log_config&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;A file storing the configuration options for logging&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># if use_env = False in torch.distributed.lunch then local_rank arg is given</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--local_rank&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Rank on local machine&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--device&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The device to run the experiment on (e.g. &#39;cuda:0&#39;)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--data_parallel_count&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Number of devices that are used for data_parallel computation&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--data_parallel_backend&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;If True, data_parallel is used.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--distributed_launch&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;if True, use DDP&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--distributed_backend&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;One of {nccl, gloo, mpi}&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--jit_module_keys&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">nargs</span><span class="o">=</span><span class="s2">&quot;*&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;A list of keys in the &#39;modules&#39; dict to jitify&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--auto_mix_prec&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;If True, automatic mixed-precision is used.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--max_grad_norm&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Gradient norm will be clipped to this value, &quot;</span>
        <span class="s2">&quot;enter negative value to disable.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--nonfinite_patience&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Max number of batches per epoch to skip if loss is nonfinite.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--progressbar&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;If True, displays a progressbar indicating dataset progress.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--ckpt_interval_minutes&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Amount of time between saving intra-epoch checkpoints &quot;</span>
        <span class="s2">&quot;in minutes. If non-positive, intra-epoch checkpoints are not saved.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Accept extra args to override yaml</span>
    <span class="n">run_opts</span><span class="p">,</span> <span class="n">overrides</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_known_args</span><span class="p">(</span><span class="n">arg_list</span><span class="p">)</span>

    <span class="c1"># Ignore items that are &quot;None&quot;, they were not passed</span>
    <span class="n">run_opts</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">vars</span><span class="p">(</span><span class="n">run_opts</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">}</span>

    <span class="n">param_file</span> <span class="o">=</span> <span class="n">run_opts</span><span class="p">[</span><span class="s2">&quot;param_file&quot;</span><span class="p">]</span>
    <span class="k">del</span> <span class="n">run_opts</span><span class="p">[</span><span class="s2">&quot;param_file&quot;</span><span class="p">]</span>

    <span class="n">overrides</span> <span class="o">=</span> <span class="n">_convert_to_yaml</span><span class="p">(</span><span class="n">overrides</span><span class="p">)</span>

    <span class="c1"># Checking that DataParallel use the right number of GPU</span>
    <span class="k">if</span> <span class="n">run_opts</span><span class="p">[</span><span class="s2">&quot;data_parallel_backend&quot;</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">run_opts</span><span class="p">[</span><span class="s2">&quot;data_parallel_count&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;data_parallel_count must be &gt; 1.&quot;</span>
                <span class="s2">&quot;if data_parallel_count = -1, then use all gpus.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">run_opts</span><span class="p">[</span><span class="s2">&quot;data_parallel_count&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;data_parallel_count must be &lt;= &quot;</span>
                <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">())</span>
                <span class="o">+</span> <span class="s2">&quot;if data_parallel_count = -1, then use all gpus.&quot;</span>
            <span class="p">)</span>

    <span class="c1"># For DDP, the device args must equal to local_rank used by</span>
    <span class="c1"># torch.distributed.launch. If run_opts[&quot;local_rank&quot;] exists,</span>
    <span class="c1"># use os.environ[&quot;LOCAL_RANK&quot;]</span>
    <span class="n">local_rank</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="s2">&quot;local_rank&quot;</span> <span class="ow">in</span> <span class="n">run_opts</span><span class="p">:</span>
        <span class="n">local_rank</span> <span class="o">=</span> <span class="n">run_opts</span><span class="p">[</span><span class="s2">&quot;local_rank&quot;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;LOCAL_RANK&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
            <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">])</span>

    <span class="c1"># force device arg to be the same as local_rank from torch.distributed.lunch</span>
    <span class="k">if</span> <span class="n">local_rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">in</span> <span class="n">run_opts</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]:</span>
        <span class="n">run_opts</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">run_opts</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">][:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">param_file</span><span class="p">,</span> <span class="n">run_opts</span><span class="p">,</span> <span class="n">overrides</span></div>


<span class="k">def</span> <span class="nf">_convert_to_yaml</span><span class="p">(</span><span class="n">overrides</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convert args to yaml for overrides&quot;&quot;&quot;</span>
    <span class="n">yaml_string</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>

    <span class="c1"># Handle &#39;--arg=val&#39; type args</span>
    <span class="n">joined_args</span> <span class="o">=</span> <span class="s2">&quot;=&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">overrides</span><span class="p">)</span>
    <span class="n">split_args</span> <span class="o">=</span> <span class="n">joined_args</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">split_args</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">arg</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;--&quot;</span><span class="p">):</span>
            <span class="n">yaml_string</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">arg</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="s2">&quot;--&quot;</span><span class="p">)</span> <span class="p">:]</span> <span class="o">+</span> <span class="s2">&quot;:&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">yaml_string</span> <span class="o">+=</span> <span class="s2">&quot; &quot;</span> <span class="o">+</span> <span class="n">arg</span>

    <span class="k">return</span> <span class="n">yaml_string</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>


<div class="viewcode-block" id="Stage"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.Stage">[docs]</a><span class="k">class</span> <span class="nc">Stage</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Simple enum to track stage of experiments.&quot;&quot;&quot;</span>

    <span class="n">TRAIN</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">VALID</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">TEST</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span></div>


<div class="viewcode-block" id="Brain"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.Brain">[docs]</a><span class="nd">@sb</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">checkpoints</span><span class="o">.</span><span class="n">register_checkpoint_hooks</span>
<span class="k">class</span> <span class="nc">Brain</span><span class="p">:</span>
    <span class="sd">r&quot;&quot;&quot;Brain class abstracts away the details of data loops.</span>

<span class="sd">    The primary purpose of the `Brain` class is the implementation of</span>
<span class="sd">    the ``fit()`` method, which iterates epochs and datasets for the</span>
<span class="sd">    purpose of &quot;fitting&quot; a set of modules to a set of data.</span>

<span class="sd">    In order to use the ``fit()`` method, one should sub-class the ``Brain``</span>
<span class="sd">    class and override any methods for which the default behavior does not</span>
<span class="sd">    match the use case. For a simple use case (e.g., training a single model</span>
<span class="sd">    with a single dataset) the only methods that need to be overridden are:</span>

<span class="sd">    * ``compute_forward()``</span>
<span class="sd">    * ``compute_objectives()``</span>

<span class="sd">    The example below illustrates how overriding these two methods is done.</span>

<span class="sd">    For more complicated use cases, such as multiple modules that need to</span>
<span class="sd">    be updated, the following methods can be overridden:</span>

<span class="sd">    * ``fit_batch()``</span>
<span class="sd">    * ``evaluate_batch()``</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    modules : dict of str:torch.nn.Module pairs</span>
<span class="sd">        These modules are passed to the optimizer by default if they have</span>
<span class="sd">        trainable parameters, and will have ``train()``/``eval()`` called on them.</span>
<span class="sd">    opt_class : torch.optim class</span>
<span class="sd">        A torch optimizer constructor that has takes only the list of</span>
<span class="sd">        parameters (e.g. a lambda or partial function definition). By default,</span>
<span class="sd">        this will be passed all modules in ``modules`` at the</span>
<span class="sd">        beginning of the ``fit()`` method. This behavior can be changed</span>
<span class="sd">        by overriding the ``configure_optimizers()`` method.</span>
<span class="sd">    hparams : dict</span>
<span class="sd">        Each key:value pair should consist of a string key and a hyperparameter</span>
<span class="sd">        that is used within the overridden methods. These will</span>
<span class="sd">        be accessible via an ``hparams`` attribute, using &quot;dot&quot; notation:</span>
<span class="sd">        e.g., self.hparams.model(x).</span>
<span class="sd">    run_opts : dict</span>
<span class="sd">        A set of options to change the runtime environment, including</span>

<span class="sd">        debug (bool)</span>
<span class="sd">            If ``True``, this will only iterate a few batches for all</span>
<span class="sd">            datasets, to ensure code runs without crashing.</span>
<span class="sd">        debug_batches (int)</span>
<span class="sd">            Number of batches to run in debug mode, Default ``2``.</span>
<span class="sd">        debug_epochs (int)</span>
<span class="sd">            Number of epochs to run in debug mode, Default ``2``.</span>
<span class="sd">            If a non-positive number is passed, all epochs are run.</span>
<span class="sd">        jit_module_keys (list of str)</span>
<span class="sd">            List of keys in ``modules`` that should be jit compiled.</span>
<span class="sd">        distributed_count (int)</span>
<span class="sd">            Number of devices to run on.</span>
<span class="sd">        distributed_backend (str)</span>
<span class="sd">            One of ``ddp_nccl``, ``ddp_gloo``, ``ddp_mpi``, ``data_parallel``.</span>
<span class="sd">        device (str)</span>
<span class="sd">            The location for performing computations.</span>
<span class="sd">        auto_mix_prec (bool)</span>
<span class="sd">            If ``True``, automatic mixed-precision is used.</span>
<span class="sd">            Activate it only with cuda.</span>
<span class="sd">        max_grad_norm (float)</span>
<span class="sd">            Default implementation of ``fit_batch()`` uses</span>
<span class="sd">            ``clip_grad_norm_`` with this value. Default: ``5``.</span>
<span class="sd">        nonfinite_patience (int)</span>
<span class="sd">            Number of times to ignore non-finite losses before stopping.</span>
<span class="sd">            Default: ``3``.</span>
<span class="sd">        progressbar (bool)</span>
<span class="sd">            Whether to display a progressbar when training. Default: ``True``.</span>
<span class="sd">        ckpt_interval_minutes (float)</span>
<span class="sd">            Amount of time between saving intra-epoch checkpoints,</span>
<span class="sd">            in minutes, default: ``15.0``. If non-positive, these are not saved.</span>
<span class="sd">    checkpointer : speechbrain.Checkpointer</span>
<span class="sd">        By default, this will be used to load checkpoints, and will have the</span>
<span class="sd">        optimizer added to continue training if interrupted.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; from torch.optim import SGD</span>
<span class="sd">    &gt;&gt;&gt; class SimpleBrain(Brain):</span>
<span class="sd">    ...     def compute_forward(self, batch, stage):</span>
<span class="sd">    ...         return self.modules.model(batch[0])</span>
<span class="sd">    ...     def compute_objectives(self, predictions, batch, stage):</span>
<span class="sd">    ...         return torch.nn.functional.l1_loss(predictions, batch[0])</span>
<span class="sd">    &gt;&gt;&gt; model = torch.nn.Linear(in_features=10, out_features=10)</span>
<span class="sd">    &gt;&gt;&gt; brain = SimpleBrain({&quot;model&quot;: model}, opt_class=lambda x: SGD(x, 0.1))</span>
<span class="sd">    &gt;&gt;&gt; brain.fit(range(1), ([torch.rand(10, 10), torch.rand(10, 10)],))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>  <span class="c1"># noqa: C901</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">modules</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">opt_class</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hparams</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">run_opts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">checkpointer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">opt_class</span> <span class="o">=</span> <span class="n">opt_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">checkpointer</span> <span class="o">=</span> <span class="n">checkpointer</span>

        <span class="c1"># Arguments passed via the run opts dictionary</span>
        <span class="n">run_opt_defaults</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;debug&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;debug_batches&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s2">&quot;debug_epochs&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
            <span class="s2">&quot;data_parallel_count&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="s2">&quot;data_parallel_backend&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;distributed_launch&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;distributed_backend&quot;</span><span class="p">:</span> <span class="s2">&quot;nccl&quot;</span><span class="p">,</span>
            <span class="s2">&quot;jit_module_keys&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;auto_mix_prec&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;max_grad_norm&quot;</span><span class="p">:</span> <span class="mf">5.0</span><span class="p">,</span>
            <span class="s2">&quot;nonfinite_patience&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
            <span class="s2">&quot;progressbar&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s2">&quot;ckpt_interval_minutes&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">for</span> <span class="n">arg</span><span class="p">,</span> <span class="n">default</span> <span class="ow">in</span> <span class="n">run_opt_defaults</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">run_opts</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">run_opts</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">hparams</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">hparams</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="s2">&quot;Info: &quot;</span> <span class="o">+</span> <span class="n">arg</span> <span class="o">+</span> <span class="s2">&quot; arg overridden by command line input&quot;</span>
                    <span class="p">)</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg</span><span class="p">,</span> <span class="n">run_opts</span><span class="p">[</span><span class="n">arg</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># If any arg from run_opt_defaults exist in hparams and</span>
                <span class="c1"># not in command line args &quot;run_opts&quot;</span>
                <span class="k">if</span> <span class="n">hparams</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">hparams</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="s2">&quot;Info: &quot;</span> <span class="o">+</span> <span class="n">arg</span> <span class="o">+</span> <span class="s2">&quot; arg from hparam file is used&quot;</span>
                    <span class="p">)</span>
                    <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg</span><span class="p">,</span> <span class="n">hparams</span><span class="p">[</span><span class="n">arg</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_parallel_backend</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">distributed_launch</span><span class="p">:</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span>
                <span class="s2">&quot;To use data_parallel backend, start your script with:</span><span class="se">\n\t</span><span class="s2">&quot;</span>
                <span class="s2">&quot;python experiment.py hyperparams.yaml &quot;</span>
                <span class="s2">&quot;--data_parallel_backend=True --data_parallel_count=2&quot;</span>
                <span class="s2">&quot;To use DDP backend, start your script with:</span><span class="se">\n\t</span><span class="s2">&quot;</span>
                <span class="s2">&quot;python -m torch.distributed.lunch [args]</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;experiment.py hyperparams.yaml --distributed_launch=True &quot;</span>
                <span class="s2">&quot;--distributed_backend=nccl&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Switch to the right context</span>
        <span class="k">if</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

        <span class="c1"># Put modules on the right device, accessible with dot notation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">modules</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Make hyperparams available with dot notation too</span>
        <span class="k">if</span> <span class="n">hparams</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span> <span class="o">=</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="n">hparams</span><span class="p">)</span>

        <span class="c1"># Checkpointer should point at a temporary directory in debug mode</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">debug</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpointer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">checkpointer</span><span class="p">,</span> <span class="s2">&quot;checkpoints_dir&quot;</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">tempdir</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Since debug mode is active, switching checkpointer &quot;</span>
                <span class="n">f</span><span class="s2">&quot;output to temporary directory: </span><span class="si">{tempdir.name}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">checkpoints_dir</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">tempdir</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

            <span class="c1"># Keep reference to tempdir as long as checkpointer exists</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">tempdir</span> <span class="o">=</span> <span class="n">tempdir</span>

        <span class="c1"># Sampler should be handled by `make_dataloader`</span>
        <span class="c1"># or if you provide a DataLoader directly, you can set</span>
        <span class="c1"># this.train_sampler = your_sampler</span>
        <span class="c1"># to have your_sampler.set_epoch() called on each epoch.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_sampler</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Automatic mixed precision init</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_mix_prec</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">GradScaler</span><span class="p">()</span>

        <span class="c1"># List parameter count for the user</span>
        <span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
            <span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">total_params</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">clsname</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span>
            <span class="n">fmt_num</span> <span class="o">=</span> <span class="n">sb</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">format_order_of_magnitude</span><span class="p">(</span><span class="n">total_params</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;</span><span class="si">{fmt_num}</span><span class="s2"> trainable parameters in </span><span class="si">{clsname}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">distributed_launch</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RANK&quot;</span><span class="p">])</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span>
                        <span class="s2">&quot; ================ WARNING ===============&quot;</span>
                        <span class="s2">&quot;Please add sb.ddp_init_group() into your exp.py&quot;</span>
                        <span class="s2">&quot;To use DDP backend, start your script with:</span><span class="se">\n\t</span><span class="s2">&quot;</span>
                        <span class="s2">&quot;python -m torch.distributed.launch [args]</span><span class="se">\n\t</span><span class="s2">&quot;</span>
                        <span class="s2">&quot;experiment.py hyperparams.yaml &quot;</span>
                        <span class="s2">&quot;--distributed_launch=True --distributed_backend=nccl&quot;</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                        <span class="s2">&quot;To use DDP, please add &quot;</span>
                        <span class="s2">&quot;sb.utils.distributed.ddp_init_group() into your exp.py&quot;</span>
                    <span class="p">)</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="s2">&quot;Only the main process is alive, &quot;</span>
                        <span class="s2">&quot;all other subprocess were killed.&quot;</span>
                    <span class="p">)</span>
            <span class="c1"># force the models to start and remain synchronized</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Prepare iterating variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avg_train_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Add this class to the checkpointer for intra-epoch checkpoints</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpointer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">add_recoverable</span><span class="p">(</span><span class="s2">&quot;brain&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

<div class="viewcode-block" id="Brain.compute_forward"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.Brain.compute_forward">[docs]</a>    <span class="k">def</span> <span class="nf">compute_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">stage</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Forward pass, to be overridden by sub-classes.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        batch : torch.Tensor or tensors</span>
<span class="sd">            An element from the dataloader, including inputs for processing.</span>
<span class="sd">        stage : Stage</span>
<span class="sd">            The stage of the experiment: Stage.TRAIN, Stage.VALID, Stage.TEST</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor or Tensors</span>
<span class="sd">            The outputs after all processing is complete.</span>
<span class="sd">            Directly passed to ``compute_objectives()``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="Brain.compute_objectives"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.Brain.compute_objectives">[docs]</a>    <span class="k">def</span> <span class="nf">compute_objectives</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">stage</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute loss, to be overridden by sub-classes.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predictions : torch.Tensor or Tensors</span>
<span class="sd">            The output tensor or tensors to evaluate.</span>
<span class="sd">            Comes directly from ``compute_forward()``.</span>
<span class="sd">        batch : torch.Tensor or tensors</span>
<span class="sd">            An element from the dataloader, including targets for comparison.</span>
<span class="sd">        stage : Stage</span>
<span class="sd">            The stage of the experiment: Stage.TRAIN, Stage.VALID, Stage.TEST</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        loss : torch.Tensor</span>
<span class="sd">            A tensor with the computed loss.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="Brain.on_stage_start"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.Brain.on_stage_start">[docs]</a>    <span class="k">def</span> <span class="nf">on_stage_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Gets called when a stage starts.</span>

<span class="sd">        Useful for defining class variables used during the stage.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        stage : Stage</span>
<span class="sd">            The stage of the experiment: Stage.TRAIN, Stage.VALID, Stage.TEST</span>
<span class="sd">        epoch : int</span>
<span class="sd">            The current epoch count.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>

<div class="viewcode-block" id="Brain.on_stage_end"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.Brain.on_stage_end">[docs]</a>    <span class="k">def</span> <span class="nf">on_stage_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">,</span> <span class="n">stage_loss</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Gets called at the end of a stage.</span>

<span class="sd">        Useful for computing stage statistics, saving checkpoints, etc.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        stage : Stage</span>
<span class="sd">            The stage of the experiment: Stage.TRAIN, Stage.VALID, Stage.TEST</span>
<span class="sd">        stage_loss : float</span>
<span class="sd">            The average loss over the completed stage.</span>
<span class="sd">        epoch : int</span>
<span class="sd">            The current epoch count.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>

<div class="viewcode-block" id="Brain.make_dataloader"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.Brain.make_dataloader">[docs]</a>    <span class="k">def</span> <span class="nf">make_dataloader</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">stage</span><span class="p">,</span> <span class="n">ckpt_prefix</span><span class="o">=</span><span class="s2">&quot;dataloader-&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">loader_kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Creates DataLoaders for Datasets.</span>

<span class="sd">        This is used by ``fit()`` and ``evaluate()`` if they just receive</span>
<span class="sd">        Datasets.</span>

<span class="sd">        Alternatively, this can be called from outside the Brain subclass.</span>
<span class="sd">        In that case, the DataLoader should be passed to ``fit()`` in place</span>
<span class="sd">        of the dataset.</span>

<span class="sd">        The Stage.TRAIN DataLoader is handled specially. It has extra args for</span>
<span class="sd">        shuffle and drop_last. In DDP a DistributedSampler is created (unless</span>
<span class="sd">        the dataset is an IterableDataset).</span>

<span class="sd">        NOTE</span>
<span class="sd">        ----</span>
<span class="sd">        Some important DataLoader arguments are passed via **loader_kwargs,</span>
<span class="sd">        e.g., batch_size, num_workers, pin_memory.</span>

<span class="sd">        NOTE</span>
<span class="sd">        ----</span>
<span class="sd">        By default, ``evaluate()`` specifies ckpt_prefix=None to stop the test</span>
<span class="sd">        DataLoader being added to the checkpointer. If you need to add a</span>
<span class="sd">        recoverable after saving checkpoints (e.g., at test time, after</span>
<span class="sd">        checkpointing the training), and still be able to recover reasonably,</span>
<span class="sd">        you should probably specify ``allow_partial_load=True``.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        dataset : Dataset</span>
<span class="sd">            A set of data to use to create data loader. If the Dataset is a</span>
<span class="sd">            DynamicItemDataset, PaddedBatch is used as the default collate_fn,</span>
<span class="sd">            unless specified in loader_kwargs.</span>
<span class="sd">        stage : Stage</span>
<span class="sd">            The stage of the experiment: Stage.TRAIN, Stage.VALID, Stage.TEST</span>
<span class="sd">        ckpt_prefix : str, None</span>
<span class="sd">            Prefix to use for SaveableDataLoader Checkpoint name. The Stage</span>
<span class="sd">            name is added to this to create the full key. Set to None to not</span>
<span class="sd">            save the DataLoader.</span>
<span class="sd">        **loader_kwargs : dict</span>
<span class="sd">            Additional keyword arguments to the DataLoader.</span>
<span class="sd">            E.g., batch_size, num_workers, pin_memory.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TRAIN stage is handled specially.</span>
        <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="n">sb</span><span class="o">.</span><span class="n">Stage</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">:</span>
            <span class="n">loader_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_loader_specifics</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">loader_kwargs</span><span class="p">)</span>
        <span class="n">dataloader</span> <span class="o">=</span> <span class="n">sb</span><span class="o">.</span><span class="n">dataio</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">make_dataloader</span><span class="p">(</span>
            <span class="n">dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">loader_kwargs</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">checkpointer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">ckpt_prefix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">SaveableDataLoader</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">ckpt_key</span> <span class="o">=</span> <span class="n">ckpt_prefix</span> <span class="o">+</span> <span class="n">stage</span><span class="o">.</span><span class="n">name</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">add_recoverable</span><span class="p">(</span><span class="n">ckpt_key</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dataloader</span></div>

    <span class="k">def</span> <span class="nf">_train_loader_specifics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">loader_kwargs</span><span class="p">):</span>
        <span class="n">sampler</span> <span class="o">=</span> <span class="n">loader_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;sampler&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="c1"># Shuffling should really only matter for the train stage. Shuffling</span>
        <span class="c1"># will also lead to more padding in batches if the order was otherwise</span>
        <span class="c1"># sorted by length.</span>
        <span class="n">shuffle</span> <span class="o">=</span> <span class="n">loader_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;shuffle&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">shuffle</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">distributed_launch</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot specify both shuffle=True &quot;</span>
                    <span class="s2">&quot;and a sampler in loader_kwargs&quot;</span>
                <span class="p">)</span>
            <span class="n">sampler</span> <span class="o">=</span> <span class="n">ReproducibleRandomSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_sampler</span> <span class="o">=</span> <span class="n">sampler</span>
            <span class="n">loader_kwargs</span><span class="p">[</span><span class="s2">&quot;sampler&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_sampler</span>
            <span class="c1"># Delete the shuffle flag, since you cannot specify both a sampler and</span>
            <span class="c1"># shuffling:</span>
            <span class="k">del</span> <span class="n">loader_kwargs</span><span class="p">[</span><span class="s2">&quot;shuffle&quot;</span><span class="p">]</span>

        <span class="c1"># Possibly make a DistributedSampler or a wrapper for some other sampler</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">distributed_launch</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">IterableDataset</span><span class="p">):</span>
            <span class="n">drop_last</span> <span class="o">=</span> <span class="n">loader_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;drop_last&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="c1"># num_replicas arg is equal to world_size</span>
            <span class="c1"># and retrieved automatically within</span>
            <span class="c1"># DistributedSampler obj.</span>
            <span class="k">if</span> <span class="n">sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_sampler</span> <span class="o">=</span> <span class="n">DistributedSamplerWrapper</span><span class="p">(</span>
                    <span class="n">sampler</span><span class="p">,</span>
                    <span class="n">rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span>
                    <span class="n">drop_last</span><span class="o">=</span><span class="n">drop_last</span><span class="p">,</span>
                    <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="c1"># with DistributedSamplerWrapper, one must disable shuffling for dataloader</span>
                <span class="n">loader_kwargs</span><span class="p">[</span><span class="s2">&quot;shuffle&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">elif</span> <span class="n">loader_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;batch_sampler&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Currently to get here, shuffle == False, so not passing it.</span>
                <span class="c1"># Otherwise we&#39;d have to handle deleting it (but it is already</span>
                <span class="c1"># deleted).</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span>
                    <span class="n">dataset</span><span class="p">,</span>
                    <span class="n">rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span>
                    <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span>
                    <span class="n">drop_last</span><span class="o">=</span><span class="n">drop_last</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="c1"># with DistributedSamplerWrapper, one must disable shuffling for dataloader</span>
                <span class="n">loader_kwargs</span><span class="p">[</span><span class="s2">&quot;shuffle&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># batch_sampler was specified</span>
                <span class="c1"># TODO: Could a DistributedSamplerWrapper actually work</span>
                <span class="c1"># just fine for wrapping a BatchSampler, as well?</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot automatically solve distributed sampling &quot;</span>
                    <span class="s2">&quot;when using a BatchSampler.&quot;</span>
                <span class="p">)</span>
            <span class="n">loader_kwargs</span><span class="p">[</span><span class="s2">&quot;sampler&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_sampler</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">distributed_launch</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">IterableDataset</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Cannot automatically solve distributed sampling &quot;</span>
                <span class="s2">&quot;for IterableDataset.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">loader_kwargs</span>

<div class="viewcode-block" id="Brain.on_fit_start"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.Brain.on_fit_start">[docs]</a>    <span class="k">def</span> <span class="nf">on_fit_start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Gets called at the beginning of ``fit()``, on multiple processes</span>
<span class="sd">        if ``distributed_count &gt; 0`` and backend is ddp.</span>

<span class="sd">        Default implementation compiles the jit modules, initializes</span>
<span class="sd">        optimizers, and loads the latest checkpoint to resume training.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Run this *after* starting all processes since jit modules cannot be</span>
        <span class="c1"># pickled.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_compile_jit</span><span class="p">()</span>

        <span class="c1"># Wrap modules with parallel backend after jit</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_wrap_distributed</span><span class="p">()</span>

        <span class="c1"># Initialize optimizers after parameters are configured</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_optimizers</span><span class="p">()</span>

        <span class="c1"># Load latest checkpoint to resume training if interrupted</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpointer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">recover_if_possible</span><span class="p">(</span>
                <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="p">)</span></div>

<div class="viewcode-block" id="Brain.init_optimizers"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.Brain.init_optimizers">[docs]</a>    <span class="k">def</span> <span class="nf">init_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Called during ``on_fit_start()``, initialize optimizers</span>
<span class="sd">        after parameters are fully configured (e.g. DDP, jit).</span>

<span class="sd">        The default implementation of this method depends on an optimizer</span>
<span class="sd">        class being passed at initialization that takes only a list</span>
<span class="sd">        of parameters (e.g., a lambda or a partial function definition).</span>
<span class="sd">        This creates a single optimizer that optimizes all trainable params.</span>

<span class="sd">        Override this class if there are multiple optimizers.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt_class</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpointer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">add_recoverable</span><span class="p">(</span><span class="s2">&quot;optimizer&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span></div>

<div class="viewcode-block" id="Brain.on_evaluate_start"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.Brain.on_evaluate_start">[docs]</a>    <span class="k">def</span> <span class="nf">on_evaluate_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_key</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_key</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Gets called at the beginning of ``evaluate()``</span>

<span class="sd">        Default implementation loads the best-performing checkpoint for</span>
<span class="sd">        evaluation, based on stored metrics.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        max_key : str</span>
<span class="sd">            Key to use for finding best checkpoint (higher is better).</span>
<span class="sd">            By default, passed to ``self.checkpointer.recover_if_possible()``.</span>
<span class="sd">        min_key : str</span>
<span class="sd">            Key to use for finding best checkpoint (lower is better).</span>
<span class="sd">            By default, passed to ``self.checkpointer.recover_if_possible()``.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Recover best checkpoint for evaluation</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpointer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">recover_if_possible</span><span class="p">(</span>
                <span class="n">max_key</span><span class="o">=</span><span class="n">max_key</span><span class="p">,</span>
                <span class="n">min_key</span><span class="o">=</span><span class="n">min_key</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
            <span class="p">)</span></div>

<div class="viewcode-block" id="Brain.fit_batch"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.Brain.fit_batch">[docs]</a>    <span class="k">def</span> <span class="nf">fit_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit one batch, override to do multiple updates.</span>

<span class="sd">        The default implementation depends on a few methods being defined</span>
<span class="sd">        with a particular behavior:</span>

<span class="sd">        * ``compute_forward()``</span>
<span class="sd">        * ``compute_objectives()``</span>

<span class="sd">        Also depends on having optimizers passed at initialization.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        batch : list of torch.Tensors</span>
<span class="sd">            Batch of data to use for training. Default implementation assumes</span>
<span class="sd">            this batch has two elements: inputs and targets.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        detached loss</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Managing automatic mixed precision</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_mix_prec</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">():</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">Stage</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_objectives</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">Stage</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">Stage</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_objectives</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">Stage</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span></div>

<div class="viewcode-block" id="Brain.check_gradients"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.Brain.check_gradients">[docs]</a>    <span class="k">def</span> <span class="nf">check_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check if gradients are finite and not too large.</span>

<span class="sd">        Automatically clips large gradients.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        loss : tensor</span>
<span class="sd">            The loss tensor after ``backward()`` has been called but</span>
<span class="sd">            before the optimizers ``step()``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        bool</span>
<span class="sd">            Whether or not the optimizer step should be carried out.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nonfinite_count</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># Print helpful debug info</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;Loss is </span><span class="si">{loss}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Parameter is not finite: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>

            <span class="c1"># Check if patience is exhausted</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">nonfinite_count</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">nonfinite_patience</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Loss is not finite and patience is exhausted. &quot;</span>
                    <span class="s2">&quot;To debug, wrap `fit()` with &quot;</span>
                    <span class="s2">&quot;autograd&#39;s `detect_anomaly()`, e.g.</span><span class="se">\n\n</span><span class="s2">with &quot;</span>
                    <span class="s2">&quot;torch.autograd.detect_anomaly():</span><span class="se">\n\t</span><span class="s2">brain.fit(...)&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Patience not yet exhausted, ignoring this batch.&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># Clip gradient norm</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span>
            <span class="p">(</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_grad_norm</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="kc">True</span></div>

<div class="viewcode-block" id="Brain.evaluate_batch"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.Brain.evaluate_batch">[docs]</a>    <span class="k">def</span> <span class="nf">evaluate_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">stage</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Evaluate one batch, override for different procedure than train.</span>

<span class="sd">        The default implementation depends on two methods being defined</span>
<span class="sd">        with a particular behavior:</span>

<span class="sd">        * ``compute_forward()``</span>
<span class="sd">        * ``compute_objectives()``</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        batch : list of torch.Tensors</span>
<span class="sd">            Batch of data to use for evaluation. Default implementation assumes</span>
<span class="sd">            this batch has two elements: inputs and targets.</span>
<span class="sd">        stage : Stage</span>
<span class="sd">            The stage of the experiment: Stage.VALID, Stage.TEST</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        detached loss</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_objectives</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">stage</span><span class="o">=</span><span class="n">stage</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span></div>

<div class="viewcode-block" id="Brain.fit"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.Brain.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">epoch_counter</span><span class="p">,</span>
        <span class="n">train_set</span><span class="p">,</span>
        <span class="n">valid_set</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">progressbar</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">train_loader_kwargs</span><span class="o">=</span><span class="p">{},</span>
        <span class="n">valid_loader_kwargs</span><span class="o">=</span><span class="p">{},</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Iterate epochs and datasets to improve objective.</span>

<span class="sd">        Relies on the existence of multiple functions that can (or should) be</span>
<span class="sd">        overridden. The following methods are used and expected to have a</span>
<span class="sd">        certain behavior:</span>

<span class="sd">        * ``fit_batch()``</span>
<span class="sd">        * ``evaluate_batch()``</span>
<span class="sd">        * ``update_average()``</span>

<span class="sd">        If the initialization was done with distributed_count &gt; 0 and the</span>
<span class="sd">        distributed_backend is ddp, this will generally handle multiprocess</span>
<span class="sd">        logic, like splitting the training data into subsets for each device and</span>
<span class="sd">        only saving a checkpoint on the main process.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        epoch_counter : iterable</span>
<span class="sd">            Each call should return an integer indicating the epoch count.</span>
<span class="sd">        train_set : Dataset, DataLoader</span>
<span class="sd">            A set of data to use for training. If a Dataset is given, a</span>
<span class="sd">            DataLoader is automatically created. If a DataLoader is given, it is</span>
<span class="sd">            used directly.</span>
<span class="sd">        valid_set : Dataset, DataLoader</span>
<span class="sd">            A set of data to use for validation. If a Dataset is given, a</span>
<span class="sd">            DataLoader is automatically created. If a DataLoader is given, it is</span>
<span class="sd">            used directly.</span>
<span class="sd">        train_loader_kwargs : dict</span>
<span class="sd">            Kwargs passed to `make_dataloader()` for making the train_loader</span>
<span class="sd">            (if train_set is a Dataset, not DataLoader).</span>
<span class="sd">            E.G. batch_size, num_workers.</span>
<span class="sd">            DataLoader kwargs are all valid.</span>
<span class="sd">        valid_loader_kwargs : dict</span>
<span class="sd">            Kwargs passed to `make_dataloader()` for making the valid_loader</span>
<span class="sd">            (if valid_set is a Dataset, not DataLoader).</span>
<span class="sd">            E.g., batch_size, num_workers.</span>
<span class="sd">            DataLoader kwargs are all valid.</span>
<span class="sd">        progressbar : bool</span>
<span class="sd">            Whether to display the progress of each epoch in a progressbar.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">):</span>
            <span class="n">train_set</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_dataloader</span><span class="p">(</span>
                <span class="n">train_set</span><span class="p">,</span> <span class="n">stage</span><span class="o">=</span><span class="n">sb</span><span class="o">.</span><span class="n">Stage</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">,</span> <span class="o">**</span><span class="n">train_loader_kwargs</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">valid_set</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">valid_set</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">):</span>
            <span class="n">valid_set</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_dataloader</span><span class="p">(</span>
                <span class="n">valid_set</span><span class="p">,</span>
                <span class="n">stage</span><span class="o">=</span><span class="n">sb</span><span class="o">.</span><span class="n">Stage</span><span class="o">.</span><span class="n">VALID</span><span class="p">,</span>
                <span class="n">ckpt_prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="o">**</span><span class="n">valid_loader_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">on_fit_start</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">progressbar</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">progressbar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">progressbar</span>

        <span class="c1"># Iterate epochs</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">epoch_counter</span><span class="p">:</span>

            <span class="c1"># Training stage</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">on_stage_start</span><span class="p">(</span><span class="n">Stage</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

            <span class="c1"># Reset nonfinite count to 0 each epoch</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nonfinite_count</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_sampler</span><span class="p">,</span> <span class="s2">&quot;set_epoch&quot;</span>
            <span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_sampler</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

            <span class="c1"># Time since last intra-epoch checkpoint</span>
            <span class="n">last_ckpt_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

            <span class="c1"># Only show progressbar if requested and main_process</span>
            <span class="n">enable</span> <span class="o">=</span> <span class="n">progressbar</span> <span class="ow">and</span> <span class="n">sb</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">if_main_process</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span>
                <span class="n">train_set</span><span class="p">,</span>
                <span class="n">initial</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">,</span>
                <span class="n">dynamic_ncols</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="n">enable</span><span class="p">,</span>
            <span class="p">)</span> <span class="k">as</span> <span class="n">t</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">t</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">avg_train_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_average</span><span class="p">(</span>
                        <span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">avg_train_loss</span>
                    <span class="p">)</span>
                    <span class="n">t</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">(</span><span class="n">train_loss</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">avg_train_loss</span><span class="p">)</span>

                    <span class="c1"># Debug mode only runs a few batches</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug_batches</span><span class="p">:</span>
                        <span class="k">break</span>

                    <span class="k">if</span> <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">checkpointer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                        <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ckpt_interval_minutes</span> <span class="o">&gt;</span> <span class="mi">0</span>
                        <span class="ow">and</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">last_ckpt_time</span>
                        <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ckpt_interval_minutes</span> <span class="o">*</span> <span class="mf">60.0</span>
                    <span class="p">):</span>
                        <span class="n">run_on_main</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_save_intra_epoch_ckpt</span><span class="p">)</span>
                        <span class="n">last_ckpt_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

            <span class="c1"># Run train &quot;on_stage_end&quot; on all processes</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">on_stage_end</span><span class="p">(</span><span class="n">Stage</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">avg_train_loss</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">avg_train_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># Validation stage</span>
            <span class="k">if</span> <span class="n">valid_set</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">on_stage_start</span><span class="p">(</span><span class="n">Stage</span><span class="o">.</span><span class="n">VALID</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
                <span class="n">avg_valid_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
                        <span class="n">valid_set</span><span class="p">,</span> <span class="n">dynamic_ncols</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="n">enable</span>
                    <span class="p">):</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
                        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">stage</span><span class="o">=</span><span class="n">Stage</span><span class="o">.</span><span class="n">VALID</span><span class="p">)</span>
                        <span class="n">avg_valid_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_average</span><span class="p">(</span>
                            <span class="n">loss</span><span class="p">,</span> <span class="n">avg_valid_loss</span>
                        <span class="p">)</span>

                        <span class="c1"># Debug mode only runs a few batches</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug_batches</span><span class="p">:</span>
                            <span class="k">break</span>

                    <span class="c1"># Only run validation &quot;on_stage_end&quot; on main process</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">run_on_main</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">on_stage_end</span><span class="p">,</span>
                        <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">Stage</span><span class="o">.</span><span class="n">VALID</span><span class="p">,</span> <span class="n">avg_valid_loss</span><span class="p">,</span> <span class="n">epoch</span><span class="p">],</span>
                    <span class="p">)</span>

            <span class="c1"># Debug mode only runs a few epochs</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug_epochs</span><span class="p">:</span>
                <span class="k">break</span></div>

    <span class="k">def</span> <span class="nf">_save_intra_epoch_ckpt</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Saves a CKPT with specific intra-epoch flag.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">checkpointer</span><span class="o">.</span><span class="n">save_and_keep_only</span><span class="p">(</span>
            <span class="n">end_of_epoch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">num_to_keep</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">ckpt_predicate</span><span class="o">=</span><span class="k">lambda</span> <span class="n">c</span><span class="p">:</span> <span class="n">INTRA_EPOCH_CKPT_FLAG</span> <span class="ow">in</span> <span class="n">c</span><span class="o">.</span><span class="n">meta</span><span class="p">,</span>
            <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="n">INTRA_EPOCH_CKPT_FLAG</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span>
            <span class="n">verbosity</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_compile_jit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compile requested modules with ``torch.jit.script``.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">jit_module_keys</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">jit_module_keys</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;module&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot; is not defined in your hparams file.&quot;</span>
                <span class="p">)</span>
            <span class="n">module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_wrap_distributed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Wrap modules with distributed wrapper when requested.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">distributed_launch</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_parallel_backend</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">distributed_launch</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
                    <span class="c1"># for ddp, all module must run on same GPU</span>
                    <span class="n">module</span> <span class="o">=</span> <span class="n">SyncBatchNorm</span><span class="o">.</span><span class="n">convert_sync_batchnorm</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
                    <span class="n">module</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">])</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># data_parallel_backend</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
                    <span class="c1"># if distributed_count = -1 then use all gpus</span>
                    <span class="c1"># otherwise, specify the set of gpu to use</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_parallel_count</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                        <span class="n">module</span> <span class="o">=</span> <span class="n">DP</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">module</span> <span class="o">=</span> <span class="n">DP</span><span class="p">(</span>
                            <span class="n">module</span><span class="p">,</span>
                            <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_parallel_count</span><span class="p">)],</span>
                        <span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>

<div class="viewcode-block" id="Brain.evaluate"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.Brain.evaluate">[docs]</a>    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">test_set</span><span class="p">,</span>
        <span class="n">max_key</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">min_key</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">progressbar</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">test_loader_kwargs</span><span class="o">=</span><span class="p">{},</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Iterate test_set and evaluate brain performance. By default, loads</span>
<span class="sd">        the best-performing checkpoint (as recorded using the checkpointer).</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        test_set : Dataset, DataLoader</span>
<span class="sd">            If a DataLoader is given, it is iterated directly. Otherwise passed</span>
<span class="sd">            to ``self.make_dataloader()``.</span>
<span class="sd">        max_key : str</span>
<span class="sd">            Key to use for finding best checkpoint, passed to</span>
<span class="sd">            ``on_evaluate_start()``.</span>
<span class="sd">        min_key : str</span>
<span class="sd">            Key to use for finding best checkpoint, passed to</span>
<span class="sd">            ``on_evaluate_start()``.</span>
<span class="sd">        progressbar : bool</span>
<span class="sd">            Whether to display the progress in a progressbar.</span>
<span class="sd">        test_loader_kwargs : dict</span>
<span class="sd">            Kwargs passed to ``make_dataloader()`` if ``test_set`` is not a</span>
<span class="sd">            DataLoader. NOTE: ``loader_kwargs[&quot;ckpt_prefix&quot;]`` gets</span>
<span class="sd">            automatically overwritten to ``None`` (so that the test DataLoader</span>
<span class="sd">            is not added to the checkpointer).</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        average test loss</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">progressbar</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">progressbar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">progressbar</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">test_set</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">):</span>
            <span class="n">test_loader_kwargs</span><span class="p">[</span><span class="s2">&quot;ckpt_prefix&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">test_set</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_dataloader</span><span class="p">(</span>
                <span class="n">test_set</span><span class="p">,</span> <span class="n">Stage</span><span class="o">.</span><span class="n">TEST</span><span class="p">,</span> <span class="o">**</span><span class="n">test_loader_kwargs</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">on_evaluate_start</span><span class="p">(</span><span class="n">max_key</span><span class="o">=</span><span class="n">max_key</span><span class="p">,</span> <span class="n">min_key</span><span class="o">=</span><span class="n">min_key</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">on_stage_start</span><span class="p">(</span><span class="n">Stage</span><span class="o">.</span><span class="n">TEST</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">avg_test_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
                <span class="n">test_set</span><span class="p">,</span> <span class="n">dynamic_ncols</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="n">progressbar</span>
            <span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">stage</span><span class="o">=</span><span class="n">Stage</span><span class="o">.</span><span class="n">TEST</span><span class="p">)</span>
                <span class="n">avg_test_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_average</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">avg_test_loss</span><span class="p">)</span>

                <span class="c1"># Debug mode only runs a few batches</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug_batches</span><span class="p">:</span>
                    <span class="k">break</span>

            <span class="c1"># Only run evaluation &quot;on_stage_end&quot; on main process</span>
            <span class="n">run_on_main</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">on_stage_end</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">Stage</span><span class="o">.</span><span class="n">TEST</span><span class="p">,</span> <span class="n">avg_test_loss</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="mi">0</span></div>

<div class="viewcode-block" id="Brain.update_average"><a class="viewcode-back" href="../../speechbrain.core.html#speechbrain.core.Brain.update_average">[docs]</a>    <span class="k">def</span> <span class="nf">update_average</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">avg_loss</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Update running average of the loss.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        loss : torch.tensor</span>
<span class="sd">            detached loss, a single float value.</span>
<span class="sd">        avg_loss : float</span>
<span class="sd">            current running average.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        avg_loss : float</span>
<span class="sd">            The average loss.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
            <span class="n">avg_loss</span> <span class="o">-=</span> <span class="n">avg_loss</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">avg_loss</span> <span class="o">+=</span> <span class="nb">float</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">avg_loss</span></div>

    <span class="nd">@sb</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">checkpoints</span><span class="o">.</span><span class="n">mark_as_saver</span>
    <span class="k">def</span> <span class="nf">_save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="n">save_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">,</span>
            <span class="s2">&quot;avg_train_loss&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">avg_train_loss</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">w</span><span class="p">:</span>
            <span class="n">w</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">yaml</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">save_dict</span><span class="p">))</span>

    <span class="nd">@sb</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">checkpoints</span><span class="o">.</span><span class="n">mark_as_loader</span>
    <span class="k">def</span> <span class="nf">_recover</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">end_of_epoch</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
        <span class="k">del</span> <span class="n">end_of_epoch</span>
        <span class="k">del</span> <span class="n">device</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">save_dict</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">safe_load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="n">save_dict</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avg_train_loss</span> <span class="o">=</span> <span class="n">save_dict</span><span class="p">[</span><span class="s2">&quot;avg_train_loss&quot;</span><span class="p">]</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, SpeechBrain

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>