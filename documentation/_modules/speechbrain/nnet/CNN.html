

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>speechbrain.nnet.CNN &mdash; SpeechBrain 0.0.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> SpeechBrain
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting started:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../installation.html">Quick installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../installation.html#install-via-pypi">Install via PyPI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../installation.html#install-locally">Install locally</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../installation.html#test-installation">Test Installation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../experiment.html">Running an experiment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../experiment.html#yaml-basics">YAML basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../experiment.html#running-arguments">Running arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../experiment.html#tensor-format">Tensor format</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../multigpu.html">Basics of multi-GPU</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../multigpu.html#multi-gpu-training-using-data-parallel">Multi-GPU training using Data Parallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../multigpu.html#multi-gpu-training-using-distributed-data-parallel-ddp">Multi-GPU training using Distributed Data Parallel (DDP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../multigpu.html#with-multiple-machines-suppose-you-have-2-servers-with-2-gpus">With multiple machines (suppose you have 2 servers with 2 GPUs):</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#zen-of-speechbrain">Zen of Speechbrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#how-to-get-your-code-in-speechbrain">How to get your code in SpeechBrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#python">Python</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#version">Version</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#formatting">Formatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#adding-dependencies">Adding dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#testing">Testing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#documentation">Documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#development-tools">Development tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#flake8">flake8</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#pre-commit">pre-commit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#the-git-pre-commit-hooks">the git pre-commit hooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#the-git-pre-push-hooks">the git pre-push hooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#pytest-doctests">pytest doctests</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#continuous-integration">Continuous integration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#what-is-ci">What is CI?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#ci-cd-pipelines">CI / CD Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#our-test-suite">Our test suite</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#pull-request-review-guide">Pull Request review guide</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../speechbrain.html">Core library (speechbrain)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.core.html">speechbrain.core module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.core.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.core.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.yaml.html">speechbrain.yaml module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.alignment.html">speechbrain.alignment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.alignment.aligner.html">speechbrain.alignment.aligner module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.alignment.aligner.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.alignment.aligner.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.dataio.html">speechbrain.dataio</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.batch.html">speechbrain.dataio.batch module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.batch.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.batch.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.dataio.html">speechbrain.dataio.dataio module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.dataio.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.dataio.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.dataloader.html">speechbrain.dataio.dataloader module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.dataloader.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.dataloader.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.dataset.html">speechbrain.dataio.dataset module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.dataset.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.dataset.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.encoder.html">speechbrain.dataio.encoder module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.encoder.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.encoder.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.legacy.html">speechbrain.dataio.legacy module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.legacy.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.legacy.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.sampler.html">speechbrain.dataio.sampler module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.sampler.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.sampler.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.wer.html">speechbrain.dataio.wer module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.wer.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.wer.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.decoders.html">speechbrain.decoders</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.decoders.ctc.html">speechbrain.decoders.ctc module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.decoders.ctc.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.decoders.ctc.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.decoders.seq2seq.html">speechbrain.decoders.seq2seq module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.decoders.seq2seq.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.decoders.seq2seq.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.decoders.transducer.html">speechbrain.decoders.transducer module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.decoders.transducer.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.decoders.transducer.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.lm.html">speechbrain.lm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.lm.arpa.html">speechbrain.lm.arpa module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lm.arpa.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lm.arpa.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.lm.counting.html">speechbrain.lm.counting module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lm.counting.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lm.counting.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.lm.ngram.html">speechbrain.lm.ngram module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lm.ngram.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lm.ngram.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.lobes.html">speechbrain.lobes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.lobes.augment.html">speechbrain.lobes.augment module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.augment.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.augment.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.lobes.features.html">speechbrain.lobes.features module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.features.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.features.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.lobes.models.html">speechbrain.lobes.models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.CRDNN.html">speechbrain.lobes.models.CRDNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.ContextNet.html">speechbrain.lobes.models.ContextNet module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.ECAPA_TDNN.html">speechbrain.lobes.models.ECAPA_TDNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.ESPnetVGG.html">speechbrain.lobes.models.ESPnetVGG module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.MetricGAN_D.html">speechbrain.lobes.models.MetricGAN_D module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.RNNLM.html">speechbrain.lobes.models.RNNLM module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.VanillaNN.html">speechbrain.lobes.models.VanillaNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.Xvector.html">speechbrain.lobes.models.Xvector module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.conv_tasnet.html">speechbrain.lobes.models.conv_tasnet module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.convolution.html">speechbrain.lobes.models.convolution module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.dual_path.html">speechbrain.lobes.models.dual_path module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.transformer.html">speechbrain.lobes.models.transformer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.nnet.html">speechbrain.nnet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.CNN.html">speechbrain.nnet.CNN module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.CNN.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.CNN.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.RNN.html">speechbrain.nnet.RNN module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.RNN.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.RNN.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.activations.html">speechbrain.nnet.activations module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.activations.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.activations.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.attention.html">speechbrain.nnet.attention module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.attention.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.attention.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.containers.html">speechbrain.nnet.containers module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.containers.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.containers.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.dropout.html">speechbrain.nnet.dropout module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.dropout.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.dropout.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.embedding.html">speechbrain.nnet.embedding module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.embedding.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.embedding.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.linear.html">speechbrain.nnet.linear module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.linear.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.linear.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.losses.html">speechbrain.nnet.losses module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.losses.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.losses.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.normalization.html">speechbrain.nnet.normalization module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.normalization.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.normalization.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.pooling.html">speechbrain.nnet.pooling module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.pooling.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.pooling.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.schedulers.html">speechbrain.nnet.schedulers module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.schedulers.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.schedulers.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.complex_networks.html">speechbrain.nnet.complex_networks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.complex_networks.c_CNN.html">speechbrain.nnet.complex_networks.c_CNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.complex_networks.c_RNN.html">speechbrain.nnet.complex_networks.c_RNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.complex_networks.c_linear.html">speechbrain.nnet.complex_networks.c_linear module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.complex_networks.c_normalization.html">speechbrain.nnet.complex_networks.c_normalization module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.complex_networks.c_ops.html">speechbrain.nnet.complex_networks.c_ops module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.loss.html">speechbrain.nnet.loss</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.loss.stoi_loss.html">speechbrain.nnet.loss.stoi_loss module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.loss.transducer_loss.html">speechbrain.nnet.loss.transducer_loss module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.quaternion_networks.html">speechbrain.nnet.quaternion_networks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.quaternion_networks.q_CNN.html">speechbrain.nnet.quaternion_networks.q_CNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.quaternion_networks.q_RNN.html">speechbrain.nnet.quaternion_networks.q_RNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.quaternion_networks.q_linear.html">speechbrain.nnet.quaternion_networks.q_linear module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.quaternion_networks.q_normalization.html">speechbrain.nnet.quaternion_networks.q_normalization module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.quaternion_networks.q_ops.html">speechbrain.nnet.quaternion_networks.q_ops module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.transducer.html">speechbrain.nnet.transducer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.transducer.transducer_joint.html">speechbrain.nnet.transducer.transducer_joint module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.processing.html">speechbrain.processing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.processing.NMF.html">speechbrain.processing.NMF module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.NMF.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.NMF.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.processing.PLDA_LDA.html">speechbrain.processing.PLDA_LDA module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.PLDA_LDA.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.PLDA_LDA.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.processing.decomposition.html">speechbrain.processing.decomposition module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.decomposition.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.decomposition.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.processing.diarization.html">speechbrain.processing.diarization module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.diarization.html#reference">Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.diarization.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.diarization.html#id1">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.processing.features.html">speechbrain.processing.features module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.features.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.features.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.processing.multi_mic.html">speechbrain.processing.multi_mic module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.multi_mic.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.multi_mic.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.processing.signal_processing.html">speechbrain.processing.signal_processing module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.signal_processing.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.signal_processing.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.processing.speech_augmentation.html">speechbrain.processing.speech_augmentation module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.speech_augmentation.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.speech_augmentation.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.tokenizers.html">speechbrain.tokenizers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.tokenizers.SentencePiece.html">speechbrain.tokenizers.SentencePiece module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.tokenizers.SentencePiece.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.tokenizers.SentencePiece.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.utils.html">speechbrain.utils</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.Accuracy.html">speechbrain.utils.Accuracy module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.Accuracy.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.Accuracy.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.DER.html">speechbrain.utils.DER module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.DER.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.DER.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.checkpoints.html">speechbrain.utils.checkpoints module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.checkpoints.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.checkpoints.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.convert_model.html">speechbrain.utils.convert_model module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.data_pipeline.html">speechbrain.utils.data_pipeline module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.data_pipeline.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.data_pipeline.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.data_utils.html">speechbrain.utils.data_utils module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.data_utils.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.data_utils.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.depgraph.html">speechbrain.utils.depgraph module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.depgraph.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.depgraph.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.distributed.html">speechbrain.utils.distributed module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.distributed.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.distributed.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.edit_distance.html">speechbrain.utils.edit_distance module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.edit_distance.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.edit_distance.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.epoch_loop.html">speechbrain.utils.epoch_loop module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.epoch_loop.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.epoch_loop.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.logger.html">speechbrain.utils.logger module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.logger.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.logger.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.metric_stats.html">speechbrain.utils.metric_stats module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.metric_stats.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.metric_stats.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.parameter_transfer.html">speechbrain.utils.parameter_transfer module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.parameter_transfer.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.parameter_transfer.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.superpowers.html">speechbrain.utils.superpowers module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.superpowers.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.superpowers.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.train_logger.html">speechbrain.utils.train_logger module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.train_logger.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.train_logger.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../tools.html">Runnable Tools (tools)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../tools.compute_wer.html">tools.compute_wer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tools.compute_wer.html#usage">Usage</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">SpeechBrain</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>speechbrain.nnet.CNN</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for speechbrain.nnet.CNN</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Library implementing convolutional neural networks.</span>

<span class="sd">Authors</span>
<span class="sd"> * Mirco Ravanelli 2020</span>
<span class="sd"> * Jianyuan Zhong 2020</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="k">import</span> <span class="n">Tuple</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="n">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="SincConv"><a class="viewcode-back" href="../../../speechbrain.nnet.CNN.html#speechbrain.nnet.CNN.SincConv">[docs]</a><span class="k">class</span> <span class="nc">SincConv</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This function implements SincConv (SincNet).</span>

<span class="sd">    M. Ravanelli, Y. Bengio, &quot;Speaker Recognition from raw waveform with</span>
<span class="sd">    SincNet&quot;, in Proc. of  SLT 2018 (https://arxiv.org/abs/1808.00158)</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    input_shape : tuple</span>
<span class="sd">        The shape of the input. Alternatively use ``in_channels``.</span>
<span class="sd">    in_channels : int</span>
<span class="sd">        The number of input channels. Alternatively use ``input_shape``.</span>
<span class="sd">    out_channels : int</span>
<span class="sd">        It is the number of output channels.</span>
<span class="sd">    kernel_size: int</span>
<span class="sd">        Kernel size of the convolutional filters.</span>
<span class="sd">    stride : int</span>
<span class="sd">        Stride factor of the convolutional filters. When the stride factor &gt; 1,</span>
<span class="sd">        a decimation in time is performed.</span>
<span class="sd">    dilation : int</span>
<span class="sd">        Dilation factor of the convolutional filters.</span>
<span class="sd">    padding : str</span>
<span class="sd">        (same, valid, causal). If &quot;valid&quot;, no padding is performed.</span>
<span class="sd">        If &quot;same&quot; and stride is 1, output shape is the same as the input shape.</span>
<span class="sd">        &quot;causal&quot; results in causal (dilated) convolutions.</span>
<span class="sd">    padding_mode : str</span>
<span class="sd">        This flag specifies the type of padding. See torch.nn documentation</span>
<span class="sd">        for more information.</span>
<span class="sd">    groups : int</span>
<span class="sd">        This option specifies the convolutional groups. See torch.nn</span>
<span class="sd">        documentation for more information.</span>
<span class="sd">    bias : bool</span>
<span class="sd">        If True, the additive bias b is adopted.</span>
<span class="sd">    sample_rate : int,</span>
<span class="sd">        Sampling rate of the input signals. It is only used for sinc_conv.</span>
<span class="sd">    min_low_hz : float</span>
<span class="sd">        Lowest possible frequency (in Hz) for a filter. It is only used for</span>
<span class="sd">        sinc_conv.</span>
<span class="sd">    min_low_hz : float</span>
<span class="sd">        Lowest possible value (in Hz) for a filter bandwidth.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; inp_tensor = torch.rand([10, 16000])</span>
<span class="sd">    &gt;&gt;&gt; conv = SincConv(input_shape=inp_tensor.shape, out_channels=25, kernel_size=11)</span>
<span class="sd">    &gt;&gt;&gt; out_tensor = conv(inp_tensor)</span>
<span class="sd">    &gt;&gt;&gt; out_tensor.shape</span>
<span class="sd">    torch.Size([10, 16000, 25])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">input_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="o">=</span><span class="s2">&quot;reflect&quot;</span><span class="p">,</span>
        <span class="n">sample_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span>
        <span class="n">min_low_hz</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">min_band_hz</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">=</span> <span class="n">padding_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample_rate</span> <span class="o">=</span> <span class="n">sample_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_low_hz</span> <span class="o">=</span> <span class="n">min_low_hz</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_band_hz</span> <span class="o">=</span> <span class="n">min_band_hz</span>

        <span class="c1"># input shape inference</span>
        <span class="k">if</span> <span class="n">input_shape</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">in_channels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must provide one of input_shape or in_channels&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">in_channels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">in_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_input_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

        <span class="c1"># Initialize Sinc filters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_sinc_conv</span><span class="p">()</span>

<div class="viewcode-block" id="SincConv.forward"><a class="viewcode-back" href="../../../speechbrain.nnet.CNN.html#speechbrain.nnet.CNN.SincConv.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns the output of the convolution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        x : torch.Tensor (batch, time, channel)</span>
<span class="sd">            input to convolve. 2d or 4d tensors are expected.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span>

        <span class="n">unsqueeze</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="k">if</span> <span class="n">unsqueeze</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_manage_padding</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;causal&quot;</span><span class="p">:</span>
            <span class="n">num_pad</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">num_pad</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Padding must be &#39;same&#39;, &#39;valid&#39; or &#39;causal&#39;. Got </span><span class="si">%s</span><span class="s2">.&quot;</span>
                <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="n">sinc_filters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_sinc_filters</span><span class="p">()</span>

        <span class="n">wx</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">sinc_filters</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">unsqueeze</span><span class="p">:</span>
            <span class="n">wx</span> <span class="o">=</span> <span class="n">wx</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">wx</span> <span class="o">=</span> <span class="n">wx</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">wx</span></div>

    <span class="k">def</span> <span class="nf">_check_input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Checks the input shape and returns the number of input channels.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">in_channels</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">in_channels</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;sincconv expects 2d or 3d inputs. Got &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
            <span class="p">)</span>

        <span class="c1"># Kernel size must be odd</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The field kernel size must be an odd number. Got </span><span class="si">%s</span><span class="s2">.&quot;</span>
                <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">in_channels</span>

    <span class="k">def</span> <span class="nf">_get_sinc_filters</span><span class="p">(</span><span class="bp">self</span><span class="p">,):</span>
        <span class="sd">&quot;&quot;&quot;This functions creates the sinc-filters to used for sinc-conv.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Computing the low frequencies of the filters</span>
        <span class="n">low</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_low_hz</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">low_hz_</span><span class="p">)</span>

        <span class="c1"># Setting minimum band and minimum freq</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
            <span class="n">low</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_band_hz</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">band_hz_</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_low_hz</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sample_rate</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">band</span> <span class="o">=</span> <span class="p">(</span><span class="n">high</span> <span class="o">-</span> <span class="n">low</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Passing from n_ to the corresponding f_times_t domain</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">f_times_t_low</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_</span><span class="p">)</span>
        <span class="n">f_times_t_high</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">high</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_</span><span class="p">)</span>

        <span class="c1"># Left part of the filters.</span>
        <span class="n">band_pass_left</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">f_times_t_high</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">f_times_t_low</span><span class="p">))</span>
            <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_</span>

        <span class="c1"># Central element of the filter</span>
        <span class="n">band_pass_center</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">band</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Right part of the filter (sinc filters are symmetric)</span>
        <span class="n">band_pass_right</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">band_pass_left</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Combining left, central, and right part of the filter</span>
        <span class="n">band_pass</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">band_pass_left</span><span class="p">,</span> <span class="n">band_pass_center</span><span class="p">,</span> <span class="n">band_pass_right</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>

        <span class="c1"># Amplitude normalization</span>
        <span class="n">band_pass</span> <span class="o">=</span> <span class="n">band_pass</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">band</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span>

        <span class="c1"># Setting up the filter coefficients</span>
        <span class="n">filters</span> <span class="o">=</span> <span class="n">band_pass</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">filters</span>

    <span class="k">def</span> <span class="nf">_init_sinc_conv</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initializes the parameters of the sinc_conv layer.&quot;&quot;&quot;</span>

        <span class="c1"># Initialize filterbanks such that they are equally spaced in Mel scale</span>
        <span class="n">high_hz</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_rate</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">-</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_low_hz</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_band_hz</span><span class="p">)</span>

        <span class="n">mel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_to_mel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_low_hz</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_to_mel</span><span class="p">(</span><span class="n">high_hz</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">hz</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to_hz</span><span class="p">(</span><span class="n">mel</span><span class="p">)</span>

        <span class="c1"># Filter lower frequency and bands</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">low_hz_</span> <span class="o">=</span> <span class="n">hz</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">band_hz_</span> <span class="o">=</span> <span class="p">(</span><span class="n">hz</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">hz</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Maiking freq and bands learnable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">low_hz_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">low_hz_</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">band_hz_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">band_hz_</span><span class="p">)</span>

        <span class="c1"># Hamming window</span>
        <span class="n">n_lin</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="nb">int</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window_</span> <span class="o">=</span> <span class="mf">0.54</span> <span class="o">-</span> <span class="mf">0.46</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span>
            <span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">n_lin</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
        <span class="p">)</span>

        <span class="c1"># Time axis  (only half is needed due to symmetry)</span>
        <span class="n">n</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_</span> <span class="o">=</span> <span class="p">(</span>
            <span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_rate</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_to_mel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hz</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Converts frequency in Hz to the mel scale.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="mi">2595</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">hz</span> <span class="o">/</span> <span class="mi">700</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_to_hz</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mel</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Converts frequency in the mel scale to Hz.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="mi">700</span> <span class="o">*</span> <span class="p">(</span><span class="mi">10</span> <span class="o">**</span> <span class="p">(</span><span class="n">mel</span> <span class="o">/</span> <span class="mi">2595</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_manage_padding</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dilation</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;This function performs zero-padding on the time axis</span>
<span class="sd">        such that their lengths is unchanged after the convolution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        x : torch.Tensor</span>
<span class="sd">            Input tensor.</span>
<span class="sd">        kernel_size : int</span>
<span class="sd">            Size of kernel.</span>
<span class="sd">        dilation : int</span>
<span class="sd">            Dilation used.</span>
<span class="sd">        stride : int</span>
<span class="sd">            Stride.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Detecting input shape</span>
        <span class="n">L_in</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Time padding</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">get_padding_elem</span><span class="p">(</span><span class="n">L_in</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="p">)</span>

        <span class="c1"># Applying padding</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="Conv1d"><a class="viewcode-back" href="../../../speechbrain.nnet.CNN.html#speechbrain.nnet.CNN.Conv1d">[docs]</a><span class="k">class</span> <span class="nc">Conv1d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This function implements 1d convolution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    out_channels : int</span>
<span class="sd">        It is the number of output channels.</span>
<span class="sd">    kernel_size : int</span>
<span class="sd">        Kernel size of the convolutional filters.</span>
<span class="sd">    input_shape : tuple</span>
<span class="sd">        The shape of the input. Alternatively use ``in_channels``.</span>
<span class="sd">    in_channels : int</span>
<span class="sd">        The number of input channels. Alternatively use ``input_shape``.</span>
<span class="sd">    stride : int</span>
<span class="sd">        Stride factor of the convolutional filters. When the stride factor &gt; 1,</span>
<span class="sd">        a decimation in time is performed.</span>
<span class="sd">    dilation : int</span>
<span class="sd">        Dilation factor of the convolutional filters.</span>
<span class="sd">    padding : str</span>
<span class="sd">        (same, valid, causal). If &quot;valid&quot;, no padding is performed.</span>
<span class="sd">        If &quot;same&quot; and stride is 1, output shape is the same as the input shape.</span>
<span class="sd">        &quot;causal&quot; results in causal (dilated) convolutions.</span>
<span class="sd">    padding_mode : str</span>
<span class="sd">        This flag specifies the type of padding. See torch.nn documentation</span>
<span class="sd">        for more information.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; inp_tensor = torch.rand([10, 40, 16])</span>
<span class="sd">    &gt;&gt;&gt; cnn_1d = Conv1d(</span>
<span class="sd">    ...     input_shape=inp_tensor.shape, out_channels=8, kernel_size=5</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; out_tensor = cnn_1d(inp_tensor)</span>
<span class="sd">    &gt;&gt;&gt; out_tensor.shape</span>
<span class="sd">    torch.Size([10, 40, 8])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">input_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="o">=</span><span class="s2">&quot;reflect&quot;</span><span class="p">,</span>
        <span class="n">skip_transpose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">=</span> <span class="n">padding_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unsqueeze</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">skip_transpose</span> <span class="o">=</span> <span class="n">skip_transpose</span>

        <span class="k">if</span> <span class="n">input_shape</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">in_channels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must provide one of input_shape or in_channels&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">in_channels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">in_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_input_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="Conv1d.forward"><a class="viewcode-back" href="../../../speechbrain.nnet.CNN.html#speechbrain.nnet.CNN.Conv1d.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns the output of the convolution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        x : torch.Tensor (batch, time, channel)</span>
<span class="sd">            input to convolve. 2d or 4d tensors are expected.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_transpose</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_manage_padding</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;causal&quot;</span><span class="p">:</span>
            <span class="n">num_pad</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">num_pad</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Padding must be &#39;same&#39;, &#39;valid&#39; or &#39;causal&#39;. Got &quot;</span>
                <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span>
            <span class="p">)</span>

        <span class="n">wx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">:</span>
            <span class="n">wx</span> <span class="o">=</span> <span class="n">wx</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_transpose</span><span class="p">:</span>
            <span class="n">wx</span> <span class="o">=</span> <span class="n">wx</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">wx</span></div>

    <span class="k">def</span> <span class="nf">_manage_padding</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dilation</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;This function performs zero-padding on the time axis</span>
<span class="sd">        such that their lengths is unchanged after the convolution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        x : torch.Tensor</span>
<span class="sd">            Input tensor.</span>
<span class="sd">        kernel_size : int</span>
<span class="sd">            Size of kernel.</span>
<span class="sd">        dilation : int</span>
<span class="sd">            Dilation used.</span>
<span class="sd">        stride : int</span>
<span class="sd">            Stride.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Detecting input shape</span>
        <span class="n">L_in</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Time padding</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">get_padding_elem</span><span class="p">(</span><span class="n">L_in</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="p">)</span>

        <span class="c1"># Applying padding</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">_check_input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Checks the input shape and returns the number of input channels.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">unsqueeze</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">in_channels</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_transpose</span><span class="p">:</span>
            <span class="n">in_channels</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">in_channels</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;conv1d expects 2d, 3d inputs. Got &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
            <span class="p">)</span>

        <span class="c1"># Kernel size must be odd</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The field kernel size must be an odd number. Got </span><span class="si">%s</span><span class="s2">.&quot;</span>
                <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">in_channels</span></div>


<div class="viewcode-block" id="Conv2d"><a class="viewcode-back" href="../../../speechbrain.nnet.CNN.html#speechbrain.nnet.CNN.Conv2d">[docs]</a><span class="k">class</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This function implements 1d convolution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    out_channels : int</span>
<span class="sd">        It is the number of output channels.</span>
<span class="sd">    kernel_size : tuple</span>
<span class="sd">        Kernel size of the 2d convolutional filters over time and frequency</span>
<span class="sd">        axis.</span>
<span class="sd">    input_shape : tuple</span>
<span class="sd">        The shape of the input. Alternatively use ``in_channels``.</span>
<span class="sd">    in_channels : int</span>
<span class="sd">        The number of input channels. Alternatively use ``input_shape``.</span>
<span class="sd">    stride: int</span>
<span class="sd">        Stride factor of the 2d convolutional filters over time and frequency</span>
<span class="sd">        axis.</span>
<span class="sd">    dilation : int</span>
<span class="sd">        Dilation factor of the 2d convolutional filters over time and</span>
<span class="sd">        frequency axis.</span>
<span class="sd">    padding : str</span>
<span class="sd">        (same, valid). If &quot;valid&quot;, no padding is performed.</span>
<span class="sd">        If &quot;same&quot; and stride is 1, output shape is same as input shape.</span>
<span class="sd">    padding_mode : str</span>
<span class="sd">        This flag specifies the type of padding. See torch.nn documentation</span>
<span class="sd">        for more information.</span>
<span class="sd">    groups : int</span>
<span class="sd">        This option specifies the convolutional groups. See torch.nn</span>
<span class="sd">        documentation for more information.</span>
<span class="sd">    bias : bool</span>
<span class="sd">        If True, the additive bias b is adopted.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; inp_tensor = torch.rand([10, 40, 16, 8])</span>
<span class="sd">    &gt;&gt;&gt; cnn_2d = Conv2d(</span>
<span class="sd">    ...     input_shape=inp_tensor.shape, out_channels=5, kernel_size=(7, 3)</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; out_tensor = cnn_2d(inp_tensor)</span>
<span class="sd">    &gt;&gt;&gt; out_tensor.shape</span>
<span class="sd">    torch.Size([10, 40, 16, 5])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">input_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="o">=</span><span class="s2">&quot;reflect&quot;</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>

        <span class="c1"># handle the case if some parameter is int</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="n">dilation</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">=</span> <span class="n">padding_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unsqueeze</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">input_shape</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">in_channels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must provide one of input_shape or in_channels&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">in_channels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">in_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

        <span class="c1"># Weights are initialized following pytorch approach</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="Conv2d.forward"><a class="viewcode-back" href="../../../speechbrain.nnet.CNN.html#speechbrain.nnet.CNN.Conv2d.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns the output of the convolution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        x : torch.Tensor (batch, time, channel)</span>
<span class="sd">            input to convolve. 2d or 4d tensors are expected.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_manage_padding</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Padding must be &#39;same&#39; or &#39;valid&#39;. Got &quot;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span>
            <span class="p">)</span>

        <span class="n">wx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">:</span>
            <span class="n">wx</span> <span class="o">=</span> <span class="n">wx</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">wx</span> <span class="o">=</span> <span class="n">wx</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">wx</span></div>

    <span class="k">def</span> <span class="nf">_manage_padding</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;This function performs zero-padding on the time and frequency axises</span>
<span class="sd">        such that their lengths is unchanged after the convolution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        x : torch.Tensor</span>
<span class="sd">        kernel_size : int</span>
<span class="sd">        dilation : int</span>
<span class="sd">        stride: int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Detecting input shape</span>
        <span class="n">L_in</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Time padding</span>
        <span class="n">padding_time</span> <span class="o">=</span> <span class="n">get_padding_elem</span><span class="p">(</span>
            <span class="n">L_in</span><span class="p">,</span> <span class="n">stride</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">kernel_size</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dilation</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="n">padding_freq</span> <span class="o">=</span> <span class="n">get_padding_elem</span><span class="p">(</span>
            <span class="n">L_in</span><span class="p">,</span> <span class="n">stride</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">kernel_size</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">dilation</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">padding_time</span> <span class="o">+</span> <span class="n">padding_freq</span>

        <span class="c1"># Applying padding</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">_check_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Checks the input shape and returns the number of input channels.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">unsqueeze</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">in_channels</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">in_channels</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected 3d or 4d inputs. Got &quot;</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>

        <span class="c1"># Kernel size must be odd</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The field kernel size must be an odd number. Got </span><span class="si">%s</span><span class="s2">.&quot;</span>
                <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">in_channels</span></div>


<div class="viewcode-block" id="DepthwiseSeparableConv1d"><a class="viewcode-back" href="../../../speechbrain.nnet.CNN.html#speechbrain.nnet.CNN.DepthwiseSeparableConv1d">[docs]</a><span class="k">class</span> <span class="nc">DepthwiseSeparableConv1d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This class implements the depthwise separable convolution.</span>

<span class="sd">    First, a channel-wise convolution is applied to the input</span>
<span class="sd">    Then, a point-wise convolution to project the input to output</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    out_channels : int</span>
<span class="sd">        It is the number of output channels.</span>
<span class="sd">    kernel_size : int</span>
<span class="sd">        Kernel size of the convolutional filters.</span>
<span class="sd">    input_shape : tuple</span>
<span class="sd">        Expected shape of the input.</span>
<span class="sd">    stride : int</span>
<span class="sd">        Stride factor of the convolutional filters. When the stride factor &gt; 1,</span>
<span class="sd">        a decimation in time is performed.</span>
<span class="sd">    dilation : int</span>
<span class="sd">        Dilation factor of the convolutional filters.</span>
<span class="sd">    padding : str</span>
<span class="sd">        (same, valid, causal). If &quot;valid&quot;, no padding is performed.</span>
<span class="sd">        If &quot;same&quot; and stride is 1, output shape is the same as the input shape.</span>
<span class="sd">        &quot;causal&quot; results in causal (dilated) convolutions.</span>
<span class="sd">    padding_mode : str</span>
<span class="sd">        This flag specifies the type of padding. See torch.nn documentation</span>
<span class="sd">        for more information.</span>
<span class="sd">    bias : bool</span>
<span class="sd">        If True, the additive bias b is adopted.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; inp = torch.randn([8, 120, 40])</span>
<span class="sd">    &gt;&gt;&gt; conv = DepthwiseSeparableConv1d(256, 3, input_shape=inp.shape)</span>
<span class="sd">    &gt;&gt;&gt; out = conv(inp)</span>
<span class="sd">    &gt;&gt;&gt; out.shape</span>
<span class="sd">    torch.Size([8, 120, 256])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">input_shape</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;input must be a 3d tensor&quot;</span>

        <span class="n">bz</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">chn</span> <span class="o">=</span> <span class="n">input_shape</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">depthwise</span> <span class="o">=</span> <span class="n">Conv1d</span><span class="p">(</span>
            <span class="n">chn</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">chn</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pointwise</span> <span class="o">=</span> <span class="n">Conv1d</span><span class="p">(</span>
            <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="DepthwiseSeparableConv1d.forward"><a class="viewcode-back" href="../../../speechbrain.nnet.CNN.html#speechbrain.nnet.CNN.DepthwiseSeparableConv1d.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns the output of the convolution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        x : torch.Tensor (batch, time, channel)</span>
<span class="sd">            input to convolve. 3d tensors are expected.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointwise</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depthwise</span><span class="p">(</span><span class="n">x</span><span class="p">))</span></div></div>


<div class="viewcode-block" id="DepthwiseSeparableConv2d"><a class="viewcode-back" href="../../../speechbrain.nnet.CNN.html#speechbrain.nnet.CNN.DepthwiseSeparableConv2d">[docs]</a><span class="k">class</span> <span class="nc">DepthwiseSeparableConv2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This class implements the depthwise separable convolution.</span>

<span class="sd">    First, a channel-wise convolution is applied to the input</span>
<span class="sd">    Then, a point-wise convolution to project the input to output</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    ut_channels : int</span>
<span class="sd">        It is the number of output channels.</span>
<span class="sd">    kernel_size : int</span>
<span class="sd">        Kernel size of the convolutional filters.</span>
<span class="sd">    stride : int</span>
<span class="sd">        Stride factor of the convolutional filters. When the stride factor &gt; 1,</span>
<span class="sd">        a decimation in time is performed.</span>
<span class="sd">    dilation : int</span>
<span class="sd">        Dilation factor of the convolutional filters.</span>
<span class="sd">    padding : str</span>
<span class="sd">        (same, valid, causal). If &quot;valid&quot;, no padding is performed.</span>
<span class="sd">        If &quot;same&quot; and stride is 1, output shape is the same as the input shape.</span>
<span class="sd">        &quot;causal&quot; results in causal (dilated) convolutions.</span>
<span class="sd">    padding_mode : str</span>
<span class="sd">        This flag specifies the type of padding. See torch.nn documentation</span>
<span class="sd">        for more information.</span>
<span class="sd">    bias : bool</span>
<span class="sd">        If True, the additive bias b is adopted.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; inp = torch.randn([8, 120, 40, 1])</span>
<span class="sd">    &gt;&gt;&gt; conv = DepthwiseSeparableConv2d(256, (3, 3), input_shape=inp.shape)</span>
<span class="sd">    &gt;&gt;&gt; out = conv(inp)</span>
<span class="sd">    &gt;&gt;&gt; out.shape</span>
<span class="sd">    torch.Size([8, 120, 40, 256])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">input_shape</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>

        <span class="c1"># handle the case if some parameter is int</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="n">dilation</span><span class="p">)</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="ow">in</span> <span class="p">{</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">},</span> <span class="s2">&quot;input must be a 3d or 4d tensor&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unsqueeze</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>

        <span class="n">bz</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">chn1</span><span class="p">,</span> <span class="n">chn2</span> <span class="o">=</span> <span class="n">input_shape</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">depthwise</span> <span class="o">=</span> <span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">chn2</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">chn2</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pointwise</span> <span class="o">=</span> <span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="DepthwiseSeparableConv2d.forward"><a class="viewcode-back" href="../../../speechbrain.nnet.CNN.html#speechbrain.nnet.CNN.DepthwiseSeparableConv2d.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns the output of the convolution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        x : torch.Tensor (batch, time, channel)</span>
<span class="sd">            input to convolve. 3d tensors are expected.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointwise</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depthwise</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span></div></div>


<div class="viewcode-block" id="get_padding_elem"><a class="viewcode-back" href="../../../speechbrain.nnet.CNN.html#speechbrain.nnet.CNN.get_padding_elem">[docs]</a><span class="k">def</span> <span class="nf">get_padding_elem</span><span class="p">(</span><span class="n">L_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dilation</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This function computes the number of elements to add for zero-padding.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    L_in : int</span>
<span class="sd">    stride: int</span>
<span class="sd">    kernel_size : int</span>
<span class="sd">    dilation : int</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">n_steps</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(((</span><span class="n">L_in</span> <span class="o">-</span> <span class="n">kernel_size</span> <span class="o">*</span> <span class="n">dilation</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">L_out</span> <span class="o">=</span> <span class="n">stride</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">kernel_size</span> <span class="o">*</span> <span class="n">dilation</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="p">[</span><span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">L_out</span> <span class="o">=</span> <span class="p">(</span><span class="n">L_in</span> <span class="o">-</span> <span class="n">dilation</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">L_out</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">L_out</span><span class="p">)</span>

        <span class="n">padding</span> <span class="o">=</span> <span class="p">[(</span><span class="n">L_in</span> <span class="o">-</span> <span class="n">L_out</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">L_in</span> <span class="o">-</span> <span class="n">L_out</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">padding</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, SpeechBrain

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>