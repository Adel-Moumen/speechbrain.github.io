

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>speechbrain.alignment.aligner &mdash; SpeechBrain 0.0.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> SpeechBrain
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting started:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../installation.html">Quick installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../installation.html#install-via-pypi">Install via PyPI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../installation.html#install-locally">Install locally</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../installation.html#test-installation">Test Installation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../experiment.html">Running an experiment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../experiment.html#yaml-basics">YAML basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../experiment.html#running-arguments">Running arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../experiment.html#tensor-format">Tensor format</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../multigpu.html">Basics of multi-GPU</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../multigpu.html#multi-gpu-training-using-data-parallel">Multi-GPU training using Data Parallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../multigpu.html#multi-gpu-training-using-distributed-data-parallel-ddp">Multi-GPU training using Distributed Data Parallel (DDP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../multigpu.html#with-multiple-machines-suppose-you-have-2-servers-with-2-gpus">With multiple machines (suppose you have 2 servers with 2 GPUs):</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#zen-of-speechbrain">Zen of Speechbrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#how-to-get-your-code-in-speechbrain">How to get your code in SpeechBrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#python">Python</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#version">Version</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#formatting">Formatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#adding-dependencies">Adding dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#testing">Testing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#documentation">Documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#development-tools">Development tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#flake8">flake8</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#pre-commit">pre-commit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#the-git-pre-commit-hooks">the git pre-commit hooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#the-git-pre-push-hooks">the git pre-push hooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#pytest-doctests">pytest doctests</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#continuous-integration">Continuous integration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#what-is-ci">What is CI?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#ci-cd-pipelines">CI / CD Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#our-test-suite">Our test suite</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#pull-request-review-guide">Pull Request review guide</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../speechbrain.html">Core library (speechbrain)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.core.html">speechbrain.core module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.core.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.core.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.yaml.html">speechbrain.yaml module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.alignment.html">speechbrain.alignment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.alignment.aligner.html">speechbrain.alignment.aligner module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.alignment.aligner.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.alignment.aligner.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.dataio.html">speechbrain.dataio</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.batch.html">speechbrain.dataio.batch module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.batch.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.batch.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.dataio.html">speechbrain.dataio.dataio module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.dataio.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.dataio.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.dataloader.html">speechbrain.dataio.dataloader module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.dataloader.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.dataloader.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.dataset.html">speechbrain.dataio.dataset module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.dataset.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.dataset.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.encoder.html">speechbrain.dataio.encoder module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.encoder.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.encoder.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.legacy.html">speechbrain.dataio.legacy module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.legacy.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.legacy.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.sampler.html">speechbrain.dataio.sampler module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.sampler.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.sampler.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.wer.html">speechbrain.dataio.wer module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.wer.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.dataio.wer.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.dataio.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.decoders.html">speechbrain.decoders</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.decoders.ctc.html">speechbrain.decoders.ctc module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.decoders.ctc.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.decoders.ctc.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.decoders.seq2seq.html">speechbrain.decoders.seq2seq module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.decoders.seq2seq.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.decoders.seq2seq.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.decoders.transducer.html">speechbrain.decoders.transducer module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.decoders.transducer.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.decoders.transducer.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.lm.html">speechbrain.lm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.lm.arpa.html">speechbrain.lm.arpa module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lm.arpa.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lm.arpa.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.lm.counting.html">speechbrain.lm.counting module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lm.counting.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lm.counting.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.lm.ngram.html">speechbrain.lm.ngram module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lm.ngram.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lm.ngram.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.lobes.html">speechbrain.lobes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.lobes.augment.html">speechbrain.lobes.augment module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.augment.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.augment.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.lobes.features.html">speechbrain.lobes.features module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.features.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.features.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.lobes.models.html">speechbrain.lobes.models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.CRDNN.html">speechbrain.lobes.models.CRDNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.ContextNet.html">speechbrain.lobes.models.ContextNet module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.ECAPA_TDNN.html">speechbrain.lobes.models.ECAPA_TDNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.ESPnetVGG.html">speechbrain.lobes.models.ESPnetVGG module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.MetricGAN_D.html">speechbrain.lobes.models.MetricGAN_D module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.RNNLM.html">speechbrain.lobes.models.RNNLM module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.VanillaNN.html">speechbrain.lobes.models.VanillaNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.Xvector.html">speechbrain.lobes.models.Xvector module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.conv_tasnet.html">speechbrain.lobes.models.conv_tasnet module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.convolution.html">speechbrain.lobes.models.convolution module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.dual_path.html">speechbrain.lobes.models.dual_path module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.lobes.models.transformer.html">speechbrain.lobes.models.transformer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.nnet.html">speechbrain.nnet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.CNN.html">speechbrain.nnet.CNN module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.CNN.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.CNN.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.RNN.html">speechbrain.nnet.RNN module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.RNN.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.RNN.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.activations.html">speechbrain.nnet.activations module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.activations.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.activations.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.attention.html">speechbrain.nnet.attention module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.attention.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.attention.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.containers.html">speechbrain.nnet.containers module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.containers.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.containers.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.dropout.html">speechbrain.nnet.dropout module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.dropout.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.dropout.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.embedding.html">speechbrain.nnet.embedding module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.embedding.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.embedding.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.linear.html">speechbrain.nnet.linear module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.linear.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.linear.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.losses.html">speechbrain.nnet.losses module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.losses.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.losses.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.normalization.html">speechbrain.nnet.normalization module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.normalization.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.normalization.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.pooling.html">speechbrain.nnet.pooling module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.pooling.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.pooling.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.schedulers.html">speechbrain.nnet.schedulers module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.schedulers.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.schedulers.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.complex_networks.html">speechbrain.nnet.complex_networks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.complex_networks.c_CNN.html">speechbrain.nnet.complex_networks.c_CNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.complex_networks.c_RNN.html">speechbrain.nnet.complex_networks.c_RNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.complex_networks.c_linear.html">speechbrain.nnet.complex_networks.c_linear module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.complex_networks.c_normalization.html">speechbrain.nnet.complex_networks.c_normalization module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.complex_networks.c_ops.html">speechbrain.nnet.complex_networks.c_ops module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.loss.html">speechbrain.nnet.loss</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.loss.stoi_loss.html">speechbrain.nnet.loss.stoi_loss module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.loss.transducer_loss.html">speechbrain.nnet.loss.transducer_loss module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.quaternion_networks.html">speechbrain.nnet.quaternion_networks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.quaternion_networks.q_CNN.html">speechbrain.nnet.quaternion_networks.q_CNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.quaternion_networks.q_RNN.html">speechbrain.nnet.quaternion_networks.q_RNN module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.quaternion_networks.q_linear.html">speechbrain.nnet.quaternion_networks.q_linear module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.quaternion_networks.q_normalization.html">speechbrain.nnet.quaternion_networks.q_normalization module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.quaternion_networks.q_ops.html">speechbrain.nnet.quaternion_networks.q_ops module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.transducer.html">speechbrain.nnet.transducer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.nnet.transducer.transducer_joint.html">speechbrain.nnet.transducer.transducer_joint module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.nnet.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.processing.html">speechbrain.processing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.processing.NMF.html">speechbrain.processing.NMF module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.NMF.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.NMF.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.processing.PLDA_LDA.html">speechbrain.processing.PLDA_LDA module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.PLDA_LDA.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.PLDA_LDA.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.processing.decomposition.html">speechbrain.processing.decomposition module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.decomposition.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.decomposition.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.processing.diarization.html">speechbrain.processing.diarization module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.diarization.html#reference">Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.diarization.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.diarization.html#id1">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.processing.features.html">speechbrain.processing.features module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.features.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.features.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.processing.multi_mic.html">speechbrain.processing.multi_mic module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.multi_mic.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.multi_mic.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.processing.signal_processing.html">speechbrain.processing.signal_processing module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.signal_processing.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.signal_processing.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.processing.speech_augmentation.html">speechbrain.processing.speech_augmentation module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.speech_augmentation.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.processing.speech_augmentation.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.tokenizers.html">speechbrain.tokenizers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.tokenizers.SentencePiece.html">speechbrain.tokenizers.SentencePiece module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.tokenizers.SentencePiece.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.tokenizers.SentencePiece.html#reference">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.utils.html">speechbrain.utils</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.Accuracy.html">speechbrain.utils.Accuracy module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.Accuracy.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.Accuracy.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.DER.html">speechbrain.utils.DER module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.DER.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.DER.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.checkpoints.html">speechbrain.utils.checkpoints module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.checkpoints.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.checkpoints.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.convert_model.html">speechbrain.utils.convert_model module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.data_pipeline.html">speechbrain.utils.data_pipeline module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.data_pipeline.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.data_pipeline.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.data_utils.html">speechbrain.utils.data_utils module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.data_utils.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.data_utils.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.depgraph.html">speechbrain.utils.depgraph module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.depgraph.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.depgraph.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.distributed.html">speechbrain.utils.distributed module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.distributed.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.distributed.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.edit_distance.html">speechbrain.utils.edit_distance module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.edit_distance.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.edit_distance.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.epoch_loop.html">speechbrain.utils.epoch_loop module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.epoch_loop.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.epoch_loop.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.logger.html">speechbrain.utils.logger module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.logger.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.logger.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.metric_stats.html">speechbrain.utils.metric_stats module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.metric_stats.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.metric_stats.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.parameter_transfer.html">speechbrain.utils.parameter_transfer module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.parameter_transfer.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.parameter_transfer.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.superpowers.html">speechbrain.utils.superpowers module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.superpowers.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.superpowers.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.train_logger.html">speechbrain.utils.train_logger module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.train_logger.html#summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../speechbrain.utils.train_logger.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.html#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../speechbrain.utils.html#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../speechbrain.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../tools.html">Runnable Tools (tools)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../tools.compute_wer.html">tools.compute_wer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tools.compute_wer.html#usage">Usage</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">SpeechBrain</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>speechbrain.alignment.aligner</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for speechbrain.alignment.aligner</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Alignment code</span>

<span class="sd">Authors</span>
<span class="sd"> * Elena Rastorgueva 2020</span>
<span class="sd"> * Loren Lugosch 2020</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">speechbrain.utils.checkpoints</span> <span class="k">import</span> <span class="n">register_checkpoint_hooks</span>
<span class="kn">from</span> <span class="nn">speechbrain.utils.checkpoints</span> <span class="k">import</span> <span class="n">mark_as_saver</span>
<span class="kn">from</span> <span class="nn">speechbrain.utils.checkpoints</span> <span class="k">import</span> <span class="n">mark_as_loader</span>
<span class="kn">from</span> <span class="nn">speechbrain.utils.data_utils</span> <span class="k">import</span> <span class="n">undo_padding</span>


<div class="viewcode-block" id="HMMAligner"><a class="viewcode-back" href="../../../speechbrain.alignment.aligner.html#speechbrain.alignment.aligner.HMMAligner">[docs]</a><span class="nd">@register_checkpoint_hooks</span>
<span class="k">class</span> <span class="nc">HMMAligner</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This class calculates Viterbi alignments in the forward method.</span>

<span class="sd">    It also records alignments and creates batches of them for use</span>
<span class="sd">    in Viterbi training.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    states_per_phoneme : int</span>
<span class="sd">        Number of hidden states to use per phoneme.</span>
<span class="sd">    output_folder : str</span>
<span class="sd">        It is the folder that the alignments will be stored in when</span>
<span class="sd">        saved to disk. Not yet implemented.</span>
<span class="sd">    neg_inf : float</span>
<span class="sd">        The float used to represent a negative infinite log probability.</span>
<span class="sd">        Using `-float(&quot;Inf&quot;)` tends to give numerical instability.</span>
<span class="sd">        A number more negative than -1e5 also sometimes gave errors when</span>
<span class="sd">        the `genbmm` library was used (currently not in use). (default: -1e5)</span>
<span class="sd">    batch_reduction : string</span>
<span class="sd">        One of &quot;none&quot;, &quot;sum&quot; or &quot;mean&quot;.</span>
<span class="sd">        What kind of batch-level reduction to apply to the loss calculated</span>
<span class="sd">        in the forward method.</span>
<span class="sd">    input_len_norm : bool</span>
<span class="sd">        Whether to normalize the loss in the forward method by the length of</span>
<span class="sd">        the inputs.</span>
<span class="sd">    target_len_norm : bool</span>
<span class="sd">        Whether to normalize the loss in the forward method by the length of</span>
<span class="sd">        the targets.</span>
<span class="sd">    lexicon_path : string</span>
<span class="sd">        The location of the lexicon.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; log_posteriors = torch.tensor([[[ -1., -10., -10.],</span>
<span class="sd">    ...                                 [-10.,  -1., -10.],</span>
<span class="sd">    ...                                 [-10., -10.,  -1.]],</span>
<span class="sd">    ...</span>
<span class="sd">    ...                                [[ -1., -10., -10.],</span>
<span class="sd">    ...                                 [-10.,  -1., -10.],</span>
<span class="sd">    ...                                 [-10., -10., -10.]]])</span>
<span class="sd">    &gt;&gt;&gt; lens = torch.tensor([1., 0.66])</span>
<span class="sd">    &gt;&gt;&gt; phns = torch.tensor([[0, 1, 2],</span>
<span class="sd">    ...                      [0, 1, 0]])</span>
<span class="sd">    &gt;&gt;&gt; phn_lens = torch.tensor([1., 0.66])</span>
<span class="sd">    &gt;&gt;&gt; aligner = HMMAligner()</span>
<span class="sd">    &gt;&gt;&gt; forward_scores = aligner(</span>
<span class="sd">    ...        log_posteriors, lens, phns, phn_lens, &#39;forward&#39;</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; forward_scores.shape</span>
<span class="sd">    torch.Size([2])</span>
<span class="sd">    &gt;&gt;&gt; viterbi_scores, alignments = aligner(</span>
<span class="sd">    ...        log_posteriors, lens, phns, phn_lens, &#39;viterbi&#39;</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; alignments</span>
<span class="sd">    [[0, 1, 2], [0, 1]]</span>
<span class="sd">    &gt;&gt;&gt; viterbi_scores.shape</span>
<span class="sd">    torch.Size([2])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">states_per_phoneme</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">output_folder</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="n">neg_inf</span><span class="o">=-</span><span class="mi">1</span><span class="n">e5</span><span class="p">,</span>
        <span class="n">batch_reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>
        <span class="n">input_len_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">target_len_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">lexicon_path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span> <span class="o">=</span> <span class="n">states_per_phoneme</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_folder</span> <span class="o">=</span> <span class="n">output_folder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neg_inf</span> <span class="o">=</span> <span class="n">neg_inf</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">batch_reduction</span> <span class="o">=</span> <span class="n">batch_reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_len_norm</span> <span class="o">=</span> <span class="n">input_len_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_len_norm</span> <span class="o">=</span> <span class="n">target_len_norm</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">align_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lexicon_path</span> <span class="o">=</span> <span class="n">lexicon_path</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lexicon_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lexicon_path</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">lines</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>

            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lines</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">line</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;;&quot;</span><span class="p">:</span>
                    <span class="n">start_index</span> <span class="o">=</span> <span class="n">i</span>
                    <span class="k">break</span>

            <span class="n">lexicon</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># {&quot;read&quot;: {0: &quot;r eh d&quot;, 1: &quot;r iy d&quot;}}</span>
            <span class="n">lexicon_phones</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_index</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">lines</span><span class="p">)):</span>
                <span class="n">line</span> <span class="o">=</span> <span class="n">lines</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">word</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">phones</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

                <span class="n">phones</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">phones</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">isdigit</span><span class="p">()])</span>

                <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">phones</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">):</span>
                    <span class="n">lexicon_phones</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

                <span class="k">if</span> <span class="s2">&quot;~&quot;</span> <span class="ow">in</span> <span class="n">word</span><span class="p">:</span>
                    <span class="n">word</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;~&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">lexicon</span><span class="p">:</span>
                    <span class="n">number_of_existing_pronunciations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lexicon</span><span class="p">[</span><span class="n">word</span><span class="p">])</span>
                    <span class="n">lexicon</span><span class="p">[</span><span class="n">word</span><span class="p">][</span><span class="n">number_of_existing_pronunciations</span><span class="p">]</span> <span class="o">=</span> <span class="n">phones</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">lexicon</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="n">phones</span><span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lexicon</span> <span class="o">=</span> <span class="n">lexicon</span>

            <span class="n">lexicon_phones</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">lexicon_phones</span><span class="p">)</span>
            <span class="n">lexicon_phones</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">lex_lab2ind</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lexicon_phones</span><span class="p">)}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lex_ind2lab</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span> <span class="n">p</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lexicon_phones</span><span class="p">)}</span>

            <span class="c1"># add sil, which is not in the lexicon</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lex_lab2ind</span><span class="p">[</span><span class="s2">&quot;sil&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lex_ind2lab</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;sil&quot;</span>

    <span class="k">def</span> <span class="nf">_use_lexicon</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">words</span><span class="p">,</span> <span class="n">interword_sils</span><span class="p">,</span> <span class="n">sample_pron</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Do processing using the lexicon to return a sequence of the possible</span>
<span class="sd">        phonemes, the transition/pi probabilities, and the possible final states.</span>
<span class="sd">        Inputs correspond to a single utterance, not a whole batch.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        words : list</span>
<span class="sd">            List of the words in the transcript.</span>
<span class="sd">        interword_sils : bool</span>
<span class="sd">            If True, optional silences will be inserted between every word.</span>
<span class="sd">            If False, optional silences will only be placed at the beginning</span>
<span class="sd">            and end of each utterance.</span>
<span class="sd">        sample_pron : bool</span>
<span class="sd">            If True, it will sample a single possible sequence of phonemes.</span>
<span class="sd">            If False, it will return statistics for all possible sequences of</span>
<span class="sd">            phonemes.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        poss_phns : torch.Tensor (phoneme)</span>
<span class="sd">            The phonemes that are thought to be in each utterance.</span>
<span class="sd">        log_transition_matrix : torch.Tensor (batch, from, to)</span>
<span class="sd">            Tensor containing transition (log) probabilities.</span>
<span class="sd">        start_states : list of ints</span>
<span class="sd">            A list of the possible starting states in each utterance.</span>
<span class="sd">        final_states : list of ints</span>
<span class="sd">            A list of the possible final states for each utterance.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">number_of_states</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">words_prime</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[]</span>
        <span class="p">)</span>  <span class="c1"># This will contain one &quot;word&quot; for each optional silence and pronunciation.</span>
        <span class="c1"># structure of each &quot;word_prime&quot;:</span>
        <span class="c1"># [word index, [[state sequence 1], [state sequence 2]], &lt;is this an optional silence?&gt;]</span>
        <span class="n">word_index</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">phoneme_indices</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">word_index</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">interword_sils</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                <span class="c1"># optional silence</span>
                <span class="n">word_prime</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">word_index</span><span class="p">,</span>
                    <span class="p">[</span>
                        <span class="p">[</span>
                            <span class="n">number_of_states</span> <span class="o">+</span> <span class="n">i</span>
                            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span><span class="p">)</span>
                        <span class="p">]</span>
                    <span class="p">],</span>
                    <span class="kc">True</span><span class="p">,</span>
                <span class="p">]</span>
                <span class="n">words_prime</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word_prime</span><span class="p">)</span>
                <span class="n">phoneme_indices</span> <span class="o">+=</span> <span class="p">[</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">silence_index</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span> <span class="o">+</span> <span class="n">i</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span><span class="p">)</span>
                <span class="p">]</span>
                <span class="n">number_of_states</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span>
                <span class="n">word_index</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># word</span>
            <span class="n">word_prime</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_index</span><span class="p">,</span> <span class="p">[],</span> <span class="kc">False</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">sample_pron</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lexicon</span><span class="p">[</span><span class="n">word</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lexicon</span><span class="p">[</span><span class="n">word</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">pron_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lexicon</span><span class="p">[</span><span class="n">word</span><span class="p">])):</span>
                <span class="n">pronunciation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lexicon</span><span class="p">[</span><span class="n">word</span><span class="p">][</span><span class="n">pron_idx</span><span class="p">]</span>
                <span class="n">phonemes</span> <span class="o">=</span> <span class="n">pronunciation</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
                <span class="n">word_prime</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span>
                <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">phonemes</span><span class="p">:</span>
                    <span class="n">phoneme_indices</span> <span class="o">+=</span> <span class="p">[</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">lex_lab2ind</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span> <span class="o">+</span> <span class="n">i</span>
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span><span class="p">)</span>
                    <span class="p">]</span>
                    <span class="n">word_prime</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">pron_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="p">[</span>
                        <span class="n">number_of_states</span> <span class="o">+</span> <span class="n">i</span>
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span><span class="p">)</span>
                    <span class="p">]</span>
                    <span class="n">number_of_states</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span>
                <span class="k">if</span> <span class="n">sample_pron</span><span class="p">:</span>
                    <span class="k">break</span>

            <span class="n">words_prime</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word_prime</span><span class="p">)</span>
            <span class="n">word_index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># optional final silence</span>
        <span class="n">word_prime</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">word_index</span><span class="p">,</span>
            <span class="p">[[</span><span class="n">number_of_states</span> <span class="o">+</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span><span class="p">)]],</span>
            <span class="kc">True</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="n">words_prime</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word_prime</span><span class="p">)</span>
        <span class="n">phoneme_indices</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">silence_index</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span> <span class="o">+</span> <span class="n">i</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">number_of_states</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span>
        <span class="n">word_index</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">transition_matrix</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span>
            <span class="n">number_of_states</span>
        <span class="p">)</span>  <span class="c1"># diagonal = all states have a self-loop</span>
        <span class="n">final_states</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">word_prime</span> <span class="ow">in</span> <span class="n">words_prime</span><span class="p">:</span>
            <span class="n">word_idx</span> <span class="o">=</span> <span class="n">word_prime</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">is_optional_silence</span> <span class="o">=</span> <span class="n">word_prime</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">next_word_exists</span> <span class="o">=</span> <span class="n">word_idx</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">words_prime</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span>
            <span class="n">this_word_last_states</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">word_prime</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_prime</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="p">]</span>

            <span class="c1"># create transitions to next state from previous state within each pronunciation</span>
            <span class="k">for</span> <span class="n">pronunciation</span> <span class="ow">in</span> <span class="n">word_prime</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                <span class="k">for</span> <span class="n">state_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pronunciation</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                    <span class="n">state</span> <span class="o">=</span> <span class="n">pronunciation</span><span class="p">[</span><span class="n">state_idx</span><span class="p">]</span>
                    <span class="n">next_state</span> <span class="o">=</span> <span class="n">pronunciation</span><span class="p">[</span><span class="n">state_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
                    <span class="n">transition_matrix</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">next_state</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>

            <span class="c1"># create transitions to next word&#39;s starting states</span>
            <span class="k">if</span> <span class="n">next_word_exists</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">is_optional_silence</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">interword_sils</span><span class="p">:</span>
                    <span class="n">next_word_idx</span> <span class="o">=</span> <span class="n">word_idx</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">next_word_idx</span> <span class="o">=</span> <span class="n">word_idx</span> <span class="o">+</span> <span class="mi">2</span>
                <span class="n">next_word_starting_states</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">words_prime</span><span class="p">[</span><span class="n">next_word_idx</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words_prime</span><span class="p">[</span><span class="n">next_word_idx</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>
                <span class="p">]</span>

                <span class="k">for</span> <span class="n">this_word_last_state</span> <span class="ow">in</span> <span class="n">this_word_last_states</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">next_word_starting_state</span> <span class="ow">in</span> <span class="n">next_word_starting_states</span><span class="p">:</span>
                        <span class="n">transition_matrix</span><span class="p">[</span>
                            <span class="n">this_word_last_state</span><span class="p">,</span> <span class="n">next_word_starting_state</span>
                        <span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">final_states</span> <span class="o">+=</span> <span class="n">this_word_last_states</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_optional_silence</span><span class="p">:</span>
                <span class="n">next_silence_idx</span> <span class="o">=</span> <span class="n">word_idx</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="n">next_silence_starting_state</span> <span class="o">=</span> <span class="n">words_prime</span><span class="p">[</span><span class="n">next_silence_idx</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span>
                    <span class="mi">0</span>
                <span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">this_word_last_state</span> <span class="ow">in</span> <span class="n">this_word_last_states</span><span class="p">:</span>
                    <span class="n">transition_matrix</span><span class="p">[</span>
                        <span class="n">this_word_last_state</span><span class="p">,</span> <span class="n">next_silence_starting_state</span>
                    <span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="n">log_transition_matrix</span> <span class="o">=</span> <span class="n">transition_matrix</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">start_states</span> <span class="o">=</span> <span class="p">[</span><span class="n">words_prime</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">start_states</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="n">words_prime</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words_prime</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>
        <span class="p">]</span>

        <span class="n">poss_phns</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">phoneme_indices</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">poss_phns</span><span class="p">,</span> <span class="n">log_transition_matrix</span><span class="p">,</span> <span class="n">start_states</span><span class="p">,</span> <span class="n">final_states</span>

<div class="viewcode-block" id="HMMAligner.use_lexicon"><a class="viewcode-back" href="../../../speechbrain.alignment.aligner.html#speechbrain.alignment.aligner.HMMAligner.use_lexicon">[docs]</a>    <span class="k">def</span> <span class="nf">use_lexicon</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">words</span><span class="p">,</span> <span class="n">interword_sils</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sample_pron</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Do processing using the lexicon to return a sequence of the possible</span>
<span class="sd">        phonemes, the transition/pi probabilities, and the possible final</span>
<span class="sd">        states.</span>
<span class="sd">        Does processing on an utterance-by-utterance basis. Each utterance</span>
<span class="sd">        in the batch is processed by a helper method `_use_lexicon`.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        words : list</span>
<span class="sd">            List of the words in the transcript</span>
<span class="sd">        interword_sils : bool</span>
<span class="sd">            If True, optional silences will be inserted between every word.</span>
<span class="sd">            If False, optional silences will only be placed at the beginning</span>
<span class="sd">            and end of each utterance.</span>
<span class="sd">        sample_pron: bool</span>
<span class="sd">            If True, it will sample a single possible sequence of phonemes.</span>
<span class="sd">            If False, it will return statistics for all possible sequences of</span>
<span class="sd">            phonemes.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        poss_phns: torch.Tensor (batch, phoneme in possible phn sequence)</span>
<span class="sd">            The phonemes that are thought to be in each utterance.</span>
<span class="sd">        poss_phn_lens: torch.Tensor (batch)</span>
<span class="sd">            The relative length of each possible phoneme sequence in the batch.</span>
<span class="sd">        trans_prob: torch.Tensor (batch, from, to)</span>
<span class="sd">            Tensor containing transition (log) probabilities.</span>
<span class="sd">        pi_prob: torch.Tensor (batch, state)</span>
<span class="sd">            Tensor containing initial (log) probabilities.</span>
<span class="sd">        final_state: list of lists of ints</span>
<span class="sd">            A list of lists of possible final states for each utterance.</span>

<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; aligner = HMMAligner()</span>
<span class="sd">        &gt;&gt;&gt; aligner.lexicon = {</span>
<span class="sd">        ...                     &quot;a&quot;: {0: &quot;a&quot;},</span>
<span class="sd">        ...                     &quot;b&quot;: {0: &quot;b&quot;, 1: &quot;c&quot;}</span>
<span class="sd">        ...                   }</span>
<span class="sd">        &gt;&gt;&gt; words = [[&quot;a&quot;, &quot;b&quot;]]</span>
<span class="sd">        &gt;&gt;&gt; aligner.lex_lab2ind = {</span>
<span class="sd">        ...                   &quot;sil&quot;: 0,</span>
<span class="sd">        ...                   &quot;a&quot;:  1,</span>
<span class="sd">        ...                   &quot;b&quot;:  2,</span>
<span class="sd">        ...                   &quot;c&quot;:  3,</span>
<span class="sd">        ...                 }</span>
<span class="sd">        &gt;&gt;&gt; poss_phns, poss_phn_lens, trans_prob, pi_prob, final_states = aligner.use_lexicon(</span>
<span class="sd">        ...     words,</span>
<span class="sd">        ...     interword_sils = True</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; poss_phns</span>
<span class="sd">        tensor([[0, 1, 0, 2, 3, 0]])</span>
<span class="sd">        &gt;&gt;&gt; poss_phn_lens</span>
<span class="sd">        tensor([1.])</span>
<span class="sd">        &gt;&gt;&gt; trans_prob</span>
<span class="sd">        tensor([[[-6.9315e-01, -6.9315e-01, -1.0000e+05, -1.0000e+05, -1.0000e+05,</span>
<span class="sd">                  -1.0000e+05],</span>
<span class="sd">                 [-1.0000e+05, -1.3863e+00, -1.3863e+00, -1.3863e+00, -1.3863e+00,</span>
<span class="sd">                  -1.0000e+05],</span>
<span class="sd">                 [-1.0000e+05, -1.0000e+05, -1.0986e+00, -1.0986e+00, -1.0986e+00,</span>
<span class="sd">                  -1.0000e+05],</span>
<span class="sd">                 [-1.0000e+05, -1.0000e+05, -1.0000e+05, -6.9315e-01, -1.0000e+05,</span>
<span class="sd">                  -6.9315e-01],</span>
<span class="sd">                 [-1.0000e+05, -1.0000e+05, -1.0000e+05, -1.0000e+05, -6.9315e-01,</span>
<span class="sd">                  -6.9315e-01],</span>
<span class="sd">                 [-1.0000e+05, -1.0000e+05, -1.0000e+05, -1.0000e+05, -1.0000e+05,</span>
<span class="sd">                   0.0000e+00]]])</span>
<span class="sd">        &gt;&gt;&gt; pi_prob</span>
<span class="sd">        tensor([[-6.9315e-01, -6.9315e-01, -1.0000e+05, -1.0000e+05, -1.0000e+05,</span>
<span class="sd">                 -1.0000e+05]])</span>
<span class="sd">        &gt;&gt;&gt; final_states</span>
<span class="sd">        [[3, 4, 5]]</span>
<span class="sd">        &gt;&gt;&gt; # With no optional silences between words</span>
<span class="sd">        &gt;&gt;&gt; poss_phns_, _, trans_prob_, pi_prob_, final_states_ = aligner.use_lexicon(</span>
<span class="sd">        ...     words,</span>
<span class="sd">        ...     interword_sils = False</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; poss_phns_</span>
<span class="sd">        tensor([[0, 1, 2, 3, 0]])</span>
<span class="sd">        &gt;&gt;&gt; trans_prob_</span>
<span class="sd">        tensor([[[-6.9315e-01, -6.9315e-01, -1.0000e+05, -1.0000e+05, -1.0000e+05],</span>
<span class="sd">                 [-1.0000e+05, -1.0986e+00, -1.0986e+00, -1.0986e+00, -1.0000e+05],</span>
<span class="sd">                 [-1.0000e+05, -1.0000e+05, -6.9315e-01, -1.0000e+05, -6.9315e-01],</span>
<span class="sd">                 [-1.0000e+05, -1.0000e+05, -1.0000e+05, -6.9315e-01, -6.9315e-01],</span>
<span class="sd">                 [-1.0000e+05, -1.0000e+05, -1.0000e+05, -1.0000e+05,  0.0000e+00]]])</span>
<span class="sd">        &gt;&gt;&gt; pi_prob_</span>
<span class="sd">        tensor([[-6.9315e-01, -6.9315e-01, -1.0000e+05, -1.0000e+05, -1.0000e+05]])</span>
<span class="sd">        &gt;&gt;&gt; final_states_</span>
<span class="sd">        [[2, 3, 4]]</span>
<span class="sd">        &gt;&gt;&gt; # With sampling of a single possible pronunciation</span>
<span class="sd">        &gt;&gt;&gt; import random</span>
<span class="sd">        &gt;&gt;&gt; random.seed(0)</span>
<span class="sd">        &gt;&gt;&gt; poss_phns_, _, trans_prob_, pi_prob_, final_states_ = aligner.use_lexicon(</span>
<span class="sd">        ...     words,</span>
<span class="sd">        ...     sample_pron = True</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; poss_phns_</span>
<span class="sd">        tensor([[0, 1, 0, 2, 0]])</span>
<span class="sd">        &gt;&gt;&gt; trans_prob_</span>
<span class="sd">        tensor([[[-6.9315e-01, -6.9315e-01, -1.0000e+05, -1.0000e+05, -1.0000e+05],</span>
<span class="sd">                 [-1.0000e+05, -1.0986e+00, -1.0986e+00, -1.0986e+00, -1.0000e+05],</span>
<span class="sd">                 [-1.0000e+05, -1.0000e+05, -6.9315e-01, -6.9315e-01, -1.0000e+05],</span>
<span class="sd">                 [-1.0000e+05, -1.0000e+05, -1.0000e+05, -6.9315e-01, -6.9315e-01],</span>
<span class="sd">                 [-1.0000e+05, -1.0000e+05, -1.0000e+05, -1.0000e+05,  0.0000e+00]]])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">silence_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lex_lab2ind</span><span class="p">[</span><span class="s2">&quot;sil&quot;</span><span class="p">]</span>

        <span class="n">poss_phns</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">trans_prob</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">start_states</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">final_states</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">words_</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
            <span class="p">(</span>
                <span class="n">poss_phns_</span><span class="p">,</span>
                <span class="n">trans_prob_</span><span class="p">,</span>
                <span class="n">start_states_</span><span class="p">,</span>
                <span class="n">final_states_</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_lexicon</span><span class="p">(</span><span class="n">words_</span><span class="p">,</span> <span class="n">interword_sils</span><span class="p">,</span> <span class="n">sample_pron</span><span class="p">)</span>
            <span class="n">poss_phns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">poss_phns_</span><span class="p">)</span>
            <span class="n">trans_prob</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">trans_prob_</span><span class="p">)</span>
            <span class="n">start_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">start_states_</span><span class="p">)</span>
            <span class="n">final_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">final_states_</span><span class="p">)</span>

        <span class="c1"># pad poss_phns, trans_prob with 0 to have same length</span>
        <span class="n">poss_phn_lens</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">poss_phns_</span><span class="p">)</span> <span class="k">for</span> <span class="n">poss_phns_</span> <span class="ow">in</span> <span class="n">poss_phns</span><span class="p">]</span>
        <span class="n">U_max</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">poss_phn_lens</span><span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">poss_phns</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">phn_pad_length</span> <span class="o">=</span> <span class="n">U_max</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">poss_phns</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
            <span class="n">poss_phns</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                <span class="n">poss_phns</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">phn_pad_length</span><span class="p">),</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span>
            <span class="p">)</span>
            <span class="n">trans_prob</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                <span class="n">trans_prob</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
                <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">phn_pad_length</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">phn_pad_length</span><span class="p">),</span>
                <span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_inf</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Stack into single tensor</span>
        <span class="n">poss_phns</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">poss_phns</span><span class="p">)</span>
        <span class="n">trans_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">trans_prob</span><span class="p">)</span>
        <span class="n">trans_prob</span><span class="p">[</span><span class="n">trans_prob</span> <span class="o">==</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neg_inf</span>

        <span class="c1"># make pi prob</span>
        <span class="n">pi_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neg_inf</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">U_max</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">start_state</span> <span class="ow">in</span> <span class="n">start_states</span><span class="p">:</span>
            <span class="n">pi_prob</span><span class="p">[:,</span> <span class="n">start_state</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">pi_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">pi_prob</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Convert poss_phn_lens from absolute to relative lengths</span>
        <span class="n">poss_phn_lens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">poss_phn_lens</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">/</span> <span class="n">U_max</span>
        <span class="k">return</span> <span class="n">poss_phns</span><span class="p">,</span> <span class="n">poss_phn_lens</span><span class="p">,</span> <span class="n">trans_prob</span><span class="p">,</span> <span class="n">pi_prob</span><span class="p">,</span> <span class="n">final_states</span></div>

    <span class="k">def</span> <span class="nf">_make_pi_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">phn_lens_abs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Creates tensor of initial (log) probabilities (known as &#39;pi&#39;).</span>
<span class="sd">        Assigns all probability mass to the first phoneme in the sequence.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        phn_lens_abs : torch.Tensor (batch)</span>
<span class="sd">            The absolute length of each phoneme sequence in the batch.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        pi_prob : torch.Tensor (batch, phn)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">phn_lens_abs</span><span class="p">)</span>
        <span class="n">U_max</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">phn_lens_abs</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>

        <span class="n">pi_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neg_inf</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">U_max</span><span class="p">])</span>
        <span class="n">pi_prob</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">return</span> <span class="n">pi_prob</span>

    <span class="k">def</span> <span class="nf">_make_trans_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">phn_lens_abs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Creates tensor of transition (log) probabilities.</span>
<span class="sd">        Only allows transitions to the same phoneme (self-loop) or the next</span>
<span class="sd">        phoneme in the phn sequence</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        phn_lens_abs : torch.Tensor (batch)</span>
<span class="sd">            The absolute length of each phoneme sequence in the batch.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        trans_prob : torch.Tensor (batch, from, to)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Extract useful values for later</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">phn_lens_abs</span><span class="p">)</span>
        <span class="n">U_max</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">phn_lens_abs</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">phn_lens_abs</span><span class="o">.</span><span class="n">device</span>

        <span class="c1">## trans_prob matrix consists of 2 diagonals:</span>
        <span class="c1">## (1) offset diagonal (next state) &amp;</span>
        <span class="c1">## (2) main diagonal (self-loop)</span>
        <span class="c1"># make offset diagonal</span>
        <span class="n">trans_prob_off_diag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">U_max</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">zero_side</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">U_max</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">zero_bottom</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">U_max</span><span class="p">])</span>
        <span class="n">trans_prob_off_diag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">zero_side</span><span class="p">,</span> <span class="n">trans_prob_off_diag</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">trans_prob_off_diag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">trans_prob_off_diag</span><span class="p">,</span> <span class="n">zero_bottom</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># make main diagonal</span>
        <span class="n">trans_prob_main_diag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">U_max</span><span class="p">)</span>

        <span class="c1"># join the diagonals and repeat for whole batch</span>
        <span class="n">trans_prob</span> <span class="o">=</span> <span class="n">trans_prob_off_diag</span> <span class="o">+</span> <span class="n">trans_prob_main_diag</span>
        <span class="n">trans_prob</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">trans_prob</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">U_max</span><span class="p">,</span> <span class="n">U_max</span><span class="p">)</span>
            <span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># clear probabilities for too-long sequences</span>
        <span class="n">mask_a</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">U_max</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&lt;</span> <span class="n">phn_lens_abs</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">mask_a</span> <span class="o">=</span> <span class="n">mask_a</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">mask_a</span> <span class="o">=</span> <span class="n">mask_a</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">U_max</span><span class="p">)</span>
        <span class="n">mask_b</span> <span class="o">=</span> <span class="n">mask_a</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">trans_prob</span> <span class="o">=</span> <span class="n">trans_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">mask_a</span> <span class="o">&amp;</span> <span class="n">mask_b</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="c1">## put -infs in place of zeros:</span>
        <span class="n">trans_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">trans_prob</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">trans_prob</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="c1">## normalize</span>
        <span class="n">trans_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">trans_prob</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1">## set nans to v neg numbers</span>
        <span class="n">trans_prob</span><span class="p">[</span><span class="n">trans_prob</span> <span class="o">!=</span> <span class="n">trans_prob</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neg_inf</span>
        <span class="c1">## set -infs to v neg numbers</span>
        <span class="n">trans_prob</span><span class="p">[</span><span class="n">trans_prob</span> <span class="o">==</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neg_inf</span>

        <span class="k">return</span> <span class="n">trans_prob</span>

    <span class="k">def</span> <span class="nf">_make_emiss_pred_useful</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">emission_pred</span><span class="p">,</span> <span class="n">lens_abs</span><span class="p">,</span> <span class="n">phn_lens_abs</span><span class="p">,</span> <span class="n">phns</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Creates a &#39;useful&#39; form of the posterior probabilities, rearranged</span>
<span class="sd">        into the order of phoneme appearance in phns.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        emission_pred : torch.Tensor (batch, time, phoneme in vocabulary)</span>
<span class="sd">            posterior probabilities from our acoustic model</span>
<span class="sd">        lens_abs : torch.Tensor (batch)</span>
<span class="sd">            The absolute length of each input to the acoustic model,</span>
<span class="sd">            i.e., the number of frames.</span>
<span class="sd">        phn_lens_abs : torch.Tensor (batch)</span>
<span class="sd">            The absolute length of each phoneme sequence in the batch.</span>
<span class="sd">        phns : torch.Tensor (batch, phoneme in phn sequence)</span>
<span class="sd">            The phonemes that are known/thought to be in each utterance.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        emiss_pred_useful : torch.Tensor</span>
<span class="sd">            Tensor shape (batch, phoneme in phn sequence, time).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Extract useful values for later</span>
        <span class="n">U_max</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">phn_lens_abs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">fb_max_length</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">lens_abs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">emission_pred</span><span class="o">.</span><span class="n">device</span>

        <span class="c1"># apply mask based on lens_abs</span>
        <span class="n">mask_lens</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">fb_max_length</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&lt;</span> <span class="n">lens_abs</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="n">emiss_pred_acc_lens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">mask_lens</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">],</span>
            <span class="n">emission_pred</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># manipulate phn tensor, and then &#39;torch.gather&#39;</span>
        <span class="n">phns</span> <span class="o">=</span> <span class="n">phns</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">phns_copied</span> <span class="o">=</span> <span class="n">phns</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">fb_max_length</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">emiss_pred_useful</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">emiss_pred_acc_lens</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">phns_copied</span><span class="p">)</span>

        <span class="c1"># apply mask based on phn_lens_abs</span>
        <span class="n">mask_phn_lens</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">U_max</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&lt;</span> <span class="n">phn_lens_abs</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">emiss_pred_useful</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">mask_phn_lens</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">emiss_pred_useful</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_inf</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">emiss_pred_useful</span> <span class="o">=</span> <span class="n">emiss_pred_useful</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">emiss_pred_useful</span>

    <span class="k">def</span> <span class="nf">_dp_forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pi_prob</span><span class="p">,</span>
        <span class="n">trans_prob</span><span class="p">,</span>
        <span class="n">emiss_pred_useful</span><span class="p">,</span>
        <span class="n">lens_abs</span><span class="p">,</span>
        <span class="n">phn_lens_abs</span><span class="p">,</span>
        <span class="n">phns</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Does forward dynamic programming algorithm.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        pi_prob : torch.Tensor (batch, phn)</span>
<span class="sd">            Tensor containing initial (log) probabilities.</span>
<span class="sd">        trans_prob : torch.Tensor (batch, from, to)</span>
<span class="sd">            Tensor containing transition (log) probabilities.</span>
<span class="sd">        emiss_pred_useful : torch.Tensor (batch, phoneme in phn sequence, time)</span>
<span class="sd">            A &#39;useful&#39; form of the posterior probabilities, rearranged</span>
<span class="sd">            into the order of phoneme appearance in phns.</span>
<span class="sd">        lens_abs : torch.Tensor (batch)</span>
<span class="sd">            The absolute length of each input to the acoustic model,</span>
<span class="sd">            i.e., the number of frames.</span>
<span class="sd">        phn_lens_abs : torch.Tensor (batch)</span>
<span class="sd">            The absolute length of each phoneme sequence in the batch.</span>
<span class="sd">        phns : torch.Tensor (batch, phoneme in phn sequence)</span>
<span class="sd">            The phonemes that are known/thought to be in each utterance.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        sum_alpha_T : torch.Tensor (batch)</span>
<span class="sd">            The (log) likelihood of each utterance in the batch.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># useful values</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">phn_lens_abs</span><span class="p">)</span>
        <span class="n">U_max</span> <span class="o">=</span> <span class="n">phn_lens_abs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="n">fb_max_length</span> <span class="o">=</span> <span class="n">lens_abs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">emiss_pred_useful</span><span class="o">.</span><span class="n">device</span>

        <span class="n">pi_prob</span> <span class="o">=</span> <span class="n">pi_prob</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">trans_prob</span> <span class="o">=</span> <span class="n">trans_prob</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># initialise</span>
        <span class="n">alpha_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neg_inf</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
            <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">U_max</span><span class="p">,</span> <span class="n">fb_max_length</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="n">alpha_matrix</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">pi_prob</span> <span class="o">+</span> <span class="n">emiss_pred_useful</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">fb_max_length</span><span class="p">):</span>
            <span class="n">utt_lens_passed</span> <span class="o">=</span> <span class="n">lens_abs</span> <span class="o">&lt;</span> <span class="n">t</span>

            <span class="k">if</span> <span class="kc">True</span> <span class="ow">in</span> <span class="n">utt_lens_passed</span><span class="p">:</span>
                <span class="n">n_passed</span> <span class="o">=</span> <span class="n">utt_lens_passed</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                <span class="n">I_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neg_inf</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_passed</span><span class="p">,</span> <span class="n">U_max</span><span class="p">,</span> <span class="n">U_max</span><span class="p">)</span>
                <span class="n">I_tensor</span><span class="p">[:,</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">U_max</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">U_max</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">0.0</span>
                <span class="n">I_tensor</span> <span class="o">=</span> <span class="n">I_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

                <span class="n">trans_prob</span><span class="p">[</span><span class="n">utt_lens_passed</span><span class="p">]</span> <span class="o">=</span> <span class="n">I_tensor</span>

            <span class="n">alpha_times_trans</span> <span class="o">=</span> <span class="n">batch_log_matvecmul</span><span class="p">(</span>
                <span class="n">trans_prob</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">alpha_matrix</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">alpha_matrix</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">alpha_times_trans</span> <span class="o">+</span> <span class="n">emiss_pred_useful</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">t</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="n">sum_alpha_T</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span>
            <span class="n">alpha_matrix</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">sum_alpha_T</span>

    <span class="k">def</span> <span class="nf">_dp_viterbi</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pi_prob</span><span class="p">,</span>
        <span class="n">trans_prob</span><span class="p">,</span>
        <span class="n">emiss_pred_useful</span><span class="p">,</span>
        <span class="n">lens_abs</span><span class="p">,</span>
        <span class="n">phn_lens_abs</span><span class="p">,</span>
        <span class="n">phns</span><span class="p">,</span>
        <span class="n">final_states</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calculates Viterbi alignment using dynamic programming.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        pi_prob : torch.Tensor (batch, phn)</span>
<span class="sd">            Tensor containing initial (log) probabilities.</span>
<span class="sd">        trans_prob : torch.Tensor (batch, from, to)</span>
<span class="sd">            Tensor containing transition (log) probabilities.</span>
<span class="sd">        emiss_pred_useful : torch.Tensor (batch, phoneme in phn sequence, time)</span>
<span class="sd">            A &#39;useful&#39; form of the posterior probabilities, rearranged</span>
<span class="sd">            into the order of phoneme appearance in phns.</span>
<span class="sd">        lens_abs : torch.Tensor (batch)</span>
<span class="sd">            The absolute length of each input to the acoustic model,</span>
<span class="sd">            i.e., the number of frames.</span>
<span class="sd">        phn_lens_abs : torch.Tensor (batch)</span>
<span class="sd">            The absolute length of each phoneme sequence in the batch.</span>
<span class="sd">        phns : torch.Tensor (batch, phoneme in phn sequence)</span>
<span class="sd">            The phonemes that are known/thought to be in each utterance.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        z_stars : list of lists of int</span>
<span class="sd">            Viterbi alignments for the files in the batch.</span>
<span class="sd">        z_stars_loc : list of lists of int</span>
<span class="sd">            The locations of the Viterbi alignments for the files in the batch.</span>
<span class="sd">            e.g., for a batch with a single utterance with 5 phonemes,</span>
<span class="sd">            `z_stars_loc` will look like:</span>
<span class="sd">            [[0, 0, 0, 1, 1, 2, 3, 3, 3, 4, 4]].</span>
<span class="sd">        viterbi_scores : torch.Tensor (batch)</span>
<span class="sd">            The (log) likelihood of the Viterbi path for each utterance.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># useful values</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">phn_lens_abs</span><span class="p">)</span>
        <span class="n">U_max</span> <span class="o">=</span> <span class="n">phn_lens_abs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="n">fb_max_length</span> <span class="o">=</span> <span class="n">lens_abs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">emiss_pred_useful</span><span class="o">.</span><span class="n">device</span>

        <span class="n">pi_prob</span> <span class="o">=</span> <span class="n">pi_prob</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">trans_prob</span> <span class="o">=</span> <span class="n">trans_prob</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">v_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neg_inf</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
            <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">U_max</span><span class="p">,</span> <span class="n">fb_max_length</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="n">backpointers</span> <span class="o">=</span> <span class="o">-</span><span class="mi">99</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
            <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">U_max</span><span class="p">,</span> <span class="n">fb_max_length</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>

        <span class="c1"># initialise</span>
        <span class="n">v_matrix</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">pi_prob</span> <span class="o">+</span> <span class="n">emiss_pred_useful</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">fb_max_length</span><span class="p">):</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">argmax</span> <span class="o">=</span> <span class="n">batch_log_maxvecmul</span><span class="p">(</span>
                <span class="n">trans_prob</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">v_matrix</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">v_matrix</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">emiss_pred_useful</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">t</span><span class="p">]</span>

            <span class="n">backpointers</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">argmax</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">)</span>

        <span class="n">z_stars</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">z_stars_loc</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">utterance_in_batch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">len_abs</span> <span class="o">=</span> <span class="n">lens_abs</span><span class="p">[</span><span class="n">utterance_in_batch</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">final_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">final_states_utter</span> <span class="o">=</span> <span class="n">final_states</span><span class="p">[</span><span class="n">utterance_in_batch</span><span class="p">]</span>
                <span class="c1"># Pick most probable of the final states</span>
                <span class="n">viterbi_finals</span> <span class="o">=</span> <span class="n">v_matrix</span><span class="p">[</span>
                    <span class="n">utterance_in_batch</span><span class="p">,</span> <span class="n">final_states_utter</span><span class="p">,</span> <span class="n">len_abs</span> <span class="o">-</span> <span class="mi">1</span>
                <span class="p">]</span>
                <span class="n">final_state_chosen</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">viterbi_finals</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">U</span> <span class="o">=</span> <span class="n">final_states_utter</span><span class="p">[</span><span class="n">final_state_chosen</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">U</span> <span class="o">=</span> <span class="n">phn_lens_abs</span><span class="p">[</span><span class="n">utterance_in_batch</span><span class="p">]</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span>

            <span class="n">z_star_i_loc</span> <span class="o">=</span> <span class="p">[</span><span class="n">U</span><span class="p">]</span>
            <span class="n">z_star_i</span> <span class="o">=</span> <span class="p">[</span><span class="n">phns</span><span class="p">[</span><span class="n">utterance_in_batch</span><span class="p">,</span> <span class="n">z_star_i_loc</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
            <span class="k">for</span> <span class="n">time_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">len_abs</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
                <span class="n">current_best_loc</span> <span class="o">=</span> <span class="n">z_star_i_loc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                <span class="n">earlier_best_loc</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">backpointers</span><span class="p">[</span>
                        <span class="n">utterance_in_batch</span><span class="p">,</span> <span class="n">current_best_loc</span><span class="p">,</span> <span class="n">time_step</span> <span class="o">-</span> <span class="mi">1</span>
                    <span class="p">]</span>
                    <span class="o">.</span><span class="n">long</span><span class="p">()</span>
                    <span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="n">earlier_z_star</span> <span class="o">=</span> <span class="n">phns</span><span class="p">[</span>
                    <span class="n">utterance_in_batch</span><span class="p">,</span> <span class="n">earlier_best_loc</span>
                <span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

                <span class="n">z_star_i_loc</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">earlier_best_loc</span><span class="p">)</span>
                <span class="n">z_star_i</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">earlier_z_star</span><span class="p">)</span>
            <span class="n">z_stars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">z_star_i</span><span class="p">)</span>
            <span class="n">z_stars_loc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">z_star_i_loc</span><span class="p">)</span>

        <span class="c1"># picking out viterbi_scores</span>
        <span class="n">viterbi_scores</span> <span class="o">=</span> <span class="n">v_matrix</span><span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">phn_lens_abs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">lens_abs</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="p">]</span>

        <span class="k">return</span> <span class="n">z_stars</span><span class="p">,</span> <span class="n">z_stars_loc</span><span class="p">,</span> <span class="n">viterbi_scores</span>

    <span class="k">def</span> <span class="nf">_loss_reduction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">input_lens</span><span class="p">,</span> <span class="n">target_lens</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Applies reduction to loss as specified during object initialization.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        loss : torch.Tensor (batch)</span>
<span class="sd">            The loss tensor to be reduced.</span>
<span class="sd">        input_lens : torch.Tensor (batch)</span>
<span class="sd">            The absolute durations of the inputs.</span>
<span class="sd">        target_lens : torch.Tensor (batch)</span>
<span class="sd">            The absolute durations of the targets.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        loss : torch.Tensor (batch, or scalar)</span>
<span class="sd">            The loss with reduction applied if it is specified.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_len_norm</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">input_lens</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_len_norm</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">target_lens</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_reduction</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_reduction</span> <span class="o">==</span> <span class="s2">&quot;sum&quot;</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_reduction</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`batch_reduction` parameter must be one of &#39;none&#39;, &#39;sum&#39; or &#39;mean&#39;&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span>

<div class="viewcode-block" id="HMMAligner.forward"><a class="viewcode-back" href="../../../speechbrain.alignment.aligner.html#speechbrain.alignment.aligner.HMMAligner.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">emission_pred</span><span class="p">,</span>
        <span class="n">lens</span><span class="p">,</span>
        <span class="n">phns</span><span class="p">,</span>
        <span class="n">phn_lens</span><span class="p">,</span>
        <span class="n">dp_algorithm</span><span class="p">,</span>
        <span class="n">prob_matrices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Prepares relevant (log) probability tensors and does dynamic</span>
<span class="sd">        programming: either the forward or the Viterbi algorithm. Applies</span>
<span class="sd">        reduction as specified during object initialization.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        emission_pred : torch.Tensor (batch, time, phoneme in vocabulary)</span>
<span class="sd">            Posterior probabilities from our acoustic model.</span>
<span class="sd">        lens : torch.Tensor (batch)</span>
<span class="sd">            The relative duration of each utterance sound file.</span>
<span class="sd">        phns : torch.Tensor (batch, phoneme in phn sequence)</span>
<span class="sd">            The phonemes that are known/thought to be in each utterance</span>
<span class="sd">        phn_lens : torch.Tensor (batch)</span>
<span class="sd">            The relative length of each phoneme sequence in the batch.</span>
<span class="sd">        dp_algorithm : string</span>
<span class="sd">            Either &quot;forward&quot; or &quot;viterbi&quot;.</span>
<span class="sd">        prob_matrices : dict</span>
<span class="sd">            (Optional) Must contain keys &#39;trans_prob&#39;, &#39;pi_prob&#39; and &#39;final_states&#39;.</span>
<span class="sd">            Used to override the default forward and viterbi operations which</span>
<span class="sd">            force traversal over all of the states in the `phns` sequence.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        tensor</span>

<span class="sd">            (1) if dp_algorithm == &quot;forward&quot;.</span>

<span class="sd">                ``forward_scores`` : torch.Tensor (batch, or scalar)</span>

<span class="sd">                The (log) likelihood of each utterance in the batch, with reduction</span>
<span class="sd">                applied if specified. (OR)</span>

<span class="sd">            (2) if dp_algorithm == &quot;viterbi&quot;.</span>

<span class="sd">                ``viterbi_scores`` : torch.Tensor (batch, or scalar)</span>

<span class="sd">                The (log) likelihood of the Viterbi path for each utterance, with</span>
<span class="sd">                reduction applied if specified.</span>

<span class="sd">                ``alignments`` : list of lists of int</span>

<span class="sd">                Viterbi alignments for the files in the batch.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">lens_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">emission_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">lens</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        <span class="n">phn_lens_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">phns</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">phn_lens</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        <span class="n">phns</span> <span class="o">=</span> <span class="n">phns</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">prob_matrices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pi_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_pi_prob</span><span class="p">(</span><span class="n">phn_lens_abs</span><span class="p">)</span>
            <span class="n">trans_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_trans_prob</span><span class="p">(</span><span class="n">phn_lens_abs</span><span class="p">)</span>
            <span class="n">final_states</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="p">(</span><span class="s2">&quot;pi_prob&quot;</span> <span class="ow">in</span> <span class="n">prob_matrices</span><span class="p">)</span>
                <span class="ow">and</span> <span class="p">(</span><span class="s2">&quot;trans_prob&quot;</span> <span class="ow">in</span> <span class="n">prob_matrices</span><span class="p">)</span>
                <span class="ow">and</span> <span class="p">(</span><span class="s2">&quot;final_states&quot;</span> <span class="ow">in</span> <span class="n">prob_matrices</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">pi_prob</span> <span class="o">=</span> <span class="n">prob_matrices</span><span class="p">[</span><span class="s2">&quot;pi_prob&quot;</span><span class="p">]</span>
                <span class="n">trans_prob</span> <span class="o">=</span> <span class="n">prob_matrices</span><span class="p">[</span><span class="s2">&quot;trans_prob&quot;</span><span class="p">]</span>
                <span class="n">final_states</span> <span class="o">=</span> <span class="n">prob_matrices</span><span class="p">[</span><span class="s2">&quot;final_states&quot;</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sd">&quot;&quot;&quot;`prob_matrices` must contain the keys</span>
<span class="sd">                `pi_prob`, `trans_prob` and `final_states`&quot;&quot;&quot;</span>
                <span class="p">)</span>

        <span class="n">emiss_pred_useful</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_emiss_pred_useful</span><span class="p">(</span>
            <span class="n">emission_pred</span><span class="p">,</span> <span class="n">lens_abs</span><span class="p">,</span> <span class="n">phn_lens_abs</span><span class="p">,</span> <span class="n">phns</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">dp_algorithm</span> <span class="o">==</span> <span class="s2">&quot;forward&quot;</span><span class="p">:</span>
            <span class="c1"># do forward training</span>
            <span class="n">forward_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dp_forward</span><span class="p">(</span>
                <span class="n">pi_prob</span><span class="p">,</span>
                <span class="n">trans_prob</span><span class="p">,</span>
                <span class="n">emiss_pred_useful</span><span class="p">,</span>
                <span class="n">lens_abs</span><span class="p">,</span>
                <span class="n">phn_lens_abs</span><span class="p">,</span>
                <span class="n">phns</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">forward_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_reduction</span><span class="p">(</span>
                <span class="n">forward_scores</span><span class="p">,</span> <span class="n">lens_abs</span><span class="p">,</span> <span class="n">phn_lens_abs</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">forward_scores</span>

        <span class="k">elif</span> <span class="n">dp_algorithm</span> <span class="o">==</span> <span class="s2">&quot;viterbi&quot;</span><span class="p">:</span>
            <span class="n">alignments</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">viterbi_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dp_viterbi</span><span class="p">(</span>
                <span class="n">pi_prob</span><span class="p">,</span>
                <span class="n">trans_prob</span><span class="p">,</span>
                <span class="n">emiss_pred_useful</span><span class="p">,</span>
                <span class="n">lens_abs</span><span class="p">,</span>
                <span class="n">phn_lens_abs</span><span class="p">,</span>
                <span class="n">phns</span><span class="p">,</span>
                <span class="n">final_states</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">viterbi_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_reduction</span><span class="p">(</span>
                <span class="n">viterbi_scores</span><span class="p">,</span> <span class="n">lens_abs</span><span class="p">,</span> <span class="n">phn_lens_abs</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">viterbi_scores</span><span class="p">,</span> <span class="n">alignments</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;dp_algorithm input must be either &#39;forward&#39; or &#39;viterbi&#39;&quot;</span>
            <span class="p">)</span></div>

<div class="viewcode-block" id="HMMAligner.expand_phns_by_states_per_phoneme"><a class="viewcode-back" href="../../../speechbrain.alignment.aligner.html#speechbrain.alignment.aligner.HMMAligner.expand_phns_by_states_per_phoneme">[docs]</a>    <span class="k">def</span> <span class="nf">expand_phns_by_states_per_phoneme</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">phns</span><span class="p">,</span> <span class="n">phn_lens</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Expands each phoneme in the phn sequence by the number of hidden</span>
<span class="sd">        states per phoneme defined in the HMM.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        phns : torch.Tensor (batch, phoneme in phn sequence)</span>
<span class="sd">            The phonemes that are known/thought to be in each utterance.</span>
<span class="sd">        phn_lens : torch.Tensor (batch)</span>
<span class="sd">            The relative length of each phoneme sequence in the batch.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        expanded_phns : torch.Tensor (batch, phoneme in expanded phn sequence)</span>

<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; phns = torch.tensor([[0., 3., 5., 0.],</span>
<span class="sd">        ...                      [0., 2., 0., 0.]])</span>
<span class="sd">        &gt;&gt;&gt; phn_lens = torch.tensor([1., 0.75])</span>
<span class="sd">        &gt;&gt;&gt; aligner = HMMAligner(states_per_phoneme = 3)</span>
<span class="sd">        &gt;&gt;&gt; expanded_phns = aligner.expand_phns_by_states_per_phoneme(</span>
<span class="sd">        ...         phns, phn_lens</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; expanded_phns</span>
<span class="sd">        tensor([[ 0.,  1.,  2.,  9., 10., 11., 15., 16., 17.,  0.,  1.,  2.],</span>
<span class="sd">                [ 0.,  1.,  2.,  6.,  7.,  8.,  0.,  1.,  2.,  0.,  0.,  0.]])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Initialise expanded_phns</span>
        <span class="n">expanded_phns</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">phns</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">phns</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span>
        <span class="p">)</span>
        <span class="n">expanded_phns</span> <span class="o">=</span> <span class="n">expanded_phns</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">phns</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">phns</span> <span class="o">=</span> <span class="n">undo_padding</span><span class="p">(</span><span class="n">phns</span><span class="p">,</span> <span class="n">phn_lens</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">phns_utt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">phns</span><span class="p">):</span>
            <span class="n">expanded_phns_utt</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">phoneme_index</span> <span class="ow">in</span> <span class="n">phns_utt</span><span class="p">:</span>
                <span class="n">expanded_phns_utt</span> <span class="o">+=</span> <span class="p">[</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span> <span class="o">*</span> <span class="n">phoneme_index</span> <span class="o">+</span> <span class="n">i_</span>
                    <span class="k">for</span> <span class="n">i_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span><span class="p">)</span>
                <span class="p">]</span>

            <span class="n">expanded_phns</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">expanded_phns_utt</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="n">expanded_phns_utt</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">expanded_phns</span></div>

<div class="viewcode-block" id="HMMAligner.store_alignments"><a class="viewcode-back" href="../../../speechbrain.alignment.aligner.html#speechbrain.alignment.aligner.HMMAligner.store_alignments">[docs]</a>    <span class="k">def</span> <span class="nf">store_alignments</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">,</span> <span class="n">alignments</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Records Viterbi alignments in `self.align_dict`.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        ids : list of str</span>
<span class="sd">            IDs of the files in the batch.</span>
<span class="sd">        alignments : list of lists of int</span>
<span class="sd">            Viterbi alignments for the files in the batch.</span>
<span class="sd">            Without padding.</span>

<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; aligner = HMMAligner()</span>
<span class="sd">        &gt;&gt;&gt; ids = [&#39;id1&#39;, &#39;id2&#39;]</span>
<span class="sd">        &gt;&gt;&gt; alignments = [[0, 2, 4], [1, 2, 3, 4]]</span>
<span class="sd">        &gt;&gt;&gt; aligner.store_alignments(ids, alignments)</span>
<span class="sd">        &gt;&gt;&gt; aligner.align_dict.keys()</span>
<span class="sd">        dict_keys([&#39;id1&#39;, &#39;id2&#39;])</span>
<span class="sd">        &gt;&gt;&gt; aligner.align_dict[&#39;id1&#39;]</span>
<span class="sd">        tensor([0, 2, 4], dtype=torch.int16)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="nb">id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ids</span><span class="p">):</span>
            <span class="n">alignment_i</span> <span class="o">=</span> <span class="n">alignments</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">alignment_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">alignment_i</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">align_dict</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">alignment_i</span></div>

    <span class="k">def</span> <span class="nf">_get_flat_start_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lens_abs</span><span class="p">,</span> <span class="n">phn_lens_abs</span><span class="p">,</span> <span class="n">phns</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Prepares flat start alignments (with zero padding) for every utterance</span>
<span class="sd">        in the batch.</span>
<span class="sd">        Every phoneme will have an equal duration, except for the final phoneme</span>
<span class="sd">        potentially. E.g. if 104 frames and 10 phonemes, 9 phonemes will have</span>
<span class="sd">        duration of 10 frames, and one phoneme will have a duration of 14 frames.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        lens_abs : torch.Tensor (batch)</span>
<span class="sd">            The absolute length of each input to the acoustic model,</span>
<span class="sd">            i.e., the number of frames.</span>

<span class="sd">        phn_lens_abs : torch.Tensor (batch)</span>
<span class="sd">            The absolute length of each phoneme sequence in the batch.</span>

<span class="sd">        phns : torch.Tensor (batch, phoneme in phn sequence)</span>
<span class="sd">            The phonemes that are known/thought to be in each utterance.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        flat_start_batch : torch.Tensor (batch, time)</span>
<span class="sd">            Flat start alignments for utterances in the batch, with zero padding.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">phns</span> <span class="o">=</span> <span class="n">phns</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lens_abs</span><span class="p">)</span>
        <span class="n">fb_max_length</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">lens_abs</span><span class="p">)</span>

        <span class="n">flat_start_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">fb_max_length</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">utter_phns</span> <span class="o">=</span> <span class="n">phns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">utter_phns</span> <span class="o">=</span> <span class="n">utter_phns</span><span class="p">[:</span> <span class="n">phn_lens_abs</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>  <span class="c1"># crop out zero padding</span>
            <span class="n">repeat_amt</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">lens_abs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">utter_phns</span><span class="p">))</span>

            <span class="c1"># make sure repeat_amt is at least 1. (the code above</span>
            <span class="c1"># may make repeat_amt==0 if self.states_per_phoneme is too large).</span>
            <span class="k">if</span> <span class="n">repeat_amt</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">repeat_amt</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="c1"># repeat each phoneme in utter_phns by repeat_amt</span>
            <span class="n">utter_phns</span> <span class="o">=</span> <span class="n">utter_phns</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">repeat_amt</span><span class="p">)</span>

            <span class="c1"># len(utter_phns) may be &lt;, == or &gt; lens_abs[i], so</span>
            <span class="c1"># make sure len(utter_phns) == lens_abs[i]</span>
            <span class="n">utter_phns</span> <span class="o">=</span> <span class="n">utter_phns</span><span class="p">[:</span> <span class="n">lens_abs</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
            <span class="n">utter_phns</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                <span class="n">utter_phns</span><span class="p">,</span>
                <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">lens_abs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">utter_phns</span><span class="p">)),</span>
                <span class="n">value</span><span class="o">=</span><span class="n">utter_phns</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># pad out with final phoneme</span>
            <span class="p">)</span>

            <span class="n">flat_start_batch</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">utter_phns</span><span class="p">)]</span> <span class="o">=</span> <span class="n">utter_phns</span>

        <span class="k">return</span> <span class="n">flat_start_batch</span>

    <span class="k">def</span> <span class="nf">_get_viterbi_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">,</span> <span class="n">lens_abs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Retrieves Viterbi alignments stored in `self.align_dict` and</span>
<span class="sd">        creates a batch of them, with zero padding.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        ids : list of str</span>
<span class="sd">            IDs of the files in the batch.</span>
<span class="sd">        lens_abs : torch.Tensor (batch)</span>
<span class="sd">            The absolute length of each input to the acoustic model,</span>
<span class="sd">            i.e., the number of frames.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        viterbi_batch : torch.Tensor (batch, time)</span>
<span class="sd">            The previously-recorded Viterbi alignments for the utterances</span>
<span class="sd">            in the batch.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lens_abs</span><span class="p">)</span>
        <span class="n">fb_max_length</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">lens_abs</span><span class="p">)</span>

        <span class="n">viterbi_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">fb_max_length</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">viterbi_preds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_dict</span><span class="p">[</span><span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
            <span class="n">viterbi_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                <span class="n">viterbi_preds</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">fb_max_length</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">viterbi_preds</span><span class="p">))</span>
            <span class="p">)</span>

            <span class="n">viterbi_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">viterbi_preds</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">viterbi_batch</span>

<div class="viewcode-block" id="HMMAligner.get_prev_alignments"><a class="viewcode-back" href="../../../speechbrain.alignment.aligner.html#speechbrain.alignment.aligner.HMMAligner.get_prev_alignments">[docs]</a>    <span class="k">def</span> <span class="nf">get_prev_alignments</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">,</span> <span class="n">emission_pred</span><span class="p">,</span> <span class="n">lens</span><span class="p">,</span> <span class="n">phns</span><span class="p">,</span> <span class="n">phn_lens</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fetches previously recorded Viterbi alignments if they are available.</span>
<span class="sd">        If not, fetches flat start alignments.</span>
<span class="sd">        Currently, assumes that if a Viterbi alignment is not available for the</span>
<span class="sd">        first utterance in the batch, it will not be available for the rest of</span>
<span class="sd">        the utterances.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        ids : list of str</span>
<span class="sd">            IDs of the files in the batch.</span>
<span class="sd">        emission_pred : torch.Tensor (batch, time, phoneme in vocabulary)</span>
<span class="sd">            Posterior probabilities from our acoustic model. Used to infer the</span>
<span class="sd">            duration of the longest utterance in the batch.</span>
<span class="sd">        lens : torch.Tensor (batch)</span>
<span class="sd">            The relative duration of each utterance sound file.</span>
<span class="sd">        phns : torch.Tensor (batch, phoneme in phn sequence)</span>
<span class="sd">            The phonemes that are known/thought to be in each utterance.</span>
<span class="sd">        phn_lens : torch.Tensor (batch)</span>
<span class="sd">            The relative length of each phoneme sequence in the batch.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor (batch, time)</span>
<span class="sd">            Zero-padded alignments.</span>

<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; ids = [&#39;id1&#39;, &#39;id2&#39;]</span>
<span class="sd">        &gt;&gt;&gt; emission_pred = torch.tensor([[[ -1., -10., -10.],</span>
<span class="sd">        ...                                [-10.,  -1., -10.],</span>
<span class="sd">        ...                                [-10., -10.,  -1.]],</span>
<span class="sd">        ...</span>
<span class="sd">        ...                               [[ -1., -10., -10.],</span>
<span class="sd">        ...                                [-10.,  -1., -10.],</span>
<span class="sd">        ...                                [-10., -10., -10.]]])</span>
<span class="sd">        &gt;&gt;&gt; lens = torch.tensor([1., 0.66])</span>
<span class="sd">        &gt;&gt;&gt; phns = torch.tensor([[0, 1, 2],</span>
<span class="sd">        ...                      [0, 1, 0]])</span>
<span class="sd">        &gt;&gt;&gt; phn_lens = torch.tensor([1., 0.66])</span>
<span class="sd">        &gt;&gt;&gt; aligner = HMMAligner()</span>
<span class="sd">        &gt;&gt;&gt; alignment_batch = aligner.get_prev_alignments(</span>
<span class="sd">        ...        ids, emission_pred, lens, phns, phn_lens</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; alignment_batch</span>
<span class="sd">        tensor([[0, 1, 2],</span>
<span class="sd">                [0, 1, 0]])</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">lens_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">emission_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">lens</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        <span class="n">phn_lens_abs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">phns</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">phn_lens</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_viterbi_batch</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">lens_abs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_flat_start_batch</span><span class="p">(</span><span class="n">lens_abs</span><span class="p">,</span> <span class="n">phn_lens_abs</span><span class="p">,</span> <span class="n">phns</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_calc_accuracy_sent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alignments_</span><span class="p">,</span> <span class="n">ends_</span><span class="p">,</span> <span class="n">phns_</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calculates the accuracy between predicted alignments and ground truth</span>
<span class="sd">        alignments for a single sentence/utterance.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        alignments_ : list of ints</span>
<span class="sd">            The predicted alignments for the utterance.</span>
<span class="sd">        ends_ : list of ints</span>
<span class="sd">            A list of the sample indices where each ground truth phoneme</span>
<span class="sd">            ends, according to the transcription.</span>
<span class="sd">        phns_ : list of ints</span>
<span class="sd">            The unpadded list of ground truth phonemes in the utterance.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        mean_acc : float</span>
<span class="sd">            The mean percentage of times that the upsampled predicted alignment</span>
<span class="sd">            matches the ground truth alignment.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Create array containing the true alignment at each sample</span>
        <span class="n">ends_</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">end</span><span class="p">)</span> <span class="k">for</span> <span class="n">end</span> <span class="ow">in</span> <span class="n">ends_</span><span class="p">]</span>
        <span class="n">true_durations</span> <span class="o">=</span> <span class="p">[</span><span class="n">ends_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">ends_</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ends_</span><span class="p">))]</span>
        <span class="n">true_alignments</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">phns_</span><span class="p">)):</span>
            <span class="n">true_alignments</span> <span class="o">+=</span> <span class="p">[</span><span class="n">phns_</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">*</span> <span class="p">(</span><span class="n">true_durations</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">true_alignments</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">true_alignments</span><span class="p">)</span>

        <span class="c1"># Upsample the predicted alignment array</span>
        <span class="c1"># and make sure length matches that of `true_alignment`</span>
        <span class="n">upsample_factor</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">true_alignments</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">alignments_</span><span class="p">)))</span>
        <span class="p">)</span>

        <span class="n">alignments_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">alignments_</span><span class="p">)</span>
        <span class="n">alignments_upsampled</span> <span class="o">=</span> <span class="n">alignments_</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">upsample_factor</span><span class="p">)</span>
        <span class="n">alignments_upsampled</span> <span class="o">=</span> <span class="n">alignments_upsampled</span><span class="p">[:</span> <span class="nb">len</span><span class="p">(</span><span class="n">true_alignments</span><span class="p">)]</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">true_alignments</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">alignments_upsampled</span><span class="p">):</span>
            <span class="n">alignments_upsampled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                <span class="n">alignments_upsampled</span><span class="p">,</span>
                <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">true_alignments</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">alignments_upsampled</span><span class="p">)),</span>
            <span class="p">)</span>

        <span class="c1"># Measure sample-wise accuracy</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">alignments_upsampled</span> <span class="o">==</span> <span class="n">true_alignments</span>
        <span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span>

        <span class="k">return</span> <span class="n">accuracy</span>

<div class="viewcode-block" id="HMMAligner.calc_accuracy"><a class="viewcode-back" href="../../../speechbrain.alignment.aligner.html#speechbrain.alignment.aligner.HMMAligner.calc_accuracy">[docs]</a>    <span class="k">def</span> <span class="nf">calc_accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alignments</span><span class="p">,</span> <span class="n">ends</span><span class="p">,</span> <span class="n">phns</span><span class="p">,</span> <span class="n">ind2labs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calculates mean accuracy between predicted alignments and ground truth</span>
<span class="sd">        alignments. Ground truth alignments are derived from ground truth phns</span>
<span class="sd">        and their ends in the audio sample.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        alignments : list of lists of ints/floats</span>
<span class="sd">            The predicted alignments for each utterance in the batch.</span>
<span class="sd">        ends : list of lists of ints</span>
<span class="sd">            A list of lists of sample indices where each ground truth phoneme</span>
<span class="sd">            ends, according to the transcription.</span>
<span class="sd">            Note: current implementation assumes that &#39;ends&#39; mark the index</span>
<span class="sd">            where the next phoneme begins.</span>
<span class="sd">        phns : list of lists of ints/floats</span>
<span class="sd">            The unpadded list of lists of ground truth phonemes in the batch.</span>
<span class="sd">        ind2labs : tuple</span>
<span class="sd">            (Optional)</span>
<span class="sd">            Contains the original index-to-label dicts for the first and second</span>
<span class="sd">            sequence of phonemes.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        mean_acc : float</span>
<span class="sd">            The mean percentage of times that the upsampled predicted alignment</span>
<span class="sd">            matches the ground truth alignment.</span>

<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; aligner = HMMAligner()</span>
<span class="sd">        &gt;&gt;&gt; alignments = [[0., 0., 0., 1.]]</span>
<span class="sd">        &gt;&gt;&gt; phns = [[0., 1.]]</span>
<span class="sd">        &gt;&gt;&gt; ends = [[2, 4]]</span>
<span class="sd">        &gt;&gt;&gt; mean_acc = aligner.calc_accuracy(alignments, ends, phns)</span>
<span class="sd">        &gt;&gt;&gt; mean_acc.item()</span>
<span class="sd">        75.0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">acc_hist</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Do conversion if states_per_phoneme &gt; 1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">alignments</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">utt</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">utt</span> <span class="ow">in</span> <span class="n">alignments</span>
            <span class="p">]</span>

        <span class="c1"># convert to common alphabet if need be</span>
        <span class="k">if</span> <span class="n">ind2labs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">alignments</span><span class="p">,</span> <span class="n">phns</span> <span class="o">=</span> <span class="n">map_inds_to_intersect</span><span class="p">(</span><span class="n">alignments</span><span class="p">,</span> <span class="n">phns</span><span class="p">,</span> <span class="n">ind2labs</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">alignments_</span><span class="p">,</span> <span class="n">ends_</span><span class="p">,</span> <span class="n">phns_</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">alignments</span><span class="p">,</span> <span class="n">ends</span><span class="p">,</span> <span class="n">phns</span><span class="p">):</span>
            <span class="n">acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_accuracy_sent</span><span class="p">(</span><span class="n">alignments_</span><span class="p">,</span> <span class="n">ends_</span><span class="p">,</span> <span class="n">phns_</span><span class="p">)</span>
            <span class="n">acc_hist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span>

        <span class="n">acc_hist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">acc_hist</span><span class="p">)</span>
        <span class="n">mean_acc</span> <span class="o">=</span> <span class="n">acc_hist</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">mean_acc</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span></div>

<div class="viewcode-block" id="HMMAligner.collapse_alignments"><a class="viewcode-back" href="../../../speechbrain.alignment.aligner.html#speechbrain.alignment.aligner.HMMAligner.collapse_alignments">[docs]</a>    <span class="k">def</span> <span class="nf">collapse_alignments</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alignments</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts alignments to 1 state per phoneme style.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        alignments : list of ints</span>
<span class="sd">            Predicted alignments for a single utterance.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        sequence : list of ints</span>
<span class="sd">            The predicted alignments converted to a 1 state per phoneme style.</span>

<span class="sd">        Example</span>
<span class="sd">        -------</span>
<span class="sd">        &gt;&gt;&gt; aligner = HMMAligner(states_per_phoneme = 3)</span>
<span class="sd">        &gt;&gt;&gt; alignments = [0, 1, 2, 3, 4, 5, 3, 4, 5, 0, 1, 2]</span>
<span class="sd">        &gt;&gt;&gt; sequence = aligner.collapse_alignments(alignments)</span>
<span class="sd">        &gt;&gt;&gt; sequence</span>
<span class="sd">        [0, 1, 1, 0]</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Filter the repetitions</span>
        <span class="n">sequence</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">v</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">alignments</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">v</span> <span class="o">!=</span> <span class="n">alignments</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="p">]</span>

        <span class="c1"># Pick out only multiples of self.states_per_phoneme</span>
        <span class="n">sequence</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">sequence</span> <span class="k">if</span> <span class="n">v</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Divide by self.states_per_phoneme</span>
        <span class="n">sequence</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">states_per_phoneme</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">sequence</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">sequence</span></div>

    <span class="nd">@mark_as_saver</span>
    <span class="k">def</span> <span class="nf">_save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">align_dict</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

    <span class="nd">@mark_as_loader</span>
    <span class="k">def</span> <span class="nf">_load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">end_of_epoch</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
        <span class="k">del</span> <span class="n">end_of_epoch</span>  <span class="c1"># Not used here.</span>
        <span class="k">del</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">align_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">)</span></div>


<div class="viewcode-block" id="map_inds_to_intersect"><a class="viewcode-back" href="../../../speechbrain.alignment.aligner.html#speechbrain.alignment.aligner.map_inds_to_intersect">[docs]</a><span class="k">def</span> <span class="nf">map_inds_to_intersect</span><span class="p">(</span><span class="n">lists1</span><span class="p">,</span> <span class="n">lists2</span><span class="p">,</span> <span class="n">ind2labs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts 2 lists containing indices for phonemes from different</span>
<span class="sd">    phoneme sets to a single phoneme so that comparing the equality</span>
<span class="sd">    of the indices of the resulting lists will yield the correct</span>
<span class="sd">    accuracy.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    lists1 : list of lists of ints</span>
<span class="sd">        Contains the indices of the first sequence of phonemes.</span>
<span class="sd">    lists2 : list of lists of ints</span>
<span class="sd">        Contains the indices of the second sequence of phonemes.</span>
<span class="sd">    ind2labs : tuple (dict, dict)</span>
<span class="sd">        Contains the original index-to-label dicts for the first and second</span>
<span class="sd">        sequence of phonemes.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    lists1_new : list of lists of ints</span>
<span class="sd">        Contains the indices of the first sequence of phonemes, mapped</span>
<span class="sd">        to the new phoneme set.</span>
<span class="sd">    lists2_new : list of lists of ints</span>
<span class="sd">        Contains the indices of the second sequence of phonemes, mapped</span>
<span class="sd">        to the new phoneme set.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; lists1 = [[0, 1]]</span>
<span class="sd">    &gt;&gt;&gt; lists2 = [[0, 1]]</span>
<span class="sd">    &gt;&gt;&gt; ind2lab1 = {</span>
<span class="sd">    ...        0: &quot;a&quot;,</span>
<span class="sd">    ...        1: &quot;b&quot;,</span>
<span class="sd">    ...        }</span>
<span class="sd">    &gt;&gt;&gt; ind2lab2 = {</span>
<span class="sd">    ...        0: &quot;a&quot;,</span>
<span class="sd">    ...        1: &quot;c&quot;,</span>
<span class="sd">    ...        }</span>
<span class="sd">    &gt;&gt;&gt; ind2labs = (ind2lab1, ind2lab2)</span>
<span class="sd">    &gt;&gt;&gt; out1, out2 = map_inds_to_intersect(lists1, lists2, ind2labs)</span>
<span class="sd">    &gt;&gt;&gt; out1</span>
<span class="sd">    [[0, 1]]</span>
<span class="sd">    &gt;&gt;&gt; out2</span>
<span class="sd">    [[0, 2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ind2lab1</span><span class="p">,</span> <span class="n">ind2lab2</span> <span class="o">=</span> <span class="n">ind2labs</span>

    <span class="c1"># Form 3 sets:</span>
    <span class="c1"># (1) labs in both mappings</span>
    <span class="c1"># (2) labs in only 1st mapping</span>
    <span class="c1"># (3) labs in only 2nd mapping</span>
    <span class="n">set1</span><span class="p">,</span> <span class="n">set2</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">ind2lab1</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> <span class="nb">set</span><span class="p">(</span><span class="n">ind2lab2</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

    <span class="n">intersect</span> <span class="o">=</span> <span class="n">set1</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">set2</span><span class="p">)</span>
    <span class="n">set1_only</span> <span class="o">=</span> <span class="n">set1</span><span class="o">.</span><span class="n">difference</span><span class="p">(</span><span class="n">set2</span><span class="p">)</span>
    <span class="n">set2_only</span> <span class="o">=</span> <span class="n">set2</span><span class="o">.</span><span class="n">difference</span><span class="p">(</span><span class="n">set1</span><span class="p">)</span>

    <span class="n">new_lab2ind</span> <span class="o">=</span> <span class="p">{</span><span class="n">lab</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">lab</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">intersect</span><span class="p">)}</span>
    <span class="n">new_lab2ind</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span><span class="n">lab</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_lab2ind</span><span class="p">)</span> <span class="o">+</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">lab</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">set1_only</span><span class="p">)}</span>
    <span class="p">)</span>
    <span class="n">new_lab2ind</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span><span class="n">lab</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_lab2ind</span><span class="p">)</span> <span class="o">+</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">lab</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">set2_only</span><span class="p">)}</span>
    <span class="p">)</span>

    <span class="c1"># Map lists to labels and apply new_lab2ind</span>
    <span class="n">lists1_lab</span> <span class="o">=</span> <span class="p">[[</span><span class="n">ind2lab1</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="n">utt</span><span class="p">]</span> <span class="k">for</span> <span class="n">utt</span> <span class="ow">in</span> <span class="n">lists1</span><span class="p">]</span>
    <span class="n">lists2_lab</span> <span class="o">=</span> <span class="p">[[</span><span class="n">ind2lab2</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="n">utt</span><span class="p">]</span> <span class="k">for</span> <span class="n">utt</span> <span class="ow">in</span> <span class="n">lists2</span><span class="p">]</span>

    <span class="n">lists1_new</span> <span class="o">=</span> <span class="p">[[</span><span class="n">new_lab2ind</span><span class="p">[</span><span class="n">lab</span><span class="p">]</span> <span class="k">for</span> <span class="n">lab</span> <span class="ow">in</span> <span class="n">utt</span><span class="p">]</span> <span class="k">for</span> <span class="n">utt</span> <span class="ow">in</span> <span class="n">lists1_lab</span><span class="p">]</span>
    <span class="n">lists2_new</span> <span class="o">=</span> <span class="p">[[</span><span class="n">new_lab2ind</span><span class="p">[</span><span class="n">lab</span><span class="p">]</span> <span class="k">for</span> <span class="n">lab</span> <span class="ow">in</span> <span class="n">utt</span><span class="p">]</span> <span class="k">for</span> <span class="n">utt</span> <span class="ow">in</span> <span class="n">lists2_lab</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">lists1_new</span><span class="p">,</span> <span class="n">lists2_new</span></div>


<div class="viewcode-block" id="batch_log_matvecmul"><a class="viewcode-back" href="../../../speechbrain.alignment.aligner.html#speechbrain.alignment.aligner.batch_log_matvecmul">[docs]</a><span class="k">def</span> <span class="nf">batch_log_matvecmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;For each &#39;matrix&#39; and &#39;vector&#39; pair in the batch, do matrix-vector</span>
<span class="sd">    multiplication in the log domain, i.e., logsumexp instead of add,</span>
<span class="sd">    add instead of multiply.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    A : torch.Tensor (batch, dim1, dim2)</span>
<span class="sd">        Tensor</span>
<span class="sd">    b : torch.Tensor (batch, dim1)</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Outputs</span>
<span class="sd">    -------</span>
<span class="sd">    x : torch.Tensor (batch, dim1)</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; A = torch.tensor([[[   0., 0.],</span>
<span class="sd">    ...                    [ -1e5, 0.]]])</span>
<span class="sd">    &gt;&gt;&gt; b = torch.tensor([[0., 0.,]])</span>
<span class="sd">    &gt;&gt;&gt; x = batch_log_matvecmul(A, b)</span>
<span class="sd">    &gt;&gt;&gt; x</span>
<span class="sd">    tensor([[0.6931, 0.0000]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; # non-log domain equivalent without batching funcionality</span>
<span class="sd">    &gt;&gt;&gt; A_ = torch.tensor([[1., 1.],</span>
<span class="sd">    ...                    [0., 1.]])</span>
<span class="sd">    &gt;&gt;&gt; b_ = torch.tensor([1., 1.,])</span>
<span class="sd">    &gt;&gt;&gt; x_ = torch.matmul(A_, b_)</span>
<span class="sd">    &gt;&gt;&gt; x_</span>
<span class="sd">    tensor([2., 1.])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">A</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="batch_log_maxvecmul"><a class="viewcode-back" href="../../../speechbrain.alignment.aligner.html#speechbrain.alignment.aligner.batch_log_maxvecmul">[docs]</a><span class="k">def</span> <span class="nf">batch_log_maxvecmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Similar to batch_log_matvecmul, but takes a maximum instead of</span>
<span class="sd">    logsumexp. Returns both the max and the argmax.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    A : torch.Tensor (batch, dim1, dim2)</span>
<span class="sd">        Tensor.</span>
<span class="sd">    b : torch.Tensor (batch, dim1)</span>
<span class="sd">        Tensor</span>

<span class="sd">    Outputs</span>
<span class="sd">    -------</span>
<span class="sd">    x : torch.Tensor (batch, dim1)</span>
<span class="sd">        Tensor.</span>
<span class="sd">    argmax : torch.Tensor (batch, dim1)</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; A = torch.tensor([[[   0., -1.],</span>
<span class="sd">    ...                    [ -1e5,  0.]]])</span>
<span class="sd">    &gt;&gt;&gt; b = torch.tensor([[0., 0.,]])</span>
<span class="sd">    &gt;&gt;&gt; x, argmax = batch_log_maxvecmul(A, b)</span>
<span class="sd">    &gt;&gt;&gt; x</span>
<span class="sd">    tensor([[0., 0.]])</span>
<span class="sd">    &gt;&gt;&gt; argmax</span>
<span class="sd">    tensor([[0, 1]])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">argmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">A</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">argmax</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, SpeechBrain

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>